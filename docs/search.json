[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Hello, and welcome to a new year at Macalester! I‚Äôm excited to spend the next 15 weeks with you journeying through a beloved field of mine.\nI will confess that early during college, I really didn‚Äôt love statistics. In my first statistics class, we focused almost entirely on formulas and procedures, and I was continually left wondering: ‚ÄúWhy should I care??‚Äù Still, I decided to give statistics another try, and the next year I had the fortune of taking a statistics class that really emphasized the connection between math and data. I began to see and imagine possibilities for collecting data in fields that could improve people‚Äôs lives, and I found this incredibly inspiring.\nIn graduate school, I quickly saw how much other disciplines use and benefit from statistics, and my admiration for the field deepened immensely. I‚Äôve had the chance to use statistics to learn about many interesting areas including biology, epidemiology, public health, and even my own hobbies! Over the years, my students have used data to learn about redlining, renewable energy, cinema, sports, ecology, and so much more.\nI love teaching introductory statistics, and I can‚Äôt wait to show you all the tools and ideas statistics has to offer. I‚Äôm also excited to find ways for you to connect to data and statistics in your own lives.\nLet‚Äôs have a great semester!"
  },
  {
    "objectID": "syllabus.html#meet-the-instructional-team",
    "href": "syllabus.html#meet-the-instructional-team",
    "title": "Syllabus",
    "section": "Meet the instructional team",
    "text": "Meet the instructional team\nInstructor: Leslie Myint (lez-lee mee-int) (she/her)\n\nAbout me: Aside from data-related topics, I love talking about games! I love playing board games, Dungeons and Dragons (D&D), and Nintendo console games. I also love staying active with weightlifting, rock climbing, Mac intramural soccer, and dance. If it snows enough this winter (fingers crossed!), I‚Äôm hoping to learn how to cross-country ski.\n\nPreceptors: We have 9 preceptors across the 6 sections of STAT 155 this semester. We will also have the support of a dedicated R preceptor who helps with R across all MSCS courses.\nRyan will be the preceptor dedicated to our section and will sometimes come to class to help with activities. Preceptor drop-in hours (office hours) will be shared across all sections of the course and posted on Moodle.\nIn addition, the Macalester Academic Excellence (MAX) Center provides tutoring for STAT 155. Check out their website for more information and their tutoring schedule.\n\n\n\n\n\n\nThe role of preceptors\n\n\n\nThe role of an MSCS preceptor is to help students with content questions, assist in the navigation of available resources, advise on studying approaches for classes, and assist with concepts, tools, and skills needed for problem sets. Students are accountable for their own learning; as such, preceptors are not allowed to share answers to assignments (unless specifically directed by the instructor), are not expected to immediately know the right approach, or provide assistance outside of office hours. Additional guidelines and expectations on how to interact with preceptors can be here."
  },
  {
    "objectID": "syllabus.html#contacting-the-instructor",
    "href": "syllabus.html#contacting-the-instructor",
    "title": "Syllabus",
    "section": "Contacting the instructor",
    "text": "Contacting the instructor\nI love getting to talk to students outside of class time‚Äîwhether about class-related topics or anything else. Come chat with me!\nI‚Äôll be setting times for drop-in hours based on feedback from the welcome survey. I‚Äôll update my drop-in hours on our course homepage and Moodle when they‚Äôre finalized.\nI‚Äôm also happy to meet one-on-one if my normal drop-in hours don‚Äôt work. You can schedule a time to meet with me via Calendly.\n\n\n\n\n\n\nCall me ‚ÄúLeslie‚Äù\n\n\n\nStudents sometimes wonder what to call their professors. I prefer to be called Leslie, but if you prefer to be more formal, I am also ok with Professor Myint. My preferred gender pronouns are she/her/hers.\nPlease help me make sure that I call you by your preferred name and pronouns too!"
  },
  {
    "objectID": "syllabus.html#community-is-key",
    "href": "syllabus.html#community-is-key",
    "title": "Syllabus",
    "section": "Community is key",
    "text": "Community is key\nA sense of community and connectedness can provide a powerful environment for learning: Research shows that learning is maximized when students feel a sense of belonging in the educational environment (e.g., Booker, 2016). A negative climate may create barriers to learning, while a positive climate can energize students‚Äô learning (e.g., Pascarella & Terenzini, cited in How Learning Works, 2012).\nFor these reasons, I will be designing our in-class group activities to intentionally foster community and connectedness. You can help cultivate our classroom community by being thoughtful about the way you engage with others in class."
  },
  {
    "objectID": "syllabus.html#reflection-is-paramount",
    "href": "syllabus.html#reflection-is-paramount",
    "title": "Syllabus",
    "section": "Reflection is paramount",
    "text": "Reflection is paramount\nWhether or not you continue learning statistics beyond this course, the skill of reflection is one that will be immensely useful whatever you do, and it is a skill that we will practice in this course.\nReflection is not just fundamental to learning content‚Äìit‚Äôs fundamental to learning any sort of intellectual, emotional, or physical skill. For this reason, we will practice reflection in our activities and assignments."
  },
  {
    "objectID": "syllabus.html#mistakes-are-essential",
    "href": "syllabus.html#mistakes-are-essential",
    "title": "Syllabus",
    "section": "Mistakes are essential",
    "text": "Mistakes are essential\n\nAn expert is a person who has made all the mistakes which can be made in a narrow field.\n\nNiels Bohr, Nobel Prize-winning physicist\n\n\nPerhaps paradoxically, an important way to gain confidence in an area is to make a lot of mistakes. Making mistakes can seem scary‚ÄîI get it. The culture of math and science education in America perpetuates a ‚Äúgenius‚Äù culture in STEM, where making mistakes immediately marks someone as ‚Äúless than‚Äù or unworthy. I wholeheartedly DO NOT believe in this. Everyone can thrive in this course and learn statistics. Mistakes are not only ok but necessary."
  },
  {
    "objectID": "syllabus.html#communication-is-a-superpower",
    "href": "syllabus.html#communication-is-a-superpower",
    "title": "Syllabus",
    "section": "Communication is a superpower",
    "text": "Communication is a superpower\n\nThe single biggest problem in communication is the illusion that it has taken place.\n\nGeorge Bernard Shaw\n\n\nBeing able to share an idea with others and have true understanding occur is a gift for all involved. And it‚Äôs really hard. We‚Äôll have a lot of opportunity for practice in this course. What‚Äôs more, the process of crafting effective communication is invaluable for deepening your own understanding:\n\n\n\nRead to collect the dots, write to connect them pic.twitter.com/YbgnKKFUNn\n\n‚Äî David Perell (@david_perell) July 5, 2021"
  },
  {
    "objectID": "syllabus.html#outside-of-class",
    "href": "syllabus.html#outside-of-class",
    "title": "Syllabus",
    "section": "Outside of class",
    "text": "Outside of class\nPre-class videos/readings: Some class periods will have required videos or reading to get acquainted with new concepts before seeing them again in class. My goal for these videos and readings is for you to get the most out of class time by being able to more easily follow explanations in class and to engage most fully in class activities. When there are required readings or videos, there will be an associated checkpoint to complete on Moodle (due just before the start of class).\n\n\n\n\n\n\nSuggestions\n\n\n\n\nAs you take notes on videos/readings, highlight or otherwise mark all the areas where you have questions. Gather up all of these questions in one place, and bring them to class with you.\nRecord any reflections from in-class time about your learning process or interactions with peers while they are still fresh.\nAfter learning a new topic in class, it is helpful to attempt the upcoming assignment as soon as possible. Just by getting some rough ideas down quickly, you avoid the difficulty of starting from a blank slate.\nCome to instructor and preceptor drop-in hours (office hours) to chat about the course or anything else! üòÉ"
  },
  {
    "objectID": "syllabus.html#during-class",
    "href": "syllabus.html#during-class",
    "title": "Syllabus",
    "section": "During class",
    "text": "During class\nClass time will be a mix of interactive lecture and longer stretches of group work. During the lecture portion, I will pause explanation frequently to prompt a short exercise or ask questions that you‚Äôll reflect on individually or together.\n\n\n\n\n\n\nSuggestions\n\n\n\n\nAs we review the material at the start of class and as you work on the class activity, reference your set of questions from the pre-class material. Have you made progress on addressing those questions? Who or what helped with improving your understanding, and how? Make notes of what concepts are still unclear so that you can review later."
  },
  {
    "objectID": "syllabus.html#my-philosophy",
    "href": "syllabus.html#my-philosophy",
    "title": "Syllabus",
    "section": "My philosophy",
    "text": "My philosophy\nGrading is thorny issue for many educators because of its known negative effects on learning and motivation. Nonetheless, it is ever-present in the US education system and at Macalester. Because I am required to submit grades for this course, it‚Äôs worth me taking a minute to share my philosophy about grading with you.\nWhat excites me about being a teacher is your learning.\nLearning flourishes in an environment where you find meaning and value in what we‚Äôre exploring, feel supported when engaging with challenges, receive useful feedback, and regularly reflect on your learning.\nIf I didn‚Äôt have to give grades, I wouldn‚Äôt. But because I am required to, it is important to me to create a course structure and grading system that allow learning to flourish:\n\nFinding meaning and value: I am striving to achieve this by creating space for authentic connection between you, your peers, and myself and by encouraging you to explore a topic that intrigues you for our course project.\nSupport in engaging with challenges: The assignments and activities that we will use to learn are meant to be challenging, and perfect understanding on your first try is not my expectation. For this reason, there will be opportunities to show improvement in your understanding over time to improve your grade on past assignments and assessments. Strong learning does not occur under excessive stress, so my hope is that having multiple chances to show your understanding alleviates much of the stress that would normally arise from needing to do as well as possible with only one try.\nReceiving useful feedback and reflecting regularly: My aim with feedback is to always provide guidance towards improvement, no matter where you are in your progress. However, good feedback alone is useless‚Äîlearners need to engage deeply with feedback in order to benefit the most from it. For this reason, opportunities to improve your scores on prior assignments will require reflecting on feedback."
  },
  {
    "objectID": "syllabus.html#assignments-and-assessments",
    "href": "syllabus.html#assignments-and-assessments",
    "title": "Syllabus",
    "section": "Assignments and assessments",
    "text": "Assignments and assessments\n\nCheckpoints\nFor class days where some reading and/or videos are required beforehand, there will be a short multiple-choice Moodle checkpoint due at 11:30am (30 minutes before class starts).\nYou can attempt a checkpoint question as many times as you want, but there is a 33% point deduction for each successive attempt on a question. For example, if your first attempt on a 1-point question is incorrect, the maximum possible score on that question is 0.67 points for the second attempt. On the third attempt, the maximum score is 0.33 points. No points are awarded for the 4th attempt and beyond.\n\n\nPractice problems\nTo practice concepts that we cover in class, there will be 8 weekly practice problems (PPs) due on Fridays at 5pm. (Note that if needed, extensions are possible‚Äîsee the late work policy.)\nYou will receive qualitative feedback on all questions on these practice problem sets. These assignments will receive on overall score on the following scale:\n\nHigh pass: 2 points\nPass: 1 point\nNeeds improvement: 0 points\n\nShowing growth: If, on a quiz, you demonstrate stronger understanding of the concepts that you missed on a PP, you have the chance to earn a higher score on the PP. Either via email or through an in-person conversation with the instructor, you must discuss how your understanding of the concept(s) changed from the PP to the quiz by addressing feedback from both the PP and the quiz. You can do this 2 times over the semester (once for Quizzes 1 and 2), and you can show improvement on multiple PPs using a single quiz.\n\n\nQuizzes\nThere will be 3 quizzes over the course of the semester.\n\nQuiz 1: Friday, 9/27. 1 hour in class.\nQuiz 2: Friday, 11/1. 1 hour in class. Will cover material from the first quiz to some extent because of the way that material in this course builds on earlier ideas.\nQuiz 3: Monday, 12/16 8:00-10:00am. ~1.25 hours during our final exam slot. Will cover content from the whole course.\n\nEach quiz question will be graded on the following scale:\n\n3 points: fully correct\n2 points: mostly correct with minor errors\n1 point: missing key understanding\n0 points: fully off target or blank\n\nQuiz format:\n\nFully pen/pencil and paper. You will not need to write code or use a calculator, but you will need to be able to read and interpret output from R code.\nYou are allowed to bring a 3x5 index card with notes written on both sides. Typing your notes and pasting them on the card is fine.\n\nShowing growth: For Quizzes 1 and 2, you can earn up to 50% of missed points back on quizzes if you complete a quiz correction and reflection. You must:\n\nWrite a reflection of how you prepared for the quiz and where you felt strongest and more uncertain in your understanding before taking the quiz\nSchedule a meeting with the instructor. During our conversation, you will correct your quiz responses and discuss the above reflection. You will get immediate feedback at this meeting. Use my Calendly page to schedule this meeting.\n\n\n\nProject\nThe goal of the course project is to apply the data analysis skills from our course to investigate a research question in a dataset of your choosing. Through milestones over the course of the semester, you will make steady progress on your projects and iterate on feedback. Full details about the project will be available on the Project page."
  },
  {
    "objectID": "syllabus.html#course-grading-system",
    "href": "syllabus.html#course-grading-system",
    "title": "Syllabus",
    "section": "Course grading system",
    "text": "Course grading system\nIn order to earn a given letter grade, all requirements listed under that column need to be met.\n\n\n\n\n\n\n\n\n\n\n\n\nGrade: A\nGrade: B\nGrade: C\n\n\n\n\nCheckpoint average\n‚â• 80%\n‚â• 70%\n‚â• 60%\n\n\nPractice problems (PPs)\n14 out of 16 points across all 8 PPs.\nMust submit all 8 PPs.\n12 out of 16 points across all 8 PPs.\n10 out of 16 points across all 8 PPs.\n\n\nQuiz average\n‚â• 90%\n‚â• 80%\n‚â• 70%\n\n\nProject\nEarn highest level (Excellent) on all 6 rubric categories\nEarn highest level (Excellent) on all but 1 rubric category\nEarn highest level (Excellent) on all but 2 rubric categories\n\n\n\nIf your work for different course components falls under different letter grades, your final letter grade will be an ‚Äúaverage‚Äù of the letter grades for the different components.\nC  C  C  C --&gt; C\nC  C  C  B --&gt; C+\nC  C  C  A --&gt; C+\nC  C  B  B --&gt; B-\nC  C  B  A --&gt; B-\nC  C  A  A --&gt; B\nC  B  B  B --&gt; B-\nC  B  B  A --&gt; B\nC  B  A  A --&gt; B+\nC  A  A  A --&gt; B+\nB  B  B  B --&gt; B\nB  B  B  A --&gt; B+\nB  B  A  A --&gt; A-\nB  A  A  A --&gt; A-\nA  A  A  A --&gt; A"
  },
  {
    "objectID": "syllabus.html#late-work",
    "href": "syllabus.html#late-work",
    "title": "Syllabus",
    "section": "Late work",
    "text": "Late work\nHomework assignments will generally be due weekly on Fridays at 5pm.\nThroughout the quarter, you may use up to three extensions on practice problems. You will These three extensions can be used on practice problems only, not checkpoints or quizzes. The purpose of deadlines (and extensions) is to keep you accountable for your own learning, to keep you on track with the pace of the course (which builds upon itself throughout the semester), and to provide preceptors and myself the ability to give you timely feedback on assignments.\nIn order to use an extension, you must email me (lmyint@macalester.edu) before the practice problem deadline to inform me you plan to use an extension. In your email, please specify (1) that you are using an extension, and (2) your self-set deadline is for the assignment. For example, you could say ‚ÄúI am using my second extension. I will have Practice Problems #7 in by 5:00pm two days from now.‚Äù Once you set your deadline, I expect you to stick to it. If you do not e-mail me prior to the practice problem deadline, the extension will not be counted, and your assignment will be counted as late. Late homework without prior communication receives no credit.\nIf you have run out of extensions and/or an extenuating circumstance occurs that impacts your ability to submit assignments on time, please email me to discuss the situation. I am happy to be flexible as long as you communicate!"
  },
  {
    "objectID": "syllabus.html#academic-integrity",
    "href": "syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic integrity",
    "text": "Academic integrity\nAcademic integrity is the cornerstone of our learning community. Students are expected to be familiar with the college‚Äôs standards on academic integrity.\nI encourage you to work with your classmates to discuss material and ideas for assignments, but in order for you to receive individualized feedback on your own learning, you must submit your own work. This involves writing your own code and putting explanations into your own words. Always cite any sources you use."
  },
  {
    "objectID": "syllabus.html#artificial-intelligence-ai-use",
    "href": "syllabus.html#artificial-intelligence-ai-use",
    "title": "Syllabus",
    "section": "Artificial intelligence (AI) use",
    "text": "Artificial intelligence (AI) use\nAI can both interfere with and enhance our capacity to learn. We must be mindful of when it might hinder us and when it might provide us with new understanding and/or assistance.\nIn an introductory course like this one, we are learning the essentials of a new language and way of thinking. AI may be able to help you in this journey by providing explanations for concepts that are different from the ones provided by the course.\nHowever, in order to truly learn, AI should not be used to generate responses that replace your thinking, so even if you use AI to help explain concepts, please write responses in your own words on assignments.\nIn general, please be aware of the limits of AI:\n\nAI does not always generate accurate output. If it gives you a number, fact, or code, assume it is wrong unless you either know the answer or can check in with another source. AI works best for topics you already understand to a sufficient extent.\nIf you provide minimum effort prompts, you will get low quality results. You will need to refine your prompts in order to get good outcomes. This will take work.\nBe thoughtful about when this tool is useful. Don‚Äôt use it if it isn‚Äôt appropriate for the case or circumstance.\nThe environmental impact of AI should not be ignored. The building and usage of AI tools consume a lot of energy (see here and here). For this reason, let‚Äôs be thoughtful about when we use AI and think about other sustainability behaviors that we can incorporate into our lives to offset this usage.\n\nIf you have any questions about your use of AI tools, please contact me to discuss them."
  },
  {
    "objectID": "r_resources.html",
    "href": "r_resources.html",
    "title": "R Resources",
    "section": "",
    "text": "Getting Help\n\nCrowd-sourced R functions\nInteractive RStudio tutorials on various topics, including R basics and visualizing data\nMarkdown (Quarto) basics, and tutorial\nRStudio cheatsheets\nGoogle! If you have a question about R, you can be fairly certain that someone else has already asked it. Google is your best friend for answering questions about R. I recommend adding ‚ÄúR tidyverse‚Äù to the search term. For example, to find out how to make a scatterplot, try searching ‚Äúscatterplot R tidyverse‚Äù.\n\n\n\nGetting Started\nFollow these instructions to set up the software that we‚Äôll be using throughout the semester. Even if you‚Äôve already downloaded both R and RStudio, you‚Äôll want to re-download to make sure that you have the most current versions.\n\nIf you have issues getting R and RStudio installed, you can use Macalester‚Äôs RStudio server. After you log in with your Macalester credentials, you will be able to work with a web version of RStudio.\n\nRequired: Change the default file download location for your internet browser.\n\nGenerally by default, internet browsers automatically save all files to the Downloads folder on your computer. This does not encourage good file organization practices. You need to change this option so that your browser asks you where to save each file before downloading it.\nThis page has information on how to do this for the most common browsers.\n\n\nRequired: Create a STAT155 folder on your Desktop or in your Documents folder. Within the STAT155 folder create 3 subfolders:\n\nActivities\nPractice Problems\nProject\n\n\nRequired: Download R and RStudio by following the directions below.\n\nFIRST: Download R here.\n\nIn the top section, you will see three links ‚ÄúDownload R for ‚Ä¶‚Äù\nChoose the link that corresponds to your computer.\nAs of August 27, 2024, the latest version of R is 4.4.1 (‚ÄúRace for Your Life‚Äù).\n\nSECOND: Download RStudio here.\n\nClick the button under step 2 to install the version of RStudio recommended for your computer.\nAs of August 27, 2024, the latest version of RStudio is 2024.04.2+764.\n\n\n\nRequired: Install required packages.\n\nAn R package is an extra bit of functionality that will help us in our data analysis efforts in a variety of ways.\nOpen RStudio and click inside the Console pane (by default, the bottom left pane). Copy and paste the following command into the Console. You should see the text below appear to the right of the &gt;, which is called the R prompt. After you paste, hit Enter.\n\n\ninstall.packages(c(\"tidyverse\"))\n\n\nYou will see a lot of text from status messages appearing in the Console as the packages are being installed. Wait until you see the &gt; again.\nEnter the command library(tidyverse) and hit Enter.\nIf you see an error message, then there was a problem installing the package. Talk to the instructor or a preceptor for help.\nQuit RStudio. You‚Äôre done setting up!\n\n\nOptional: For a tour of RStudio features, watch this video. It also shows you how to customize the layout and color scheme of RStudio.\n\nRequired: Set essential RStudio options.\nGo to Edit &gt; Preferences (or Settings‚Ä¶) &gt; General\nNavigate to the ‚ÄúWorkspace‚Äù section. You‚Äôll see 2 options:\n\nRestore .RData into workspace at startup: Leave this unchecked\nSave workspace to .RData on exit: Select ‚ÄúNever‚Äù\n\nWithout doing this RStudio will save and reload everything that you‚Äôve been working on from the start of the semester. Since we‚Äôll be working with new data each class, we want to keep our digital environment clean. Essentially, this is like an artist getting a clean canvas for each new painting rather than trying to paint all paintings on a single canvas."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "",
    "text": "Learn the fundamentals of summarizing, visualizing, and modeling data to answer research questions\n\n\nInstructor: Leslie Myint  Class meeting times: MWF 12:00-1:00pm  Class location: THEATR 204  Instructor drop-in hours:\n\nOlin-Rice 232\nMondays and Fridays: 2-3pm\nTuesdays and Thursdays: 3:30-4:30pm"
  },
  {
    "objectID": "assignments/pp3.html",
    "href": "assignments/pp3.html",
    "title": "Practice Problems 3",
    "section": "",
    "text": "Due Friday, 10/4 at 5pm on Moodle.",
    "crumbs": [
      "Practice Problems 3"
    ]
  },
  {
    "objectID": "assignments/pp3.html#context",
    "href": "assignments/pp3.html#context",
    "title": "Practice Problems 3",
    "section": "Context",
    "text": "Context\nPowerlifting is a sport in which athletes compete to lift as much as possible in 3 events: bench press, squat, and deadlift.\nOpen Powerlifting maintains a database of competition results for powerlifters across the world. We have information on 100,000 lifters from this database. Take a look at the codebook here.\nResearch question: Are lighter or heavier lifters proportionately stronger?",
    "crumbs": [
      "Practice Problems 3"
    ]
  },
  {
    "objectID": "assignments/pp3.html#exercise-1-define-outcome-variable",
    "href": "assignments/pp3.html#exercise-1-define-outcome-variable",
    "title": "Practice Problems 3",
    "section": "Exercise 1: Define outcome variable",
    "text": "Exercise 1: Define outcome variable\nUse the mutate() function from the dplyr to define an outcome variable called SWR that stands for strength-to-weight ratio. It should be computed as TotalKg divided by BodyweightKg. SWR measures how many times their bodyweight an athlete can lift in total. Higher numbers indicate higher relative strength.",
    "crumbs": [
      "Practice Problems 3"
    ]
  },
  {
    "objectID": "assignments/pp3.html#exercise-2-exploratory-visualizations",
    "href": "assignments/pp3.html#exercise-2-exploratory-visualizations",
    "title": "Practice Problems 3",
    "section": "Exercise 2: Exploratory visualizations",
    "text": "Exercise 2: Exploratory visualizations\nGuiding question: How are age, sex, bodyweight, and equipment usage related to strength?\nConstruct one visualization for each of these 4 explanatory variables and SWR. For each, write 1-2 sentences summarizing what you learn from the plot. Be sure to discuss trend, variability/dispersion about the trend, and any notable outliers.",
    "crumbs": [
      "Practice Problems 3"
    ]
  },
  {
    "objectID": "assignments/pp3.html#exercise-3-causal-diagram",
    "href": "assignments/pp3.html#exercise-3-causal-diagram",
    "title": "Practice Problems 3",
    "section": "Exercise 3: Causal diagram",
    "text": "Exercise 3: Causal diagram\nWe are interested in the relationship between BodyweightKg and SWR but are concerned about Age, Sex, and Equipment as potential confounders.\n\nPart a\nDraw a causal diagram that shows how these 5 variables might be related. Draw this by hand or software and save the file as pp3_dag.jpg or pp3_dag.png in the same folder as this .qmd file. You can then insert the diagram as below:\n![](pp3_dag.jpg)\n![](pp3_dag.png)\n\n\nPart b\nUse visualizations to explore if Age, Sex, and Equipment have a relationship with BodyweightKg. Explain how these explorations relate to your causal diagram. Do you think that there are other confounders that would be important to consider but are missing from our data?",
    "crumbs": [
      "Practice Problems 3"
    ]
  },
  {
    "objectID": "assignments/pp3.html#exercise-4-linear-regression-modeling",
    "href": "assignments/pp3.html#exercise-4-linear-regression-modeling",
    "title": "Practice Problems 3",
    "section": "Exercise 4: Linear regression modeling",
    "text": "Exercise 4: Linear regression modeling\nResearch question: Are lighter or heavier lifters proportionately stronger?\nPut another way, this question is getting at the causal effect of bodyweight on SWR.\n\nPart a\nFit an appropriate linear regression model that answers our research question.\n\n\nPart b\nInterpret the coefficient that answers our research question. Make sure to use appropriate causation vs.¬†association language, include units, and talk about averages rather than individual cases.\n\n\nPart c\nInterpret the remainder of the coefficients (including the intercept). Is it meaningful to interpret the intercept in this context?",
    "crumbs": [
      "Practice Problems 3"
    ]
  },
  {
    "objectID": "assignments/pp1.html",
    "href": "assignments/pp1.html",
    "title": "Practice Problems 1",
    "section": "",
    "text": "Due Friday, 9/13 at 5pm on Moodle.",
    "crumbs": [
      "Practice Problems 1"
    ]
  },
  {
    "objectID": "assignments/pp1.html#context",
    "href": "assignments/pp1.html#context",
    "title": "Practice Problems 1",
    "section": "Context",
    "text": "Context\nThis cartoon by Alison Bechdel inspired the ‚ÄúBechdel test‚Äù (image source):\n\nA movie passes the test if it meets the following criteria:\n\nthere are at least 2 women-identifying characters;\n\nthese characters talk to each other at least 1 time; and\n\nthey talk about something other than a male-identifying character\n\nWe‚Äôll work with the bechdel data within the fivethirtyeight package. You can enter ?bechdel in the Console to view the codebook.\n\n# Load the data\ndata(bechdel)",
    "crumbs": [
      "Practice Problems 1"
    ]
  },
  {
    "objectID": "assignments/pp1.html#exercise-1-define-two-new-variables",
    "href": "assignments/pp1.html#exercise-1-define-two-new-variables",
    "title": "Practice Problems 1",
    "section": "Exercise 1: Define two new variables",
    "text": "Exercise 1: Define two new variables\nWe can use the mutate() function from dplyr to define new variables. For example:\n\n# Define a new variable which measures years_since_1900\nbechdel &lt;- bechdel %&gt;% \n    mutate(years_since_1900 = year - 1900)\n\n# Check it out / confirm it's right\nbechdel %&gt;% \n    select(year, years_since_1900) %&gt;% \n    head(3)\n\nYour turn. Define a new variable budget_mil which measures budget in MILLIONS of dollars (on the 2013 scale). Similarly, define a new variable intgross_mil which measures international gross in MILLIONS of dollars (on the 2013 scale). Store these in bechdel and confirm your results match those here:\n## # A tibble: 3 x 2\n##   budget_mil intgross_mil\n##        &lt;dbl&gt;        &lt;dbl&gt;\n## 1       13           42.2\n## 2       45.7         41.5\n## 3       20          159.",
    "crumbs": [
      "Practice Problems 1"
    ]
  },
  {
    "objectID": "assignments/pp1.html#exercise-2-get-to-know-the-data",
    "href": "assignments/pp1.html#exercise-2-get-to-know-the-data",
    "title": "Practice Problems 1",
    "section": "Exercise 2: Get to know the data",
    "text": "Exercise 2: Get to know the data\nComplete each task below using dim(), nrow(), head(), or one of the dplyr functions: filter(), summarize(), select(). For any questions about movie budget and gross earnings, use budget_mil and gross_mil. NOTE: You don‚Äôt need to write out any discussion for this exercise, just include your code and output.\n\n# How many films are in the dataset?\n\n# When was the oldest film in the dataset made?\n\n# When was the most recent film in the dataset made?\n\n# What is the median film budget in 2013 dollars?\n\n# What films had budgets that exceeded 300 million in 2013 dollars?\n# (Just show the films and budgets, not all variables)\n\n# What films were made in 1971?\n# (Just show the films and year, not all variables)\n\n# What were the budget_mil and intgross_mil values for \"Titanic\"?\n# (Just show the budget_mil and intgross_mil values)",
    "crumbs": [
      "Practice Problems 1"
    ]
  },
  {
    "objectID": "assignments/pp1.html#exercise-3-does-it-pass-the-bechdel",
    "href": "assignments/pp1.html#exercise-3-does-it-pass-the-bechdel",
    "title": "Practice Problems 1",
    "section": "Exercise 3: Does it pass the Bechdel?",
    "text": "Exercise 3: Does it pass the Bechdel?\nThe clean_test variable records the Bechdel category into which each movie falls. From worst to best (if you care about the representation of women in film):\n\nnowomen = there are no significant women characters in the film\nnotalk = there are women characters but they don‚Äôt talk to one another\nmen = there are women characters that talk to one another but they only talk about men\ndubious = barely passes the Bechdel test\nok = passes the Bechdel test\n\nConstruct a visualization of the clean_test variable, and construct a corresponding table of counts. In one sentence, summarize your findings about clean_test as if you were writing a newspaper article about movies.",
    "crumbs": [
      "Practice Problems 1"
    ]
  },
  {
    "objectID": "assignments/pp1.html#exercise-4-how-much-money-did-it-make",
    "href": "assignments/pp1.html#exercise-4-how-much-money-did-it-make",
    "title": "Practice Problems 1",
    "section": "Exercise 4: How much money did it make?",
    "text": "Exercise 4: How much money did it make?\nThe intgross_mil variable records each movie‚Äôs total international gross, in millions of dollars (and adjusted to 2013).\n\nConstruct a visualization of the intgross_mil variable. Summarize your findings as if you were writing a newspaper article about movies.\nCalculate the mean and median values of intgross_mil. Explain why the mean exceeds the median by quite a bit and which is the more appropriate measure of trend here.\n\nNOTE: Since some films are missing information on intgross_mil, you‚Äôll need to use mean(___, na.rm = TRUE) where na.rm = TRUE says to remove the NA (missing) values from the calculation. Similarly for the median.\n\nCalculate the standard deviations for the intgross_mil and budget_mil variables. Comment on whether there‚Äôs more variability among how much money is spent making movies (budgets) or how much money a movie makes (grosses)?",
    "crumbs": [
      "Practice Problems 1"
    ]
  },
  {
    "objectID": "assignments/pp1.html#exercise-5-explaining-intgross_mil",
    "href": "assignments/pp1.html#exercise-5-explaining-intgross_mil",
    "title": "Practice Problems 1",
    "section": "Exercise 5: Explaining intgross_mil",
    "text": "Exercise 5: Explaining intgross_mil\nNext, let‚Äôs try to explain some of the variability in how much money films gross. To this end, we‚Äôll focus on modeling intgross_mil by budget_mil. That is, to what extent is the money a film makes related to its budget?\n\nIn the relationship between intgross_mil and budget_mil, which is the response variable?\nConstruct a visualization of the relationship between intgross_mil and budget_mil. Include a regression line that represents the trend in this relationship.\nIn 1-3 sentences, summarize your findings about this relationship as if you were writing a newspaper article about movies. Be sure to comment on both the trend and variability in this relationship.",
    "crumbs": [
      "Practice Problems 1"
    ]
  },
  {
    "objectID": "assignments/pp1.html#exercise-6-modeling-intgross_mil",
    "href": "assignments/pp1.html#exercise-6-modeling-intgross_mil",
    "title": "Practice Problems 1",
    "section": "Exercise 6: Modeling intgross_mil",
    "text": "Exercise 6: Modeling intgross_mil\nLet‚Äôs examine the relationship between intgross_mil and budget_mil in more detail.\n\nUse lm() to construct a linear regression model of the trend in the relationship between intgross_mil and budget_mil. Store this as movie_model and report a model summary table.\nWrite out a formula for the trend.\nProvide a newspaper appropriate interpretation of the budget_mil coefficient. Make sure to use non-causal language, include units, and talk about averages rather than individual cases.\nDoes it make sense to interpret the intercept in this model? Explain.",
    "crumbs": [
      "Practice Problems 1"
    ]
  },
  {
    "objectID": "assignments/pp1.html#exercise-7-predictions-and-residuals",
    "href": "assignments/pp1.html#exercise-7-predictions-and-residuals",
    "title": "Practice Problems 1",
    "section": "Exercise 7: Predictions and residuals",
    "text": "Exercise 7: Predictions and residuals\n\nIn exercise 2 you identified the budget (budget_mil) for the movie Titanic. Use the model formula above to predict its international gross. You can check your work using the predict() function. This provides a more accurate prediction (it doesn‚Äôt round the model coefficients) so don‚Äôt worry if your calculation doesn‚Äôt exactly match.\n\n\npredict(___, newdata = data.frame(budget_mil = ___))\n\n\nCalculate the Titanic residual. HINT: You‚Äôll need to use the observed intgross_mil value from exercise 2.\nBased on your calculation in part b, did the model over- or under-estimate Titanic‚Äôs gross?",
    "crumbs": [
      "Practice Problems 1"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html",
    "href": "activities/09_mlr_principles.html",
    "title": "Multiple regression principles",
    "section": "",
    "text": "You can download a template file to work with here.\nFile organization: Save this file in the ‚ÄúActivities‚Äù subfolder of your ‚ÄúSTAT155‚Äù folder.\n\n\n\nWorking with multiple predictors in our plots and models can get complicated! There are no recipes for this process, but there are some guiding principles that assist in long-term retention, deeper understanding, and the ability to generalize our tools in new settings.\nBy the end of this lesson, you should be familiar with some general principles for‚Ä¶\n\nincorporating additional quantitative or categorical predictors in a visualization\nhow additional quantitative or categorical predictors impact the physical representation of a model\ninterpreting quantitative or categorical coefficients in a multiple regression model\n\n\n\n\nBefore class you should have watched the following video:\n\nInterpreting multivariate models (slides)",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#learning-goals",
    "href": "activities/09_mlr_principles.html#learning-goals",
    "title": "Multiple regression principles",
    "section": "",
    "text": "Working with multiple predictors in our plots and models can get complicated! There are no recipes for this process, but there are some guiding principles that assist in long-term retention, deeper understanding, and the ability to generalize our tools in new settings.\nBy the end of this lesson, you should be familiar with some general principles for‚Ä¶\n\nincorporating additional quantitative or categorical predictors in a visualization\nhow additional quantitative or categorical predictors impact the physical representation of a model\ninterpreting quantitative or categorical coefficients in a multiple regression model",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#readings-and-videos",
    "href": "activities/09_mlr_principles.html#readings-and-videos",
    "title": "Multiple regression principles",
    "section": "",
    "text": "Before class you should have watched the following video:\n\nInterpreting multivariate models (slides)",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-1-review-visualization",
    "href": "activities/09_mlr_principles.html#exercise-1-review-visualization",
    "title": "Multiple regression principles",
    "section": "Exercise 1: Review visualization",
    "text": "Exercise 1: Review visualization\nLet‚Äôs prepare to build a model of rides by windspeed (quantitative) and weekend status (categorical) by construcing a visualization of the relationship between these 3 variables.\nInclude linear smoothing lines so that we can see a representation of the regression model that we‚Äôll build in the next exercise.\n\n# Plot of rides vs windspeed & weekend\n# HINT: Start with a plot of rides vs windspeed",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-2-review-model",
    "href": "activities/09_mlr_principles.html#exercise-2-review-model",
    "title": "Multiple regression principles",
    "section": "Exercise 2: Review model",
    "text": "Exercise 2: Review model\nLet‚Äôs build the model:\n\nbike_model_1 &lt;- lm(rides ~ windspeed + weekend, data = bikes)\ncoef(summary(bike_model_1))\n\nThe model formula thus is:\nE[rides | windspeed, weekendTRUE] = 4738.38 - 63.97 * windspeed - 925.16 * weekendTRUE\nThis model formula is represented by 2 lines, one corresponding to weekends and the other to weekdays. Specify the corresponding sub-model formulas:\nweekdays: rides = ___ - ___ windspeed\nweekends: rides = ___ - ___ windspeed",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-3-review-coefficient-interpretation",
    "href": "activities/09_mlr_principles.html#exercise-3-review-coefficient-interpretation",
    "title": "Multiple regression principles",
    "section": "Exercise 3: Review coefficient interpretation",
    "text": "Exercise 3: Review coefficient interpretation\n\nThe intercept coefficient, 4738.38, represents the intercept of the sub-model for weekdays, the reference category. What‚Äôs its contextual interpretation?\nThe windspeed coefficient, -63.97, represents the shared slope of the weekend and weekday sub-models. What‚Äôs its contextual interpretation?\nThe weekendTRUE coefficient, -925.16, represents the change in intercept for the weekend vs weekday sub-model. What‚Äôs its contextual interpretation?",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-4-2-categorical-predictors-visualization",
    "href": "activities/09_mlr_principles.html#exercise-4-2-categorical-predictors-visualization",
    "title": "Multiple regression principles",
    "section": "Exercise 4: 2 categorical predictors ‚Äì visualization",
    "text": "Exercise 4: 2 categorical predictors ‚Äì visualization\nThus far, we‚Äôve explored a couple examples of multiple regression models that have 2 predictors, 1 quantitative and 1 categorical.\nSo what happens when both predictors are categorical?!\nTo this end, let‚Äôs model rides by weekend status and season.\nThe below code plots rides vs season.\n\n# rides vs season\nbikes %&gt;% \n    ggplot(aes(y = rides, x = season)) + \n    geom_boxplot()\n\nModify this code to also include information about weekend. (Hint: Remember the visualization principle that additional categorical predictors require some sort of grouping mechanism / mechanism that distinguishes between the 2 groups.)\n\n# rides vs season AND weekend\nbikes %&gt;%\n    ggplot(aes(y = rides, x = season, ___ = ___)) +\n    geom_boxplot()",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-5-follow-up",
    "href": "activities/09_mlr_principles.html#exercise-5-follow-up",
    "title": "Multiple regression principles",
    "section": "Exercise 5: follow-up",
    "text": "Exercise 5: follow-up\n\nDescribe (in words) the relationship of ridership with season and weekend status.\nA model of rides by season alone would be represented by only 4 expected outcomes, 1 for each season. Considering this and the plot above, how do you anticipate a model of rides by season and weekend status will be represented?\n\n2 lines, 1 for each weekend status\n8 lines, 1 for each possible combination of season and weekend\n2 expected outcomes, 1 for each weekend status\n8 expected outcomes, 1 for each possible combination of season and weekend",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-6-2-categorical-predictors-build-the-model",
    "href": "activities/09_mlr_principles.html#exercise-6-2-categorical-predictors-build-the-model",
    "title": "Multiple regression principles",
    "section": "Exercise 6: 2 categorical predictors ‚Äì build the model",
    "text": "Exercise 6: 2 categorical predictors ‚Äì build the model\nLet‚Äôs build the multiple regression model of rides vs season and weekend:\n\nbike_model_2 &lt;- lm(rides ~ weekend + season, bikes)\ncoef(summary(bike_model_2))\n\nThus the model formula is\nE[rides | weekend, season] = 4260.45 - 912.33 weekendTRUE - 116.38 seasonspring + 438.44 seasonsummer - 1719.06 seasonwinter\n\nUse this model to predict the ridership on the following days:\n\n\n# a fall weekday\n4260.45 - 912.33*___ - 116.38*___  + 438.44*___ - 1719.06*___\n\n# a winter weekday    \n4260.45 - 912.33*___ - 116.38*___  + 438.44*___ - 1719.06*___\n\n# a fall weekend day        \n4260.45 - 912.33*___ - 116.38*___  + 438.44*___ - 1719.06*___\n\n# a winter weekend day\n4260.45 - 912.33*___ - 116.38*___  + 438.44*___ - 1719.06*___\n\n\nWe only made 4 predictions here. How many possible predictions does this model produce? Is this consistent with your intuition in the previous exercise?",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-7-2-categorical-predictors-interpret-the-model",
    "href": "activities/09_mlr_principles.html#exercise-7-2-categorical-predictors-interpret-the-model",
    "title": "Multiple regression principles",
    "section": "Exercise 7: 2 categorical predictors ‚Äì interpret the model",
    "text": "Exercise 7: 2 categorical predictors ‚Äì interpret the model\nUse your above predictions and visualization to fill in the below interpretations of the model coefficients.\nHint: What is the consequence of plugging in 0 or 1 for the different weekend and season categories?\n\nInterpreting 4260: We expect there to be 4260 riders on (weekdays/weekends) during the (fall/spring/summer/winter).\nInterpreting -912: In any season, we expect there to be 912 (more/fewer) riders on weekends than on ___.\nInterpreting -1719: On both weekdays and weekends, we expect there to be 1719 (more/fewer) riders in winter than in ___.",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-8-2-quantitative-predictors-visualization",
    "href": "activities/09_mlr_principles.html#exercise-8-2-quantitative-predictors-visualization",
    "title": "Multiple regression principles",
    "section": "Exercise 8: 2 quantitative predictors ‚Äì visualization",
    "text": "Exercise 8: 2 quantitative predictors ‚Äì visualization\nNext, consider the relationship between rides and 2 quantitative predictors: windspeed and temp_feel.\nCheck out 2 plots of this relationship below.\nThese reflect the visualization principle that quantitative variables require some sort of numerical scaling mechanism ‚Äì rides and windspeed get numerical axes, and temp_feel gets size and color scales.\n\nModify the code below to recreate these 2 plots.\n\n# Left plot\nbikes %&gt;%\n    ggplot(aes(y = rides, x = windspeed, ___ = ___)) +\n    geom_point()\n\n# Right plot\nbikes %&gt;%\n    ggplot(aes(y = rides, x = windspeed, ___ = ___)) +\n    geom_point()\n\nWith some new code in the plotly (not ggplot2) package, we can also plot these 3 quantitative variables in 3D, each getting their own axis!\nFirst, enter install.packages(\"plotly\") in the console (not Rmd).\nThen play around with this code:\n\nlibrary(plotly)\n\nplot_ly(data = bikes,\n        z = ~rides, y = ~windspeed, x = ~temp_feel, \n        type = \"scatter3d\", mode = \"markers\", size = 0.5)",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-9-follow-up",
    "href": "activities/09_mlr_principles.html#exercise-9-follow-up",
    "title": "Multiple regression principles",
    "section": "Exercise 9: follow-up",
    "text": "Exercise 9: follow-up\n\nDescribe (in words) the relationship of ridership with windspeed & temperature.\nA model of rides by windspeed alone would be represented by 1 line. How do you anticipate a model of rides by windspeed and temp_feel status will be represented? HINT: Think about the 3D scatterplot.",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-10-2-quantitative-predictors-modeling",
    "href": "activities/09_mlr_principles.html#exercise-10-2-quantitative-predictors-modeling",
    "title": "Multiple regression principles",
    "section": "Exercise 10: 2 quantitative predictors ‚Äì modeling",
    "text": "Exercise 10: 2 quantitative predictors ‚Äì modeling\nLet‚Äôs build the multiple regression model of rides vs windspeed and temp_feel:\n\nbike_model_3 &lt;- lm(rides ~ windspeed + temp_feel, bikes)\ncoef(summary(bike_model_3))\n\nThus the model formula is\nE[rides | windspeed, temp_feel] = -24.06 - 36.54 windspeed + 55.52 temp_feel\nThis is the formula of a plane, drawn here:\n\n\nInterpret the intercept coefficient, -24.06, in context.\nInterpret the windspeed coefficient, -36.54, in context. HINT: This is the slope of the red lines in the right plot, drawn at 3 arbitrary temperatures (40, 60, and 90 degrees).\nInterpret the temp_feel coefficient, 55.52, in context. HINT: This is the slope of the red lines in the left plot, drawn at 3 arbitrary wind speeds (5, 20, and 30 mph).",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-11-which-is-best",
    "href": "activities/09_mlr_principles.html#exercise-11-which-is-best",
    "title": "Multiple regression principles",
    "section": "Exercise 11: Which is best?",
    "text": "Exercise 11: Which is best?\nWe‚Äôve now observed 3 different models of ridership, each having 2 predictors. The R-squared values of these models, along with those of the simple linear regression models with each predictor alone, are summarized below.\n\n\n\nmodel\npredictors\nR-squared\n\n\n\n\nbike_model_1\nwindspeed & weekend\n0.119\n\n\nbike_model_2\nweekend & season\n0.349\n\n\nbike_model_3\nwindspeed & temp_feel\n0.310\n\n\nbike_model_4\nwindspeed\n0.047\n\n\nbike_model_5\ntemp_feel\n0.296\n\n\nbike_model_6\nweekend\n0.074\n\n\nbike_model_7\nseason\n0.279\n\n\n\n\nWhich model does the best job of explaining the variability in ridership from day to day?\nIf you could only pick one predictor, which would it be?\nWhat happens to R-squared when we add a second predictor to our model, and why does this make sense? For example, how does the R-squared for model 1 (with both windspeed and weekend) compare to those of model 4 (only windspeed) and model 6 (only weekend)?\nAre 2 predictors always better than 1? Provide evidence and explain why this makes sense.",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-12-principles-of-interpretation",
    "href": "activities/09_mlr_principles.html#exercise-12-principles-of-interpretation",
    "title": "Multiple regression principles",
    "section": "Exercise 12: Principles of interpretation",
    "text": "Exercise 12: Principles of interpretation\nThese exercises have revealed some principles behind interpreting model coefficients, summarized below.\nReview and confirm that these make sense.\n\nPrinciples of interpretation\nConsider a multiple linear regression model:\n\\[E[y | x_1, x_2, ..., x_p] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p\\]\nWe can interpret the coefficients as follows:\n\n\\(\\beta_0\\) (‚Äúbeta 0‚Äù) is the y-intercept. It describes the typical value of \\(y\\) when \\(x_1, x_2,..., x_k\\) are all 0, ie. when all quantitative predictors are set to 0 and the categorical predictors are set to their reference levels.\n\\(\\beta_i\\) (‚Äúbeta i‚Äù) is the coefficient of \\(x_i\\).\n\nIf \\(x_i\\) is quantitative, \\(\\beta_i\\) describes the typical change in \\(y\\) per 1-unit increase in \\(x_i\\) while at a fixed set of the other \\(x\\).\n\nIf \\(x_i\\) represents a category of a categorical variable, \\(\\beta_i\\) describes the typical change in \\(y\\) when we move to this category from the reference category while at a fixed set of the other \\(x\\).",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-13-practice-1",
    "href": "activities/09_mlr_principles.html#exercise-13-practice-1",
    "title": "Multiple regression principles",
    "section": "Exercise 13: Practice 1",
    "text": "Exercise 13: Practice 1\nConsider the relationship of rides vs weekend and weather_cat.\n\nConstruct a visualization of this relationship.\n\nConstruct a model of this relationship.\n\nInterpret the first 3 model coefficients.",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-14-practice-2",
    "href": "activities/09_mlr_principles.html#exercise-14-practice-2",
    "title": "Multiple regression principles",
    "section": "Exercise 14: Practice 2",
    "text": "Exercise 14: Practice 2\nConsider the relationship of rides vs temp_feel and humidity.\n\nConstruct a visualization of this relationship.\n\nConstruct a model of this relationship.\n\nInterpret the first 3 model coefficients.",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-15-practice-3",
    "href": "activities/09_mlr_principles.html#exercise-15-practice-3",
    "title": "Multiple regression principles",
    "section": "Exercise 15: Practice 3",
    "text": "Exercise 15: Practice 3\nConsider the relationship of rides vs temp_feel and weather_cat.\n\nConstruct a visualization of this relationship.\n\nConstruct a model of this relationship.\n\nInterpret the first 3 model coefficients.",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-16-challenge",
    "href": "activities/09_mlr_principles.html#exercise-16-challenge",
    "title": "Multiple regression principles",
    "section": "Exercise 16: CHALLENGE",
    "text": "Exercise 16: CHALLENGE\nWe‚Äôve explored models with 2 predictors. What about 3 predictors?! Consider the relationship of rides vs temp_feel, humidity, AND weekend.\n\nConstruct a visualization of this relationship.\n\nConstruct a model of this relationship.\n\nInterpret each model coefficient.\n\nIf you had to draw the shape of the model trend, what would it look like: a line, parallel lines, a plane, parallel planes?",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-1-review-visualization-1",
    "href": "activities/09_mlr_principles.html#exercise-1-review-visualization-1",
    "title": "Multiple regression principles",
    "section": "Exercise 1: Review visualization",
    "text": "Exercise 1: Review visualization\n\nbikes %&gt;% \n    ggplot(aes(y = rides, x = windspeed, color = weekend)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-2-review-model-1",
    "href": "activities/09_mlr_principles.html#exercise-2-review-model-1",
    "title": "Multiple regression principles",
    "section": "Exercise 2: Review model",
    "text": "Exercise 2: Review model\n\nbike_model_1 &lt;- lm(rides ~ windspeed + weekend, data = bikes)\ncoef(summary(bike_model_1))\n\n              Estimate Std. Error   t value      Pr(&gt;|t|)\n(Intercept) 4738.38053  147.53653 32.116659 1.208405e-141\nwindspeed    -63.97072   10.45274 -6.119997  1.528443e-09\nweekendTRUE -925.15701  119.86330 -7.718434  3.891082e-14\n\n\nweekdays: rides = 4738.38 - 63.97 windspeed\nweekends: rides = 4738.38 - 63.97 windspeed - 925.16 = 3813.22 - 63.97 windspeed",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-3-review-coefficient-interpretation-1",
    "href": "activities/09_mlr_principles.html#exercise-3-review-coefficient-interpretation-1",
    "title": "Multiple regression principles",
    "section": "Exercise 3: Review coefficient interpretation",
    "text": "Exercise 3: Review coefficient interpretation\n\nOn average, there are roughly 4738 riders on weekdays with 0 windspeed.\n\nOn both weekends and weekdays, every 1 mph increase in windspeed is associated with 64 fewer riders on average.\nHolding day type fixed (weekend or weekday), every 1 mph increase in windspeed is associated with 64 fewer riders on average.\n\n\nAt any windspeed, weekend days have an average of 925 fewer riders than weekdays.\nHolding windspeed fixed, weekend days have an average of 925 fewer riders than weekdays.",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-4-2-categorical-predictors-visualization-1",
    "href": "activities/09_mlr_principles.html#exercise-4-2-categorical-predictors-visualization-1",
    "title": "Multiple regression principles",
    "section": "Exercise 4: 2 categorical predictors ‚Äì visualization",
    "text": "Exercise 4: 2 categorical predictors ‚Äì visualization\n\nbikes %&gt;% \n    ggplot(aes(y = rides, x = season, fill = weekend)) + \n    geom_boxplot()",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-5-follow-up-1",
    "href": "activities/09_mlr_principles.html#exercise-5-follow-up-1",
    "title": "Multiple regression principles",
    "section": "Exercise 5: follow-up",
    "text": "Exercise 5: follow-up\n\nIn every season, ridership tends to be lower on weekends. Across weekend status, ridership tends to be highest in summer and lowest in winter.\n8 expected outcomes",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-6-2-categorical-predictors-build-the-model-1",
    "href": "activities/09_mlr_principles.html#exercise-6-2-categorical-predictors-build-the-model-1",
    "title": "Multiple regression principles",
    "section": "Exercise 6: 2 categorical predictors ‚Äì build the model",
    "text": "Exercise 6: 2 categorical predictors ‚Äì build the model\n\nbike_model_2 &lt;- lm(rides ~ weekend + season, bikes)\ncoef(summary(bike_model_2))\n\n               Estimate Std. Error     t value      Pr(&gt;|t|)\n(Intercept)   4260.4492   99.16363  42.9638294 1.384994e-201\nweekendTRUE   -912.3324  103.23016  -8.8378473  7.298199e-18\nseasonspring  -116.3824  132.76018  -0.8766364  3.809741e-01\nseasonsummer   438.4424  132.06413   3.3199205  9.454177e-04\nseasonwinter -1719.0572  133.30505 -12.8956646  2.081758e-34\n\n\n\n\n\n\n# fall weekday:    \n4260.45 - 912.33*0 - 116.38*0 + 438.44*0 - 1719.06*0\n\n[1] 4260.45\n\n# winter weekday:\n4260.45 - 912.33*0 - 116.38*0 + 438.44*0 - 1719.06*1\n\n[1] 2541.39\n\n# fall weekend:    \n4260.45 - 912.33*1 - 116.38*0 + 438.44*0 - 1719.06*0\n\n[1] 3348.12\n\n# winter weekend:\n4260.45 - 912.33*1 - 116.38*0 + 438.44*0 - 1719.06*1\n\n[1] 1629.06\n\n\n\n8: 2 weekend categories * 4 season categories",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-7-2-categorical-predictors-interpret-the-model-1",
    "href": "activities/09_mlr_principles.html#exercise-7-2-categorical-predictors-interpret-the-model-1",
    "title": "Multiple regression principles",
    "section": "Exercise 7: 2 categorical predictors ‚Äì interpret the model",
    "text": "Exercise 7: 2 categorical predictors ‚Äì interpret the model\n\nOn average there are 4260 riders on weekdays during the fall.\n\nIn any season, there are on oaverage 912 fewer riders on weekends than on weekdays.\nHolding season constant (for a fixed season), there are on average 912 fewer riders on weekends than on weekdays.\n\n\nOn both weekdays and weekends, there are on average 1719 fewer riders in winter than in fall.\nHolding day type constant (weekday vs.¬†weekend), there are on average 1719 fewer riders in winter than in fall.",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-8-2-quantitative-predictors-visualization-1",
    "href": "activities/09_mlr_principles.html#exercise-8-2-quantitative-predictors-visualization-1",
    "title": "Multiple regression principles",
    "section": "Exercise 8: 2 quantitative predictors ‚Äì visualization",
    "text": "Exercise 8: 2 quantitative predictors ‚Äì visualization\n\nbikes %&gt;% \n    ggplot(aes(y = rides, x = windspeed, size = temp_feel)) + \n    geom_point() \n\n\n\n\n\n\n\nbikes %&gt;% \n    ggplot(aes(y = rides, x = windspeed, color = temp_feel)) + \n    geom_point() \n\n\n\n\n\n\n\n\n\nlibrary(plotly)\n\nplot_ly(data = bikes,\n        z = ~rides, y = ~windspeed, x = ~temp_feel, \n        type = \"scatter3d\", mode = \"markers\", size = 0.5)",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-9-follow-up-1",
    "href": "activities/09_mlr_principles.html#exercise-9-follow-up-1",
    "title": "Multiple regression principles",
    "section": "Exercise 9: follow-up",
    "text": "Exercise 9: follow-up\nRidership tends to increase with temperature (no matter the windspeed) and decrease with windspeed (no matter the temperature).",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-10-2-quantitative-predictors-modeling-1",
    "href": "activities/09_mlr_principles.html#exercise-10-2-quantitative-predictors-modeling-1",
    "title": "Multiple regression principles",
    "section": "Exercise 10: 2 quantitative predictors ‚Äì modeling",
    "text": "Exercise 10: 2 quantitative predictors ‚Äì modeling\n\nbike_model_3 &lt;- lm(rides ~ windspeed + temp_feel, bikes)\ncoef(summary(bike_model_3))\n\n             Estimate Std. Error     t value     Pr(&gt;|t|)\n(Intercept) -24.06464 299.303032 -0.08040225 9.359394e-01\nwindspeed   -36.54372   9.408116 -3.88427585 1.119805e-04\ntemp_feel    55.51648   3.330739 16.66791759 4.436963e-53\n\n\n\n-24.06 = typical ridership on days with 0 windspeed and a 0 degree temperature. (This doesn‚Äôt make contextual sense, but indicates where the plane ‚Äúlives‚Äù in space!)\n\nNo matter the temperature, every 1 mph increase in windspeed is associated with an average of 37 fewer riders.\nHolding temperature constant, every 1 mph increase in windspeed is associated with an average of 37 fewer riders.\n\n\nNo matter the windspeed, every 1 degree Fahrenheit increase in temperature is associated with an average of 56 more riders.\nHolding windspeed constant, every 1 degree Fahrenheit increase in temperature is associated with an average of 56 more riders.",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-11-which-is-best-1",
    "href": "activities/09_mlr_principles.html#exercise-11-which-is-best-1",
    "title": "Multiple regression principles",
    "section": "Exercise 11: Which is best?",
    "text": "Exercise 11: Which is best?\n\nmodel 2\ntemperature\nR-squared increases (our model is stronger when we include another predictor)\nnope. model 1 (with windspeed and weekend) has a lower R-squared than model 5 (with only temperature)",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-13-practice-1-1",
    "href": "activities/09_mlr_principles.html#exercise-13-practice-1-1",
    "title": "Multiple regression principles",
    "section": "Exercise 13: Practice 1",
    "text": "Exercise 13: Practice 1\n\nbikes %&gt;% \n    ggplot(aes(y = rides, x = weekend, fill = weather_cat)) + \n    geom_boxplot()\n\n\n\n\n\n\n\nnew_model_1 &lt;- lm(rides ~ weekend + weather_cat, bikes)\ncoef(summary(new_model_1))\n\n                    Estimate Std. Error   t value      Pr(&gt;|t|)\n(Intercept)        4211.8741   75.54724 55.751529 9.461947e-265\nweekendTRUE        -982.2106  117.24719 -8.377264  2.786301e-16\nweather_catcateg2  -608.8640  113.00211 -5.388077  9.628947e-08\nweather_catcateg3 -2360.2049  319.71640 -7.382183  4.270163e-13\n\n\n\nThe average ridership on a weekday with nice weather (categ1) is 4212 rides.\nOn days with the same weather (holding weather category constant), the average ridership on a weekend is 982 rides less than on weekdays.\nOn days of the same type (weekend vs weekday) (holding day type constant), the average ridership when the weather is ‚Äúdreary‚Äù (categ2) is 609 rides less than when the weather is nice.",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-14-practice-2-1",
    "href": "activities/09_mlr_principles.html#exercise-14-practice-2-1",
    "title": "Multiple regression principles",
    "section": "Exercise 14: Practice 2",
    "text": "Exercise 14: Practice 2\n\nggplot(bikes, aes(y = rides, x = temp_feel, color = humidity)) + \n        geom_point()\n\n\n\n\n\n\n\nnew_model_2 &lt;- lm(rides ~ temp_feel + humidity, bikes)\ncoef(summary(new_model_2))\n\n               Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)   315.83704 303.777334  1.039699 2.988249e-01\ntemp_feel      60.43316   3.272315 18.468015 9.451345e-63\nhumidity    -1868.99356 336.963661 -5.546573 4.078901e-08\n\n\n\nIt doesn‚Äôt really make sense to interpret the intercept ‚Äì we didn‚Äôt see any days that were 0 degrees with 0 humidity.\nOn days with the same humidity (holding humidity constant), each 1 degree increase in temperature is associated with a decrease in 60 rides on average.\nOn days with the same temperature (holding temperature constant), each 1 unit increase in humidity (which corresponds to a change from 0 to 100 percent!) is associated with a decrease of 1867 rides on average. On a more sensible scale, a 10% increase in humidity is linked with an average of 1867 * 0.1 = 187 fewer rides.",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-15-practice-3-1",
    "href": "activities/09_mlr_principles.html#exercise-15-practice-3-1",
    "title": "Multiple regression principles",
    "section": "Exercise 15: Practice 3",
    "text": "Exercise 15: Practice 3\n\nnew_model_3 &lt;- lm(rides ~ temp_feel + weather_cat, bikes)\ncoef(summary(new_model_3))\n\n                     Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)        -288.68840 251.264383 -1.148943 2.509574e-01\ntemp_feel            55.30133   3.215495 17.198387 7.082670e-56\nweather_catcateg2  -386.42241 100.187725 -3.856984 1.249775e-04\nweather_catcateg3 -1919.01375 283.022420 -6.780430 2.481218e-11\n\nbikes %&gt;% \n    ggplot(aes(y = rides, x = temp_feel, color = weather_cat)) + \n    geom_point() + \n    geom_line(aes(y = new_model_3$fitted.values), linewidth = 1.5)\n\n\n\n\n\n\n\n\n\nIt doesn‚Äôt really make sense to interpret the intercept ‚Äì we didn‚Äôt see any days that were 0 degrees.\nOn days with the same weather (holding weather category constant), every 1 degree increase in temperature is associated with an average increase of 55 rides.\nOn days with the same temperature (holding temperature constant), average ridership is 386 rides lower on a dreary weather day (categ2) than a nice weather day (categ1).",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/09_mlr_principles.html#exercise-16-challenge-1",
    "href": "activities/09_mlr_principles.html#exercise-16-challenge-1",
    "title": "Multiple regression principles",
    "section": "Exercise 16: CHALLENGE",
    "text": "Exercise 16: CHALLENGE\n\nbikes %&gt;% \n    ggplot(aes(y = rides, x = temp_feel, color = weekend, size = humidity)) + \n    geom_point()\n\n\n\n\n\n\n\nnew_model_4 &lt;- lm(rides ~ temp_feel + humidity + weekend, bikes)\ncoef(summary(new_model_4))\n\n               Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)   668.60236 292.181063  2.288315 2.240530e-02\ntemp_feel      59.36751   3.119256 19.032585 7.626695e-66\nhumidity    -1906.43437 320.982938 -5.939364 4.433789e-09\nweekendTRUE  -869.05771 100.057822 -8.685555 2.471050e-17\n\n\n\nIt doesn‚Äôt really make sense to interpret the intercept ‚Äì we didn‚Äôt see any days that were 0 degrees.\nOn days with the same humidity and time of week (holding humidity and day type constant), each 1 degree increase in temperature is associated with an increase of 59 rides on average.\nOn days with the same temperature and time of week (holding temperature and day type constant), each 0.1 point increase (10%) in humidity levels is associated with 1906*0.1 = 190.6 rides fewer rides on average.\nOn days with the same temperature and humidity (holding temperature and humidity constant), average ridership is 869 rides lower on weekends than on weekdays.\n\nThis model would look like 2 parallel planes, one for weekends and one for weekdays! Why? rides vs temp_feel (quant) would be a line. Adding in humidity (quant) would turn the model into a plane. Adding in weekend (cat) would split this one plane into two unique planes, one for each weekend category!",
    "crumbs": [
      "Multiple regression principles"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html",
    "href": "activities/07_slr_cat_predictor.html",
    "title": "Simple linear regression: categorical predictor",
    "section": "",
    "text": "You can download a template file to work with here.\nFile organization: Save this file in the ‚ÄúActivities‚Äù subfolder of your ‚ÄúSTAT155‚Äù folder.\n\n\n\nBy the end of this lesson, you should be able to:\n\nWrite a model formula for a simple linear regression model with a categorical predictor using indicator variables\nInterpret the intercept and slope coefficients in a simple linear regression model with a categorical predictor\n\n\n\n\nBefore class you should have read and/or watched:\n\nReading: Section 3.9 in the STAT 155 Notes only up through section 3.9.1 Indicator Variables\nVideos:\n\nSimple linear regression: categorical predictor (slides)\nR Code for Categorical Predictors\n\n\n\n\n\nSo far we‚Äôve focused on modeling a quantitative response/outcome as a function of a single quantitative predictor.\n\\[\nE[\\text{price} \\mid \\text{weight\\_grams}] = \\beta_0 + \\beta_1 \\text{weight\\_grams}\n\\]\nBut when building models to explain why the outcome varies, we will likely have categorical predictors too. For example, the quality of the product might be encoded as ‚Äúlow‚Äù or ‚Äúhigh‚Äù.\nUsing a model like\n\\[\nE[\\text{price} \\mid \\text{quality}] = \\beta_0 + \\beta_1 \\text{quality}\n\\]\ndoesn‚Äôt make much sense because what does \\(\\beta_1\\,\\text{quality}\\) mean? How can we multiply a number by a word/category label? We can‚Äôt!\n\nFor this reason, we make use of indicator variables. An indicator variable is a binary variable that only takes the values 0 or 1.\n\nExample: turn the quality variable into a qualityHigh indicator variable:\n\nqualityHigh = 1 if the quality of the product is ‚Äúhigh‚Äù\nqualityHigh = 0 if the quality of the product is ‚Äúlow‚Äù\n\n\nQuestion: Alternatively, we could have turned quality variable into a qualityLow indicator variable. What would the qualityHigh and qualityLow indicator variables look like for the following 4 cases?\n\n\n\nprice\nquality\nqualityHigh\nqualityLow\n\n\n\n\n10\nlow\n\n\n\n\n20\nhigh\n\n\n\n\n11\nlow\n\n\n\n\n21\nhigh\n\n\n\n\n\n\nThe model statement in terms of indicator variables would be:\n\\[\nE[\\text{price} \\mid \\text{quality}] = \\beta_0 + \\beta_1 \\text{qualityHigh}\n\\]\n\nNote: for brevity, we can write ‚Äúquality‚Äù instead of ‚ÄúqualityHigh‚Äù in the expected value (E[‚Ä¶] part) because we‚Äôre using this to indicate that price depends on the quality variable as a whole rather than the specific indicator variables chosen.\n\nIf we listed all of the indicator variables inside the expected value, this would get long when a categorical variable has 3 or more categories\n\n\nQuestion: In terms of the \\(\\beta\\) coefficients, what is the average price of a low quality product? What about a high quality product?\n\nFor categorical variables with more than 2 categories, we need to create multiple indicator variables. To do this we pick one category to be the reference category and create indicator variables for the other categories.\nFor example, if quality had ‚Äúlow‚Äù, ‚Äúmedium‚Äù, and ‚Äúhigh‚Äù categories, we could pick ‚Äúlow‚Äù to be the reference category (conventional for the ‚Äúfirst‚Äù category to be the reference) and create qualityMedium and qualityHigh indicator variables:\n\\[\nE[\\text{price} \\mid \\text{quality}] = \\beta_0 + \\beta_1 \\text{qualityMedium} + \\beta_2 \\text{qualityHigh}\n\\]\nQuestion: In terms of the \\(\\beta\\) coefficients, what is the average price of a low quality product? A medium quality product? A high quality product?",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#learning-goals",
    "href": "activities/07_slr_cat_predictor.html#learning-goals",
    "title": "Simple linear regression: categorical predictor",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nWrite a model formula for a simple linear regression model with a categorical predictor using indicator variables\nInterpret the intercept and slope coefficients in a simple linear regression model with a categorical predictor",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#readings-and-videos",
    "href": "activities/07_slr_cat_predictor.html#readings-and-videos",
    "title": "Simple linear regression: categorical predictor",
    "section": "",
    "text": "Before class you should have read and/or watched:\n\nReading: Section 3.9 in the STAT 155 Notes only up through section 3.9.1 Indicator Variables\nVideos:\n\nSimple linear regression: categorical predictor (slides)\nR Code for Categorical Predictors",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#review",
    "href": "activities/07_slr_cat_predictor.html#review",
    "title": "Simple linear regression: categorical predictor",
    "section": "",
    "text": "So far we‚Äôve focused on modeling a quantitative response/outcome as a function of a single quantitative predictor.\n\\[\nE[\\text{price} \\mid \\text{weight\\_grams}] = \\beta_0 + \\beta_1 \\text{weight\\_grams}\n\\]\nBut when building models to explain why the outcome varies, we will likely have categorical predictors too. For example, the quality of the product might be encoded as ‚Äúlow‚Äù or ‚Äúhigh‚Äù.\nUsing a model like\n\\[\nE[\\text{price} \\mid \\text{quality}] = \\beta_0 + \\beta_1 \\text{quality}\n\\]\ndoesn‚Äôt make much sense because what does \\(\\beta_1\\,\\text{quality}\\) mean? How can we multiply a number by a word/category label? We can‚Äôt!\n\nFor this reason, we make use of indicator variables. An indicator variable is a binary variable that only takes the values 0 or 1.\n\nExample: turn the quality variable into a qualityHigh indicator variable:\n\nqualityHigh = 1 if the quality of the product is ‚Äúhigh‚Äù\nqualityHigh = 0 if the quality of the product is ‚Äúlow‚Äù\n\n\nQuestion: Alternatively, we could have turned quality variable into a qualityLow indicator variable. What would the qualityHigh and qualityLow indicator variables look like for the following 4 cases?\n\n\n\nprice\nquality\nqualityHigh\nqualityLow\n\n\n\n\n10\nlow\n\n\n\n\n20\nhigh\n\n\n\n\n11\nlow\n\n\n\n\n21\nhigh\n\n\n\n\n\n\nThe model statement in terms of indicator variables would be:\n\\[\nE[\\text{price} \\mid \\text{quality}] = \\beta_0 + \\beta_1 \\text{qualityHigh}\n\\]\n\nNote: for brevity, we can write ‚Äúquality‚Äù instead of ‚ÄúqualityHigh‚Äù in the expected value (E[‚Ä¶] part) because we‚Äôre using this to indicate that price depends on the quality variable as a whole rather than the specific indicator variables chosen.\n\nIf we listed all of the indicator variables inside the expected value, this would get long when a categorical variable has 3 or more categories\n\n\nQuestion: In terms of the \\(\\beta\\) coefficients, what is the average price of a low quality product? What about a high quality product?\n\nFor categorical variables with more than 2 categories, we need to create multiple indicator variables. To do this we pick one category to be the reference category and create indicator variables for the other categories.\nFor example, if quality had ‚Äúlow‚Äù, ‚Äúmedium‚Äù, and ‚Äúhigh‚Äù categories, we could pick ‚Äúlow‚Äù to be the reference category (conventional for the ‚Äúfirst‚Äù category to be the reference) and create qualityMedium and qualityHigh indicator variables:\n\\[\nE[\\text{price} \\mid \\text{quality}] = \\beta_0 + \\beta_1 \\text{qualityMedium} + \\beta_2 \\text{qualityHigh}\n\\]\nQuestion: In terms of the \\(\\beta\\) coefficients, what is the average price of a low quality product? A medium quality product? A high quality product?",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#exercise-1-get-to-know-the-data",
    "href": "activities/07_slr_cat_predictor.html#exercise-1-get-to-know-the-data",
    "title": "Simple linear regression: categorical predictor",
    "section": "Exercise 1: Get to know the data",
    "text": "Exercise 1: Get to know the data\nWrite R code to answer the following:\n\nHow many cases and variables do we have? What does a case represent?\nWhat do the first few rows of the data look like?\nConstruct and interpret two different visualizations of the price variable.\nConstruct and interpret a visualization of the cut variable.",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#exercise-2-visualizations",
    "href": "activities/07_slr_cat_predictor.html#exercise-2-visualizations",
    "title": "Simple linear regression: categorical predictor",
    "section": "Exercise 2: Visualizations",
    "text": "Exercise 2: Visualizations\nStart by visualizing this relationship of interest, that between price and cut.\n\nThe appropriate plot depends upon the type of variables we‚Äôre plotting. When exploring the relationship between a quantitative response (ridership) and a quantitative predictor (temperature), a scatterplot was an effective choice. After running the code below, explain why a scatterplot is not effective for exploring the relationship between ridership and our categorical cut predictor.\n\n\n# Try a scatterplot\nggplot(diamonds, aes(y = price, x = cut)) + \n    geom_point()\n\n\nSeparately run each chunk below, with two plots. Comment (#) on what changes in the code / output.\n\n\n# Univariate boxplot\nggplot(diamonds, aes(y = price)) + \n    geom_boxplot()\n\n\n# ???\nggplot(diamonds, aes(y = price, x = cut)) + \n    geom_boxplot()\n\n\n# Univariate density plot\nggplot(diamonds, aes(x = price)) + \n    geom_density()\n\n\n# ???\nggplot(diamonds, aes(x = price, color = cut)) + \n    geom_density()\n\n\n# Univariate histogram\nggplot(diamonds, aes(x = price)) + \n    geom_histogram()\n\n\n# ???\nggplot(diamonds, aes(x = price)) + \n    geom_histogram() + \n    facet_wrap(~ cut)\n\n\nDo you notice anything interesting about the relationship between price and cut? What do you think might be happening here?",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#exercise-3-numerical-summaries",
    "href": "activities/07_slr_cat_predictor.html#exercise-3-numerical-summaries",
    "title": "Simple linear regression: categorical predictor",
    "section": "Exercise 3: Numerical summaries",
    "text": "Exercise 3: Numerical summaries\nLet‚Äôs follow up our plots with some numerical summaries.\n\nTo warm up, first calculate the mean price across all diamonds.\n\n\ndiamonds %&gt;% \n    ___(mean(___))\n\n\nTo summarize the trends we observed in the grouped plots above, we can calculate the mean price for each type of cut. This requires the inclusion of the group_by() function:\n\n\n# Calculate mean price by cut\ndiamonds %&gt;% \n    group_by(cut) %&gt;% \n    ___(mean(___))\n\n\nExamine the group mean measurements, and make sure that you can match these numbers up with what you see in the plots.\nBased on the results above, we can see that, on average, diamonds with a ‚ÄúFair‚Äù cut tend to cost more than higher-quality cuts. Let‚Äôs construct a new variable named cutFair, using on the following criteria:\n\n\ncutFair = 1 if the diamond is of Fair cut\ncutFair = 0 otherwise (any other value of cut (Good, Very Good, Premium, Ideal))\n\n\ndiamonds &lt;- diamonds %&gt;%\n  mutate(cutFair=ifelse(___ == \"Fair\", __, __))\n\nVariables like cutFair that are coded as 0/1 to numerically indicate if a categorical variable is at a particular state are known as an indicator variable. You will sometimes see these referred to as a ‚Äúbinary variable‚Äù or ‚Äúdichotomous variable‚Äù; you may also encounter the term ‚Äúdummy variable‚Äù in older statistical literature.\n\nNow, let‚Äôs calculate the group means based on the new cutFair indicator variable:\n\n\ndiamonds %&gt;% \n    group_by(cutFair) %&gt;% \n    summarize(mean(price))",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#exercise-4-modeling-trend-using-a-categorical-predictor-with-exactly-2-categories",
    "href": "activities/07_slr_cat_predictor.html#exercise-4-modeling-trend-using-a-categorical-predictor-with-exactly-2-categories",
    "title": "Simple linear regression: categorical predictor",
    "section": "Exercise 4: Modeling trend using a categorical predictor with exactly 2 categories",
    "text": "Exercise 4: Modeling trend using a categorical predictor with exactly 2 categories\nNext, let‚Äôs model the trend in the relationship between the cutFair and price variables using a simple linear regression model:\n\n# Construct the model\ndiamond_mod0 &lt;- lm(price ~ cutFair, data = diamonds)\n\n# Summarize the model\ncoef(summary(diamond_mod0))\n\nCompare these results to the output of exercise 3e. What do you notice? How do you interpret the intercept and cutFair coefficient terms from this model?\n\nyour answer here",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#exercise-5-modeling-trend-using-a-categorical-predictor-with-2-categories",
    "href": "activities/07_slr_cat_predictor.html#exercise-5-modeling-trend-using-a-categorical-predictor-with-2-categories",
    "title": "Simple linear regression: categorical predictor",
    "section": "Exercise 5: Modeling trend using a categorical predictor with >2 categories",
    "text": "Exercise 5: Modeling trend using a categorical predictor with &gt;2 categories\nUsing a single binary predictor like the cutFair indicator variable is useful when there are two clearly delineated categories. However, the cut variable actually contains 5 categories! Because we‚Äôve collapsed all non-Fair classifications into a single category (i.e.¬†cutFair = 0), the model above can‚Äôt tell us anything about the difference in expected price between, say, Premium and Ideal cuts. The good news is that it is very straightforward to model categorical predictors with &gt;2 categories. We can do this by using the cut variable as our predictor:\n\n# Construct the model\ndiamond_mod &lt;- lm(price ~ cut, data = diamonds)\n\n# Summarize the model\ncoef(summary(diamond_mod))\n\n\nEven though we specified a single predictor variable in the model, we are seeing 4 coefficient estimates‚Äìwhy do you think this is the case?\n\n\nyour answer here\n\n\nNOTE: We see 4 indicator variables (for Good, Very Good, Premium, and Ideal), but we do not see cutFair in the model output. This is because Fair is the reference level of the cut variable (it‚Äôs first alphabetically). You‚Äôll see below that it is, indeed, still in the model. You‚Äôll also see why the term ‚Äúreference level‚Äù makes sense!\n\n\nAfter examining the summary table output from the code chunk above, complete the model formula:\n\n\n\nE[price | cut] = ___ +/- ___ cutGood +/- ___ cutVery Good +/- ___ cutPremium +/- ___ cutIdeal",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#exercise-6-making-sense-of-the-model",
    "href": "activities/07_slr_cat_predictor.html#exercise-6-making-sense-of-the-model",
    "title": "Simple linear regression: categorical predictor",
    "section": "Exercise 6: Making sense of the model",
    "text": "Exercise 6: Making sense of the model\nRecall our model: E[price | cut] = 4358.7578 - 429.8933 cutGood - 376.9979 cutVery Good + 225.4999 cutPremium - 901.2158 cutIdeal\n\nUse the model formula to calculate the expected/typical price for diamonds of Good cut.\nSimilarly, calculate the expected/typical price for diamonds of Fair cut.\nRe-examine these 2 calculations. Where have you seen these numbers before?!",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#exercise-7-interpreting-coefficients",
    "href": "activities/07_slr_cat_predictor.html#exercise-7-interpreting-coefficients",
    "title": "Simple linear regression: categorical predictor",
    "section": "Exercise 7: Interpreting coefficients",
    "text": "Exercise 7: Interpreting coefficients\nRecall that our model formula is not a formula for a line. Thus we can‚Äôt interpret the coefficients as ‚Äúslopes‚Äù as we have before. Taking this into account and reflecting upon your calculations above‚Ä¶\n\nInterpret the intercept coefficient (4358.7578) in terms of the data context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases.\nInterpret the cutGood and cutVery Good coefficients (-429.8933 and -376.9979) in terms of the data context. Hint: where did you use these value in the prediction calculations above?",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#exercise-8-modeling-choices-challenge",
    "href": "activities/07_slr_cat_predictor.html#exercise-8-modeling-choices-challenge",
    "title": "Simple linear regression: categorical predictor",
    "section": "Exercise 8: Modeling choices (CHALLENGE)",
    "text": "Exercise 8: Modeling choices (CHALLENGE)\nWhy do we fit this model in this way (using 4 indicator variables cutGood, cutVery Good, cutPremium, cutIdeal)? Instead, suppose that we created a single variable cutCat that gave each category a numerical value: 0 for Fair, 1 for Good, 2 for Very Good, 3 for Premium, and 4 for Ideal.\nHow would this change things? What are the pros and cons of each approach?",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#reflection",
    "href": "activities/07_slr_cat_predictor.html#reflection",
    "title": "Simple linear regression: categorical predictor",
    "section": "Reflection",
    "text": "Reflection\nThrough the exercises above, you learned how to build and interpret models that incorporate a categorical predictor variable. For the benefit of your future self, summarize how one can interpret the coefficients for a categorical predictor.\n\nResponse: Put your response here.",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#render-your-work",
    "href": "activities/07_slr_cat_predictor.html#render-your-work",
    "title": "Simple linear regression: categorical predictor",
    "section": "Render your work",
    "text": "Render your work\n\nClick the ‚ÄúRender‚Äù button in the menu bar for this pane (blue arrow pointing right). This will create an HTML file containing all of the directions, code, and responses from this activity. A preview of the HTML will appear in the browser.\nScroll through and inspect the document to check that your work translated to the HTML format correctly.\nClose the browser tab.\nGo to the ‚ÄúBackground Jobs‚Äù pane in RStudio and click the Stop button to end the rendering process.\nNavigate to your ‚ÄúActivities‚Äù subfolder within your ‚ÄúSTAT155‚Äù folder and locate the HTML file. You can open it again in your browser to double check.",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#exercise-9-diamond-color",
    "href": "activities/07_slr_cat_predictor.html#exercise-9-diamond-color",
    "title": "Simple linear regression: categorical predictor",
    "section": "Exercise 9: Diamond color",
    "text": "Exercise 9: Diamond color\nConsider modeling price by color.\n\nBefore creating a visualization that shows the relationship between price and color, write down what you expect the plot to look like. Then construct and interpret an apporpriate plot.\nCompute the average price for each color.\nFit an appropriate linear model with lm() and display a short summary of the model.\nWrite out the model formula from the above summary.\nWhich color is the reference level? How can you tell from the model summary?\nInterpret the intercept and two other coefficients from the model in terms of the data context.",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#exercise-10-diamond-clarity",
    "href": "activities/07_slr_cat_predictor.html#exercise-10-diamond-clarity",
    "title": "Simple linear regression: categorical predictor",
    "section": "Exercise 10: Diamond clarity",
    "text": "Exercise 10: Diamond clarity\nIf you want more practice, repeat the steps from Exercise 8 for the clarity variable.",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#exercise-1-get-to-know-the-data-1",
    "href": "activities/07_slr_cat_predictor.html#exercise-1-get-to-know-the-data-1",
    "title": "Simple linear regression: categorical predictor",
    "section": "Exercise 1: Get to know the data",
    "text": "Exercise 1: Get to know the data\n\nA case represents a single diamond.\nThe distribution of price is right skewed with considerable high outliers. The right skew is evidenced by the mean price ($3932) being much higher than the median price ($2401).\nMost diamonds in this data are of Good cut or better. Ideal cut diamonds are the most common with each succesive grade being the next most common.\n\n\ndim(diamonds)\n\n[1] 53940    10\n\nhead(diamonds)\n\n# A tibble: 6 √ó 10\n  carat cut       color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n\n# Visualize price (outcome variable)\nggplot(diamonds, aes(x = price)) +\n    geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(diamonds, aes(y = price)) +\n    geom_boxplot()\n\n\n\n\n\n\n\ndiamonds %&gt;%\n    summarize(mean(price), median(price), sd(price))\n\n# A tibble: 1 √ó 3\n  `mean(price)` `median(price)` `sd(price)`\n          &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;\n1         3933.            2401       3989.\n\n# Visualize cut (predictor variable)\nggplot(diamonds, aes(x = cut)) +\n    geom_bar()\n\n\n\n\n\n\n\ndiamonds %&gt;% \n    count(cut)\n\n# A tibble: 5 √ó 2\n  cut           n\n  &lt;fct&gt;     &lt;int&gt;\n1 Fair       1610\n2 Good       4906\n3 Very Good 12082\n4 Premium   13791\n5 Ideal     21551",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#exercise-2-visualizations-1",
    "href": "activities/07_slr_cat_predictor.html#exercise-2-visualizations-1",
    "title": "Simple linear regression: categorical predictor",
    "section": "Exercise 2: Visualizations",
    "text": "Exercise 2: Visualizations\nStart by visualizing this relationship of interest, that between price and cut.\n\nWe just don‚Äôt see anything clearly on a scatterplot. With the small number of unique values of the predictor variable, all of the points are bunched up on each other.\n\n\n# Try a scatterplot\nggplot(diamonds, aes(y = price, x = cut)) + \n    geom_point()\n\n\n\n\n\n\n\n\n\nSeparately run each chunk below, with two plots. Comment (#) on what changes in the code / output.\n\n\n# Univariate boxplot\nggplot(diamonds, aes(y = price)) + \n    geom_boxplot()\n\n\n\n\n\n\n\n\n\n# Separate boxes by category\nggplot(diamonds, aes(y = price, x = cut)) + \n    geom_boxplot()\n\n\n\n\n\n\n\n\n\n# Univariate density plot\nggplot(diamonds, aes(x = price)) + \n    geom_density()\n\n\n\n\n\n\n\n\n\n# Separate density plots by category\nggplot(diamonds, aes(x = price, color = cut)) + \n    geom_density()\n\n\n\n\n\n\n\n\n\n# Univariate histogram\nggplot(diamonds, aes(x = price)) + \n    geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n# Separate histograms by category\nggplot(diamonds, aes(x = price)) + \n    geom_histogram() + \n    facet_wrap(~ cut)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nThe relationship between price and cut seems to be opposite what we would expect. The diamonds with the best cut (Ideal) have the lowest average price, and the ones with the worst cut (Fair) are woth the most. Maybe something else is different between the diamonds with the best and worst cuts‚Ä¶size maybe?",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#exercise-3-numerical-summaries-1",
    "href": "activities/07_slr_cat_predictor.html#exercise-3-numerical-summaries-1",
    "title": "Simple linear regression: categorical predictor",
    "section": "Exercise 3: Numerical summaries",
    "text": "Exercise 3: Numerical summaries\nLet‚Äôs follow up our plots with some numerical summaries.\n\nMean price across all diamonds:\n\n\ndiamonds %&gt;% \n    summarize(mean(price))\n\n# A tibble: 1 √ó 1\n  `mean(price)`\n          &lt;dbl&gt;\n1         3933.\n\n\n\nMean price for each type of cut:\n\n\ndiamonds %&gt;% \n    group_by(cut) %&gt;% \n    summarize(mean(price))\n\n# A tibble: 5 √ó 2\n  cut       `mean(price)`\n  &lt;fct&gt;             &lt;dbl&gt;\n1 Fair              4359.\n2 Good              3929.\n3 Very Good         3982.\n4 Premium           4584.\n5 Ideal             3458.\n\n\n\nGroup means should reflect what you see in the plots (easiest to see in the boxplots)\nCreate our new cutFair variable:\n\n\ndiamonds &lt;- diamonds %&gt;%\n  mutate(cutFair=ifelse(cut == \"Fair\", 1, 0))\n\n\nCalculate the group means based on this new variable\n\n\ndiamonds %&gt;% \n    group_by(cutFair) %&gt;% \n    summarize(mean(price))\n\n# A tibble: 2 √ó 2\n  cutFair `mean(price)`\n    &lt;dbl&gt;         &lt;dbl&gt;\n1       0         3920.\n2       1         4359.",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#exercise-4-modeling-trend-using-a-categorical-predictor-with-exactly-2-categories-1",
    "href": "activities/07_slr_cat_predictor.html#exercise-4-modeling-trend-using-a-categorical-predictor-with-exactly-2-categories-1",
    "title": "Simple linear regression: categorical predictor",
    "section": "Exercise 4: Modeling trend using a categorical predictor with exactly 2 categories",
    "text": "Exercise 4: Modeling trend using a categorical predictor with exactly 2 categories\n\n# Construct the model\ndiamond_mod0 &lt;- lm(price ~ cutFair, data = diamonds)\n\n# Summarize the model\ncoef(summary(diamond_mod0))\n\n             Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept) 3919.6946    17.4367 224.795616 0.000000e+00\ncutFair      439.0632   100.9269   4.350309 1.361951e-05\n\n\nThe intercept is the expected value (mean) of the price for all diamonds with a cut quality that isn‚Äôt Fair (Good, Very Good, Premium, or Ideal, i.e.¬†when cutFair = 0)‚Äìthe same as we saw in exercise 3e.\n\nWhen we add the intercept and coefficient for cutFair, we get 3919.69 + 439.06 = 4358.75‚Äìthis is the mean price for all diamonds with a Fair cut quality that we saw in exercise 3e! Therefore, the coefficient of cutFair (439.06) is interpreted as the difference between the mean value of diamonds with a Fair cut quality and the mean value of diamonds with a higher cut quality.",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#exercise-5-modeling-trend-using-a-categorical-predictor-with-2-categories-1",
    "href": "activities/07_slr_cat_predictor.html#exercise-5-modeling-trend-using-a-categorical-predictor-with-2-categories-1",
    "title": "Simple linear regression: categorical predictor",
    "section": "Exercise 5: Modeling trend using a categorical predictor with >2 categories",
    "text": "Exercise 5: Modeling trend using a categorical predictor with &gt;2 categories\n\n# Construct the model\ndiamond_mod &lt;- lm(price ~ cut, data = diamonds)\n\n# Summarize the model\ncoef(summary(diamond_mod))\n\n              Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  4358.7578   98.78795 44.122361 0.000000e+00\ncutGood      -429.8933  113.84940 -3.775982 1.595493e-04\ncutVery Good -376.9979  105.16422 -3.584849 3.375707e-04\ncutPremium    225.4999  104.39521  2.160060 3.077240e-02\ncutIdeal     -901.2158  102.41155 -8.799943 1.408406e-18\n\n\n\nWe are seeing 4 coefficient estimates because each category is being assigned to a separate indicator variable‚ÄìcutGood = 1 when cut == \"Good\" and 0 otherwise, cutVery Good = 1 when `cut == ‚ÄúVery Good‚Äù and 0 otherwise, and so on.\nE[price | cut] = 4358.7578 - 429.8933 cutGood - 376.9979 cutVery Good + 225.4999 cutPremium - 901.2158 cutIdeal",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#exercise-6-making-sense-of-the-model-1",
    "href": "activities/07_slr_cat_predictor.html#exercise-6-making-sense-of-the-model-1",
    "title": "Simple linear regression: categorical predictor",
    "section": "Exercise 6: Making sense of the model",
    "text": "Exercise 6: Making sense of the model\n\nExpected/typical price for diamonds of Good cut:\n\nE[price | cut] = 4358.7578 - 429.8933 * 1 - 376.9979 * 0 + 225.4999 * 0 - 901.2158 * 0 = 4358.7578 - 429.8933 = $3928.865\n\npredict(diamond_mod, newdata = data.frame(cut = \"Good\"))\n\n       1 \n3928.864 \n\n\n\nExpected/typical price for diamonds of Fair cut:\n\nE[price | cut] = 4358.7578 - 429.8933 * 0 - 376.9979 * 0 + 225.4999 * 0 - 901.2158 * 0 = $4358.7578\n\npredict(diamond_mod, newdata = data.frame(cut = \"Fair\"))\n\n       1 \n4358.758 \n\n\n\nThese come from our group mean calculations in Exercise 3b! The predicted value for diamonds of Fair cut is also the same as what we obtained using the SLR model in exercise 4 with only a single cutFair indicator variable.",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#exercise-7-interpreting-coefficients-1",
    "href": "activities/07_slr_cat_predictor.html#exercise-7-interpreting-coefficients-1",
    "title": "Simple linear regression: categorical predictor",
    "section": "Exercise 7: Interpreting coefficients",
    "text": "Exercise 7: Interpreting coefficients\nRecall that our model formula is not a formula for a line. Thus we can‚Äôt interpret the coefficients as ‚Äúslopes‚Äù as we have before. Taking this into account and reflecting upon your calculations above‚Ä¶\n\nThe average price of a Fair cut diamonds is $4358.7578.\n\nInterpretation of cutGood coefficient: On average, Good cut diamonds are worth $429.89 less than Fair cut diamonds.\nInterpretation of cutVery Good coefficient: On average, Very Good cut diamonds are worth $377.00 less than Fair cut diamonds.",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#exercise-8-modeling-choices-challenge-1",
    "href": "activities/07_slr_cat_predictor.html#exercise-8-modeling-choices-challenge-1",
    "title": "Simple linear regression: categorical predictor",
    "section": "Exercise 8: Modeling choices (CHALLENGE)",
    "text": "Exercise 8: Modeling choices (CHALLENGE)\nWhy do we fit this model in this way (using 4 indicator variables cutGood, cutVery Good, cutPremium, cutIdeal)? Instead, suppose that we created a single variable cutCat that gave each category a numerical value: 0 for Fair, 1 for Good, 2 for Very Good, 3 for Premium, and 4 for Ideal.\n\nIf we used 0-4 instead of creating indicator variables, we would be constraining the change from 0 to 1, from 1 to 2, etc. to always be of the same magnitude. That is, a 1 unit change in the cut variable would always have the same change in price in our model.\nUsing separate indicator variables allows the difference between subsequent categories to be different, which allows our model to be a bit more nuanced. It is possible to take nuance too far though. For example, in our previous investigations of bikeshare data, we modeled ridership versus temperature. We treated temperature as a quantitative predictor. Imagine if we had created an indicator variable for each unique temperature in the data‚Äîthat would be so many variables! Having so many variables creates a very complex model which can be hard to make sense of. (These ideas are addressed further in STAT 253: Statistical Machine Learning!)",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#exercise-9-diamond-color-1",
    "href": "activities/07_slr_cat_predictor.html#exercise-9-diamond-color-1",
    "title": "Simple linear regression: categorical predictor",
    "section": "Exercise 9: Diamond color",
    "text": "Exercise 9: Diamond color\nConsider modeling price by color.\n\nThe best color diamonds are J, and worst are D. We would expect D diamonds to have the lowest price and increase steadily as we get to J. This is in fact what we see in the boxplots.\n\n\nggplot(diamonds, aes(x = color, y = price)) +\n    geom_boxplot()\n\n\n\n\n\n\n\ndiamonds %&gt;% \n    group_by(color) %&gt;% \n    summarize(mean(price))\n\n# A tibble: 7 √ó 2\n  color `mean(price)`\n  &lt;fct&gt;         &lt;dbl&gt;\n1 D             3170.\n2 E             3077.\n3 F             3725.\n4 G             3999.\n5 H             4487.\n6 I             5092.\n7 J             5324.\n\n\n\nWe fit a linear model and obtain the model formula: E[price | color] = 3169.95 - 93.20 colorE + 554.93 colorF + 829.18 colorG + 1316.72 colorH + 1921.92 colorI + 2153.86 colorJ\n\n\ndiamond_mod2 &lt;- lm(price ~ color, data = diamonds)\n\ncoef(summary(diamond_mod2))\n\n              Estimate Std. Error   t value      Pr(&gt;|t|)\n(Intercept) 3169.95410   47.70694 66.446391  0.000000e+00\ncolorE       -93.20162   62.04724 -1.502107  1.330752e-01\ncolorF       554.93230   62.38527  8.895246  6.004834e-19\ncolorG       829.18158   60.34470 13.740751  6.836340e-43\ncolorH      1316.71510   64.28715 20.481777  7.074714e-93\ncolorI      1921.92086   71.55308 26.860072 7.078041e-158\ncolorJ      2153.86392   88.13203 24.439060 3.414906e-131\n\n\n\nColor D is the reference level because we don‚Äôt see its indicator variable in the model output.\nInterpretation of the intercept: Diamonds with D color cost $3169.95 on average.\nInterpretation of the colorE coefficient: Diamonds with E color cost $93.20 less than D color diamonds on average.\nInterpretation of the colorF coefficient: Diamonds with F color cost $554.93 more than D color diamonds on average.",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/07_slr_cat_predictor.html#exercise-10-diamond-clarity-1",
    "href": "activities/07_slr_cat_predictor.html#exercise-10-diamond-clarity-1",
    "title": "Simple linear regression: categorical predictor",
    "section": "Exercise 10: Diamond clarity",
    "text": "Exercise 10: Diamond clarity\nWe see the unexpected result that diamonds of better clarity (VS1 and higher) have lower average prices. In fact the best clarity diamonds (VVS1 and IF) have the lowest average prices. What might be going on? What if the most clear diamonds were also quite small‚Ä¶\n\nggplot(diamonds, aes(x = clarity, y = price)) +\n    geom_boxplot()\n\n\n\n\n\n\n\ndiamonds %&gt;% \n    group_by(clarity) %&gt;% \n    summarize(mean(price))\n\n# A tibble: 8 √ó 2\n  clarity `mean(price)`\n  &lt;fct&gt;           &lt;dbl&gt;\n1 I1              3924.\n2 SI2             5063.\n3 SI1             3996.\n4 VS2             3925.\n5 VS1             3839.\n6 VVS2            3284.\n7 VVS1            2523.\n8 IF              2865.\n\ndiamond_mod3 &lt;- lm(price ~ clarity, data = diamonds)\n\ncoef(summary(diamond_mod3))\n\n                 Estimate Std. Error      t value      Pr(&gt;|t|)\n(Intercept)  3924.1686910   144.5619 27.145247517 3.513547e-161\nclaritySI2   1138.8599147   150.2746  7.578526239  3.550711e-14\nclaritySI1     71.8324571   148.6049  0.483378837  6.288287e-01\nclarityVS2      0.8207037   148.8672  0.005512992  9.956013e-01\nclarityVS1    -84.7132999   150.9746 -0.561109670  5.747251e-01\nclarityVVS2  -640.4316203   154.7737 -4.137858008  3.510944e-05\nclarityVVS1 -1401.0540535   158.5401 -8.837224284  1.010097e-18\nclarityIF   -1059.3295848   171.8990 -6.162510636  7.210567e-10",
    "crumbs": [
      "Simple linear regression: categorical predictor"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html",
    "href": "activities/05_slr_model_eval.html",
    "title": "Simple linear regression: model evaluation",
    "section": "",
    "text": "You can download a template file to work with here.\nFile organization: Save this file in the ‚ÄúActivities‚Äù subfolder of your ‚ÄúSTAT155‚Äù folder.\n\n\n\nBy the end of this lesson, you should be able to:\n\nUse residual plots to evaluate the correctness of a model\nExplain the rationale for the R-squared metric of model strength\nInterpret the R-squared metric\nThink about ethical implications of modeling by examining the impacts of biased data, power dynamics, the role of categorization, and the role of emotion and lived experience\n\n\n\n\nBefore class you should have read and/or watched:\n\nReading: Sections 1.7, 3.7, and 3.8 in the STAT 155 Notes\nVideos:\n\nModel evaluation: is the model wrong? (slides)\nModel evaluation: is the model strong? (slides)\nModel evaluation: is the model fair? (slides)\nR Code for Evaluating and Using a Linear Model\n\n\n\n\n\n\nGuiding question: How good is a model?\n\nWhen we evaluate the quality of a model, it helps to consider 3 questions:\n\nIs the model wrong?\nIs the model strong?\nIs the model fair?\n\n\nIs the model wrong?\n\nA model is wrong if it is systematically inaccurate in describing reality.\nWe can assess this with residual plots in which we plot:\n\nthe predicted values (fitted values) from the model on the x axis.\n\nWe get the predicted values by taking each case and plugging the predictor into the model to get a prediction.\n\nthe residuals from the model on the y axis\n\nWe get the residuals by computing for each case: observed y - predicted y\n\n\n\nExample: Slide 6 from the Is the model wrong? video\n\nIs the model strong?\n\nA strong model is able to explain a lot about why the outcome/response variable varies.\nWe explored correlation last week.\n\nThe closer the correlation is to -1 or 1 for linearly related variables, the stronger the trend.\nCorrelation is unfortunately limited to looking at the linear relationship between 2 quantitative variables.\n\nThe R-squared (\\(R^2\\)) metric can be used for linear regression models with a quantitative predictor, a categorical predictor, and a mix of multiple quantitative and/or categorical predictors.\n\nRanges from 0 to 1.\nValues closer to 1 indicate a stronger model.\n\\(R^2\\) is interpreted as the proportion of variation in the outcome that is explained by the model.\n\n\nExample: Slide 4 from the Is the model strong? video\n\nIs the model fair?\n\nWhen we make models, we are able to use them by:\n\nUsing the coefficients to gain some understanding about the world.\n\nExample: How much do movie earnings tend to increase as movie budget increases? (The slope coefficient from a model of earnings vs.¬†budget can help us address this.)\n\nMake predictions for individual cases\n\nExample: Predict house price given physical characteristics of the house.\n\n\nWhen assessing fairness we need to think about:\n\nAre we generating explanations of the world that may harm others?\nAre we making predictions from our models that systematically harm certain groups?\nIf the answers to any of the above 2 questions is ‚Äúyes‚Äù, why is that happening? How can we think about the who, what, when, where, why, and how behind the data to dig deeper?\nExample: Lawsuit Alleging Racial Bias in Home Appraisals Is Settled",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#learning-goals",
    "href": "activities/05_slr_model_eval.html#learning-goals",
    "title": "Simple linear regression: model evaluation",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nUse residual plots to evaluate the correctness of a model\nExplain the rationale for the R-squared metric of model strength\nInterpret the R-squared metric\nThink about ethical implications of modeling by examining the impacts of biased data, power dynamics, the role of categorization, and the role of emotion and lived experience",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#readings-and-videos",
    "href": "activities/05_slr_model_eval.html#readings-and-videos",
    "title": "Simple linear regression: model evaluation",
    "section": "",
    "text": "Before class you should have read and/or watched:\n\nReading: Sections 1.7, 3.7, and 3.8 in the STAT 155 Notes\nVideos:\n\nModel evaluation: is the model wrong? (slides)\nModel evaluation: is the model strong? (slides)\nModel evaluation: is the model fair? (slides)\nR Code for Evaluating and Using a Linear Model",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#review",
    "href": "activities/05_slr_model_eval.html#review",
    "title": "Simple linear regression: model evaluation",
    "section": "",
    "text": "Guiding question: How good is a model?\n\nWhen we evaluate the quality of a model, it helps to consider 3 questions:\n\nIs the model wrong?\nIs the model strong?\nIs the model fair?\n\n\nIs the model wrong?\n\nA model is wrong if it is systematically inaccurate in describing reality.\nWe can assess this with residual plots in which we plot:\n\nthe predicted values (fitted values) from the model on the x axis.\n\nWe get the predicted values by taking each case and plugging the predictor into the model to get a prediction.\n\nthe residuals from the model on the y axis\n\nWe get the residuals by computing for each case: observed y - predicted y\n\n\n\nExample: Slide 6 from the Is the model wrong? video\n\nIs the model strong?\n\nA strong model is able to explain a lot about why the outcome/response variable varies.\nWe explored correlation last week.\n\nThe closer the correlation is to -1 or 1 for linearly related variables, the stronger the trend.\nCorrelation is unfortunately limited to looking at the linear relationship between 2 quantitative variables.\n\nThe R-squared (\\(R^2\\)) metric can be used for linear regression models with a quantitative predictor, a categorical predictor, and a mix of multiple quantitative and/or categorical predictors.\n\nRanges from 0 to 1.\nValues closer to 1 indicate a stronger model.\n\\(R^2\\) is interpreted as the proportion of variation in the outcome that is explained by the model.\n\n\nExample: Slide 4 from the Is the model strong? video\n\nIs the model fair?\n\nWhen we make models, we are able to use them by:\n\nUsing the coefficients to gain some understanding about the world.\n\nExample: How much do movie earnings tend to increase as movie budget increases? (The slope coefficient from a model of earnings vs.¬†budget can help us address this.)\n\nMake predictions for individual cases\n\nExample: Predict house price given physical characteristics of the house.\n\n\nWhen assessing fairness we need to think about:\n\nAre we generating explanations of the world that may harm others?\nAre we making predictions from our models that systematically harm certain groups?\nIf the answers to any of the above 2 questions is ‚Äúyes‚Äù, why is that happening? How can we think about the who, what, when, where, why, and how behind the data to dig deeper?\nExample: Lawsuit Alleging Racial Bias in Home Appraisals Is Settled",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#exercise-1-is-the-model-correct",
    "href": "activities/05_slr_model_eval.html#exercise-1-is-the-model-correct",
    "title": "Simple linear regression: model evaluation",
    "section": "Exercise 1: Is the model correct?",
    "text": "Exercise 1: Is the model correct?\nWe previously explored modeling daily ridership among registered users as a function of temperature. Create a plot of this relationship with both a curved and linear trend line. Based on this plot, do you think a linear model is correct?\n\n# Plot riders_registered vs temp_feel with both a curved and linear trend\n___(___, aes(x = ___, y = ___)) + \n    geom___() + \n    geom___(se = FALSE) +\n    geom___(method = \"lm\", se = FALSE, color = \"red\")",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#exercise-2-fixing-the-model",
    "href": "activities/05_slr_model_eval.html#exercise-2-fixing-the-model",
    "title": "Simple linear regression: model evaluation",
    "section": "Exercise 2: Fixing the model",
    "text": "Exercise 2: Fixing the model\nIn this course, we have and will continue to build ‚Äúlinear‚Äù regression models. ‚ÄúLinear‚Äù means we have a linear combination of predictors. It does not mean that the models themselves must look linear! It‚Äôs possible to include ‚Äútransformations‚Äù in our models in order to better match the trend. Below we create a squared version of temperature and visualize the predictions from 2 models: (1) with just temperature as a linear term and (2) with both temperature and squared temperature. (Don‚Äôt worry about the new syntax in geom_smooth().)\nHow does the quality (correctness) of the two models compare?\n\nbikes &lt;- bikes %&gt;% \n    mutate(temp_feel_squared = temp_feel^2)\n\n# Plot the model WITHOUT squared temperature\nggplot(bikes, aes(x = temp_feel, y = riders_registered)) + \n    geom_point() + \n    geom_smooth(se = FALSE) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\")\n# Plot the model WITH squared temperature\nggplot(bikes, aes(x = temp_feel, y = riders_registered)) + \n    geom_point() + \n    geom_smooth(se = FALSE) +\n    geom_smooth(method = \"lm\", formula = y ~ x + I(x^2), se = FALSE, color = \"red\")",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#exercise-3-residual-plots",
    "href": "activities/05_slr_model_eval.html#exercise-3-residual-plots",
    "title": "Simple linear regression: model evaluation",
    "section": "Exercise 3: Residual plots",
    "text": "Exercise 3: Residual plots\nPlotting the residuals vs the predictions (also called ‚Äúfitted values‚Äù) for each case can help us assess how wrong our model is. This will be a particularly important tool when evaluating models with multiple predictors. Construct the residual plots for our two temperature models. Do these suggest that bike_mod1 is wrong? What about bike_mod2? Explain.\nNote: Information about the residuals (.resid) and predictions (.fitted) are stored within our model, thus we start our ggplot() with the model name as opposed to the raw dataset. We will rarely start ggplot() with a model instead of the data.\n\n# Fit a linear model\nbike_mod1 &lt;- lm(riders_registered ~ temp_feel, data = bikes)\n# Fit a quadratic model\nbike_mod2 &lt;- lm(riders_registered ~ temp_feel + temp_feel_squared, data = bikes)\n\n# Check out the residual plot for bike_mod1 (the incorrect model)\nggplot(bike_mod1, aes(x = .fitted, y = .resid)) + \n    geom_point() + \n    geom_hline(yintercept = 0) +\n    geom_smooth(se = FALSE)\n\n# Construct the residual plot for bike_mod2 (the good model)",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#exercise-4-another-example-of-an-incorrect-model",
    "href": "activities/05_slr_model_eval.html#exercise-4-another-example-of-an-incorrect-model",
    "title": "Simple linear regression: model evaluation",
    "section": "Exercise 4: Another example of an incorrect model",
    "text": "Exercise 4: Another example of an incorrect model\nThe Bechdel test is a test applied to films to assess the quality of women‚Äôs presence in the film. We‚Äôll be looking at movies that have had the Bechdel test applied available in the fivethirtyeight R package.\nWe‚Äôll be using the bechdel data. You can view the codebook by entering ?bechdel in the Console.\nLet‚Äôs examine the relationship between international earnings (intgross) and movie budget (budget) for films made in 1997.\n\n# Import the data\nlibrary(fivethirtyeight)\ndata(bechdel)\n\n# Get only 1997 movies\nmovies_1997 &lt;- bechdel %&gt;% \n    filter(year == 1997)\n\n# Construct the model\nbechdel_model &lt;- lm(intgross ~ budget, movies_1997)\n\n\nConstruct two plots:\n\n\n# Scatterplot of earnings and budget with linear and curved trend lines\n\n\n# Residual plot for bechdel_model\n\n\nThese two plots confirm that our model is wrong. What is wrong and how might we fix it?\nIdentify which movie is causing the problem. Hint: filter() according to budget. Also, note that we could, but won‚Äôt, take that film out of the data set and re-build the model.\n\n\n# Many numbers could go in the ___ to successfully identify the outlier\nmovies_1997 %&gt;%\n    filter(budget &gt; ___)",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#exercise-5-is-the-model-strong-developing-r-squared-intuition",
    "href": "activities/05_slr_model_eval.html#exercise-5-is-the-model-strong-developing-r-squared-intuition",
    "title": "Simple linear regression: model evaluation",
    "section": "Exercise 5: Is the model strong? Developing R-squared intuition",
    "text": "Exercise 5: Is the model strong? Developing R-squared intuition\nThe R-squared metric is a way to quantify the strength of a model. It measures how much variation in the outcome/response variable can be explained by the model.\nWhere does R-squared come from? Well, it turns out that we can partition the variance of the observed response values into the variability that‚Äôs explained by the model (the variance of the predictions) and the variability that‚Äôs left unexplained by the model (the variance of the residuals):\n\\[\\text{Var(observed) = Var(predicted) + Var(residuals)}\\]\n‚ÄúGood‚Äù models have residuals that don‚Äôt deviate far from 0. So the smaller the variance in the residuals (thus larger the variance in the predictions), the stronger the model. Take a look at the picture below and write a few sentences addressing the following:\n\nThe two rows of plots show a stronger and a weaker model. Just by looking at the blue trend line and the dispersion of the points about the line, which row corresponds to the stronger model? How can you tell? Which row would you expect to have a higher correlation?\nWhat is different about the variance of the residuals from the first to the second row?\n\n\nPutting this together, the R-squared compares Var(predicted) to Var(response):\n\\[R^2 = \\frac{\\text{variance of predicted values}}{\\text{variance of observed response values}} = 1 - \\frac{\\text{variance of residuals}}{\\text{variance of observed response values}}\\]",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#exercise-6-further-exploring-r-squared",
    "href": "activities/05_slr_model_eval.html#exercise-6-further-exploring-r-squared",
    "title": "Simple linear regression: model evaluation",
    "section": "Exercise 6: Further exploring R-squared",
    "text": "Exercise 6: Further exploring R-squared\nIn this exercise, we‚Äôll look at data from a synthetic dataset called Anscombe‚Äôs quartet. Load the data in as follows, and look at the first few rows:\n\ndata(anscombe)\n\n# Look at the first few rows\n\nThe anscombe data is actually 4 datasets in one: x1 and y1 go together, and so forth. Examine the coefficient estimates (in the ‚ÄúEstimate‚Äù column of the ‚ÄúCoefficients:‚Äù part) and the ‚ÄúMultiple R-squared‚Äù value on the second to last line. What do you notice? How do these models compare?\n\nanscombe_mod1 &lt;- lm(y1 ~ x1, data = anscombe)\nanscombe_mod2 &lt;- lm(y2 ~ x2, data = anscombe)\nanscombe_mod3 &lt;- lm(y3 ~ x3, data = anscombe)\nanscombe_mod4 &lt;- lm(y4 ~ x4, data = anscombe)\n\nsummary(anscombe_mod1)\nsummary(anscombe_mod2)\nsummary(anscombe_mod3)\nsummary(anscombe_mod4)\n\nNow take a look at the following scatterplots of the 4 pairs of variables. What do you notice? What takeaway can we draw from this exercise?\n\nggplot(anscombe, aes(x = x1, y = y1)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\nggplot(anscombe, aes(x = x2, y = y2)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\nggplot(anscombe, aes(x = x3, y = y3)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\nggplot(anscombe, aes(x = x4, y = y4)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#exercise-7-biased-data-biased-results-example-1",
    "href": "activities/05_slr_model_eval.html#exercise-7-biased-data-biased-results-example-1",
    "title": "Simple linear regression: model evaluation",
    "section": "Exercise 7: Biased data, biased results: example 1",
    "text": "Exercise 7: Biased data, biased results: example 1\nDATA ARE NOT NEUTRAL. Data can reflect personal biases, institutional biases, power dynamics, societal biases, the limits of our knowledge, and so on. In turn, biased data can lead to biased analyses. Consider an example.\n\nDo a Google image search for ‚Äústatistics professor.‚Äù What do you observe?\nThese search results are produced by a search algorithm / model. Explain why the data used by this model are not neutral.\nWhat are the potential implications, personal or societal, of the search results produced from this biased data?",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#exercise-8-biased-data-biased-results-example-2",
    "href": "activities/05_slr_model_eval.html#exercise-8-biased-data-biased-results-example-2",
    "title": "Simple linear regression: model evaluation",
    "section": "Exercise 8: Biased data, biased results: example 2",
    "text": "Exercise 8: Biased data, biased results: example 2\nConsider the example of a large company that developed a model / algorithm to review the r√©sum√©s of applicants for software developer & other tech positions. The model then gave each applicant a score indicating their hireability or potential for success at the company. You can think of this model as something like:\n\\[\\text{potential for success } = \\beta_0 + \\beta_1 (\\text{features from the r√©sum√©})\\]\nSkim this Reuter‚Äôs article about the company‚Äôs r√©sum√© model.\n\nExplain why the data used by this model are not neutral.\nWhat are the potential implications, personal or societal, of the results produced from this biased data?",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#exercise-9-rigid-data-collection-systems",
    "href": "activities/05_slr_model_eval.html#exercise-9-rigid-data-collection-systems",
    "title": "Simple linear regression: model evaluation",
    "section": "Exercise 9: Rigid data collection systems",
    "text": "Exercise 9: Rigid data collection systems\nWhen working with categorical variables, we‚Äôve seen that our units of observation fall into neat groups. Reality isn‚Äôt so discrete. For example, check out questions 6 and 9 on page 2 of the 2020 US Census. With your group, discuss the following:\n\nWhat are a couple of issues you see with these questions?\nWhat impact might this type of data collection have on a subsequent analysis of the census responses and the policies it might inform?\nCan you think of a better way to write these questions while still preserving the privacy of respondents?\n\nFOR A DEEPER DISCUSSION: Read Chapter 4 of Data Feminism on ‚ÄúWhat gets counted counts‚Äù.",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#exercise-10-presenting-data-elevating-emotion-and-embodiment",
    "href": "activities/05_slr_model_eval.html#exercise-10-presenting-data-elevating-emotion-and-embodiment",
    "title": "Simple linear regression: model evaluation",
    "section": "Exercise 10: Presenting data: ‚ÄúElevating emotion and embodiment‚Äù",
    "text": "Exercise 10: Presenting data: ‚ÄúElevating emotion and embodiment‚Äù\nNote: The following example highlights work done by W.E.B. Du Bois in the late 1800s / early 1900s. His work uses language common to that time period and addresses the topic of slavery.\nThe types of visualizations we‚Äôve been learning in this course are standard practice, hence widely understood. Yet these standard visualizations can also suppress the lived experiences of people represented in the data, hence can miss the larger point. W.E.B. Du Bois (1868‚Äì1963), a ‚Äúsociologist, socialist, historian, civil rights activist, Pan-Africanist, author, writer, and editor‚Äù1, was a pioneer in elevating emotion and embodiment in data visualization. For the Paris World Fair of 1900, Du Bois and his team of students from Atlanta University presented 60 data visualizations of the Black experience in America, less than 50 years after the abolishment of slavery. To this end, Du Bois noted that ‚ÄúI wanted to set down its aim and method in some outstanding way which would bring my work to notice by the thinking world.‚Äù That is, he wanted to increase the impact of his work by partnering technical visualizations with design that better connects to lived experiences. Check out:\n\nAn article by Allen Hillery (@AlDatavizguy).\nA complete set of the data visualizations provided by Anthony Starks (@ajstarks).\n\nDiscuss your observations. In what ways do you think the W.E.B. Du Bois visualizations might have been more effective at sharing his work than, say, plainer bar charts?\nFOR A DEEPER DISCUSSION AND MORE MODERN EXAMPLES: Read Chapter 3 of Data Feminism on the principle of elevating emotion and embodiment, i.e.¬†the value of ‚Äúmultiple forms of knowledge, including the knowledge that comes from people as living, feeling bodies in the world.‚Äù",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#reflection",
    "href": "activities/05_slr_model_eval.html#reflection",
    "title": "Simple linear regression: model evaluation",
    "section": "Reflection",
    "text": "Reflection\nWhat has stuck with you most in our exploration of model evaluation? Why?\n\nResponse: Put your response here.",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#render-your-work",
    "href": "activities/05_slr_model_eval.html#render-your-work",
    "title": "Simple linear regression: model evaluation",
    "section": "Render your work",
    "text": "Render your work\n\nClick the ‚ÄúRender‚Äù button in the menu bar for this pane (blue arrow pointing right). This will create an HTML file containing all of the directions, code, and responses from this activity. A preview of the HTML will appear in the browser.\nScroll through and inspect the document to check that your work translated to the HTML format correctly.\nClose the browser tab.\nGo to the ‚ÄúBackground Jobs‚Äù pane in RStudio and click the Stop button to end the rendering process.\nNavigate to your ‚ÄúActivities‚Äù subfolder within your ‚ÄúSTAT155‚Äù folder and locate the HTML file. You can open it again in your browser to double check.",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#exercise-1-is-the-model-correct-1",
    "href": "activities/05_slr_model_eval.html#exercise-1-is-the-model-correct-1",
    "title": "Simple linear regression: model evaluation",
    "section": "Exercise 1: Is the model correct?",
    "text": "Exercise 1: Is the model correct?\nThe blue curved trend line shows a clear downward trend around 85 degrees, which contextually makes plenty of sense‚Äîextremely hot days would naturally see less riders. Overall the combination of the upward trend and downward trend makes for a curved relationship that is not captured well by a straight line of best fit.\n\nggplot(bikes, aes(x = temp_feel, y = riders_registered)) + \n    geom_point() + \n    geom_smooth(se = FALSE) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#exercise-2-fixing-the-model-1",
    "href": "activities/05_slr_model_eval.html#exercise-2-fixing-the-model-1",
    "title": "Simple linear regression: model evaluation",
    "section": "Exercise 2: Fixing the model",
    "text": "Exercise 2: Fixing the model\nThe second plot (showing the model with squared temperature) follows the natural curve in the trend better.\n\nbikes &lt;- bikes %&gt;% \n    mutate(temp_feel_squared = temp_feel^2)\n\n# Plot the model WITHOUT squared temperature\nggplot(bikes, aes(x = temp_feel, y = riders_registered)) + \n    geom_point() + \n    geom_smooth(se = FALSE) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Plot the model WITH squared temperature\nggplot(bikes, aes(x = temp_feel, y = riders_registered)) + \n    geom_point() + \n    geom_smooth(se = FALSE) +\n    geom_smooth(method = \"lm\", formula = y ~ x + I(x^2), se = FALSE, color = \"red\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#exercise-3-residual-plots-1",
    "href": "activities/05_slr_model_eval.html#exercise-3-residual-plots-1",
    "title": "Simple linear regression: model evaluation",
    "section": "Exercise 3: Residual plots",
    "text": "Exercise 3: Residual plots\nThe first residual plot (from the model with just a straight line trend) shows a lingering trend in the residuals‚Äîthe blue curve traces the trend in the residuals, and it does not lie flat on the y = 0 line.\nOn the other hand, the second residual plot (from the model which uses a squared term to allow for curvature) shows very little trend in the residuals‚Äîthe blue curve is almost flat on the y = 0 line.\n\n# Fit a linear model\nbike_mod1 &lt;- lm(riders_registered ~ temp_feel, data = bikes)\n# Fit a quadratic model\nbike_mod2 &lt;- lm(riders_registered ~ temp_feel + temp_feel_squared, data = bikes)\n\n# Check out the residual plot for bike_mod1 (the incorrect model)\nggplot(bike_mod1, aes(x = .fitted, y = .resid)) + \n    geom_point() + \n    geom_hline(yintercept = 0) +\n    geom_smooth(se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Construct the residual plot for bike_mod2 (the good model)\nggplot(bike_mod2, aes(x = .fitted, y = .resid)) + \n    geom_point() + \n    geom_hline(yintercept = 0) +\n    geom_smooth(se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#exercise-4-another-example-of-an-incorrect-model-1",
    "href": "activities/05_slr_model_eval.html#exercise-4-another-example-of-an-incorrect-model-1",
    "title": "Simple linear regression: model evaluation",
    "section": "Exercise 4: Another example of an incorrect model",
    "text": "Exercise 4: Another example of an incorrect model\n\n\n\n\n# Scatterplot of earnings and budget with linear and curved trend lines\nggplot(movies_1997, aes(x = budget, y = intgross)) +\n    geom_point() +\n    geom_smooth(se = FALSE) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Residual plot for bechdel_model\nggplot(bechdel_model, aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    geom_hline(yintercept = 0) +\n    geom_smooth(se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nFrom the scatterplot, we can see that there is one movie that is a massive outlier in both budget and earnings, and this outlier is pulling up the trend line that makes the model for ‚Äúregular‚Äù movies that have budgets and earnings in ‚Äúnormal‚Äù ranges.\nThe outlier movie is Titanic:\n\n\n# One of many ways to filter to find the outlier movie!\nmovies_1997 %&gt;% \n    filter(intgross &gt; 2000000000)\n\n# A tibble: 1 √ó 15\n   year imdb      title   test  clean_test binary budget domgross intgross code \n  &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;ord&gt;      &lt;chr&gt;   &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;\n1  1997 tt0120338 Titanic ok    ok         PASS      2e8   6.59e8   2.19e9 1997‚Ä¶\n# ‚Ñπ 5 more variables: budget_2013 &lt;int&gt;, domgross_2013 &lt;dbl&gt;,\n#   intgross_2013 &lt;dbl&gt;, period_code &lt;int&gt;, decade_code &lt;int&gt;",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#exercise-5-is-the-model-strong-developing-r-squared-intuition-1",
    "href": "activities/05_slr_model_eval.html#exercise-5-is-the-model-strong-developing-r-squared-intuition-1",
    "title": "Simple linear regression: model evaluation",
    "section": "Exercise 5: Is the model strong? Developing R-squared intuition",
    "text": "Exercise 5: Is the model strong? Developing R-squared intuition\nThe R-squared metric is a way to quantify the strength of a model. It measures how much variation in the outcome/response variable can be explained by the model.\nWhere does R-squared come from? Well, it turns out that we can partition the variance of the observed response values into the variability that‚Äôs explained by the model (the variance of the predictions) and the variability that‚Äôs left unexplained by the model (the variance of the residuals):\n\\[\\text{Var(observed) = Var(predicted) + Var(residuals)}\\]\n‚ÄúGood‚Äù models have residuals that don‚Äôt deviate far from 0. So the smaller the variance in the residuals (thus larger the variance in the predictions), the stronger the model. Take a look at the picture below and write a few sentences addressing the following:\n\nThe first row corresponds to the weaker model. We can tell because the points are much more dispersed from the trend line than in the second row. Recall that the correlation metric measures how closely clustered points are about a straight line of best fit, so we would expect the correlation to be lower for the first row than the second row.\nThe variance of the residuals is much lower for the second row‚Äîthe residuals are all quite small. This indicates a stronger model.",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#exercise-6-further-exploring-r-squared-1",
    "href": "activities/05_slr_model_eval.html#exercise-6-further-exploring-r-squared-1",
    "title": "Simple linear regression: model evaluation",
    "section": "Exercise 6: Further exploring R-squared",
    "text": "Exercise 6: Further exploring R-squared\nIn this exercise, we‚Äôll look at data from a synthetic dataset called Anscombe‚Äôs quartet. Load the data in as follows, and look at the first few rows:\n\ndata(anscombe)\n\n# Look at the first few rows\nhead(anscombe)\n\n  x1 x2 x3 x4   y1   y2    y3   y4\n1 10 10 10  8 8.04 9.14  7.46 6.58\n2  8  8  8  8 6.95 8.14  6.77 5.76\n3 13 13 13  8 7.58 8.74 12.74 7.71\n4  9  9  9  8 8.81 8.77  7.11 8.84\n5 11 11 11  8 8.33 9.26  7.81 8.47\n6 14 14 14  8 9.96 8.10  8.84 7.04\n\n\nAll of these models have close to the same intercept, slope, and R-squared!\n\nanscombe_mod1 &lt;- lm(y1 ~ x1, data = anscombe)\nanscombe_mod2 &lt;- lm(y2 ~ x2, data = anscombe)\nanscombe_mod3 &lt;- lm(y3 ~ x3, data = anscombe)\nanscombe_mod4 &lt;- lm(y4 ~ x4, data = anscombe)\n\nsummary(anscombe_mod1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nsummary(anscombe_mod2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nsummary(anscombe_mod3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nsummary(anscombe_mod4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\n\nBut when we look at the scatterplots, they all look substantially different, and we would want to approach our modeling differently for each one:\n\nx1 and y1: A linear model seems appropriate for this data.\nx2 and y2: The scatterplot is clearly curved‚Äîa ‚Äúlinear‚Äù regression model with squared terms, for example, would be more appropriate for this data. (We‚Äôll talk more about ways to handle nonlinear relationships soon!)\nx3 and y3: There is a very clear outlier at about x3 = 13 that we would want to dig into to better understand the context. After that investigation, we might consider removing this outlier and refitting the model.\nx4 and y4: There is clearly something strange going on with most of the cases having an x4 value of exactly 8. We would not want to jump straight into modeling. Instead, we should dig deeper to find out more about this data.\n\n\nggplot(anscombe, aes(x = x1, y = y1)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x2, y = y2)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x3, y = y3)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x4, y = y4)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#exercises-7---10",
    "href": "activities/05_slr_model_eval.html#exercises-7---10",
    "title": "Simple linear regression: model evaluation",
    "section": "Exercises 7 - 10",
    "text": "Exercises 7 - 10\nNo solutions for these exercises. These require longer discussions, not discrete answers.",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/05_slr_model_eval.html#footnotes",
    "href": "activities/05_slr_model_eval.html#footnotes",
    "title": "Simple linear regression: model evaluation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/W._E._B._Du_Bois‚Ü©Ô∏é",
    "crumbs": [
      "Simple linear regression: model evaluation"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html",
    "href": "activities/03_slr_introduction.html",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nVisualize and describe the relationship between two quantitative variables using a scatterplot\nWrite R code to create a scatterplot and compute the linear correlation between two quantitative variables\nDescribe/identify weak / strong, and positive / negative correlation from a point cloud\nBuild intuition for fitting lines to quantify the relationship between two quantitative variables\n\n\n\n\nThese readings and videos are for AFTER class.\n\nReading: Sections 2.8, 3.1-3.3, 3.6 in the STAT 155 Notes\nVideos:\n\nSummarizing the Relationships between Two Quantitative Variables (Time: 12:12)\nIntroduction to Linear Models (Time: 10:57)\nMethod of Least Squares (Time: 5:10)\nInterpretation of Intercept and Slope (Time: 11:09)\nR Code for Fitting a Linear Model (Time: 11:07)\n\n\nFile organization: You can download a template file to work with here. Save this file in the ‚ÄúActivities‚Äù subfolder of your ‚ÄúSTAT155‚Äù folder.",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#learning-goals",
    "href": "activities/03_slr_introduction.html#learning-goals",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nVisualize and describe the relationship between two quantitative variables using a scatterplot\nWrite R code to create a scatterplot and compute the linear correlation between two quantitative variables\nDescribe/identify weak / strong, and positive / negative correlation from a point cloud\nBuild intuition for fitting lines to quantify the relationship between two quantitative variables",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#readings-and-videos",
    "href": "activities/03_slr_introduction.html#readings-and-videos",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "",
    "text": "These readings and videos are for AFTER class.\n\nReading: Sections 2.8, 3.1-3.3, 3.6 in the STAT 155 Notes\nVideos:\n\nSummarizing the Relationships between Two Quantitative Variables (Time: 12:12)\nIntroduction to Linear Models (Time: 10:57)\nMethod of Least Squares (Time: 5:10)\nInterpretation of Intercept and Slope (Time: 11:09)\nR Code for Fitting a Linear Model (Time: 11:07)\n\n\nFile organization: You can download a template file to work with here. Save this file in the ‚ÄúActivities‚Äù subfolder of your ‚ÄúSTAT155‚Äù folder.",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#exercise-1-get-to-know-the-data",
    "href": "activities/03_slr_introduction.html#exercise-1-get-to-know-the-data",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "Exercise 1: Get to know the data",
    "text": "Exercise 1: Get to know the data\n\nCreate a new code chunk by clicking the green ‚ÄúC‚Äù button with a green + sign in the top right of the menu bar. In this code chunk, use an appropriate function to look at the first few rows of the data.\nCreate a new code chunk, and use an appropriate function to learn how much data we have (in terms of cases and variables).\nWhat does a case represent?\nNavigate to the FAQ page and read the response to the ‚ÄúHow does this site work? Do you just download results from the federations?‚Äù question. What do you learn about data quality and completeness from this response?",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#exercise-2-mutating-our-data",
    "href": "activities/03_slr_introduction.html#exercise-2-mutating-our-data",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "Exercise 2: Mutating our data",
    "text": "Exercise 2: Mutating our data\nStrength-to-weight ratio (SWR) is defined as TotalKg/BodyweightKg. We can use the mutate() function from the dplyr package to create a new variable in our dataframe for SWR using the following code:\n\n# The %&gt;% is called a \"pipe\" and feeds what comes before it\n# into what comes after (lifts data is \"fed into\" the mutate() function).\n# When creating a new variable, we often reassign the data frame to itself,\n# which updates the existing columns in lifts with the additional \"new\" column(s)\n# in lifts!\nlifts &lt;- lifts %&gt;% \n    mutate(NEW_VARIABLE_NAME = Age/BestSquatKg)\n\nAdapt the example above to create a new variable called SWR, where SWR is defined as TotalKg/BodyweightKg.",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#exercise-3-get-to-know-the-outcomeresponse-variable",
    "href": "activities/03_slr_introduction.html#exercise-3-get-to-know-the-outcomeresponse-variable",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "Exercise 3: Get to know the outcome/response variable",
    "text": "Exercise 3: Get to know the outcome/response variable\nLet‚Äôs get acquainted with the SWR variable.\n\nConstruct an appropriate plot to visualize the distribution of this variable, and compute appropriate numerical summaries.\nWrite a good paragraph interpreting the plot and numerical summaries.",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#exercise-4-data-visualization---two-quantitative-variables",
    "href": "activities/03_slr_introduction.html#exercise-4-data-visualization---two-quantitative-variables",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "Exercise 4: Data visualization - two quantitative variables",
    "text": "Exercise 4: Data visualization - two quantitative variables\nWe‚Äôd like to visualize the relationship between body weight and the strength-to-weight ratio. A scatterplot (or informally, a ‚Äúpoint cloud‚Äù) allows us to do this! The code below creates a scatterplot of body weight vs.¬†SWR using ggplot().\n\n# scatterplot\n\n# The alpha = 0.5 in geom_point() adds transparency to the points\n# to make them easier to see. You can make this smaller for more transparency\nlifts %&gt;%\n    ggplot(aes(x = BodyweightKg, y = SWR)) +\n    geom_point(alpha = 0.5)\n\n\nThis is your first bivariate data visualization (visualization for two variables)! What differences do you notice in the code structure when creating a bivariate visualization, compared to univariate visualizations we‚Äôve worked with before?\nWhat similarities do you notice in the code structure?\nDoes there appear to be some sort of pattern in the structure of the point cloud? Describe it, in no more than three sentences! Comment on the direction of the relationship between the two variables (positive? negative?) and the spread of the points (are they dispersed? close together?).",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#exercise-5-scatterplots---patterns-in-point-clouds",
    "href": "activities/03_slr_introduction.html#exercise-5-scatterplots---patterns-in-point-clouds",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "Exercise 5: Scatterplots - patterns in point clouds",
    "text": "Exercise 5: Scatterplots - patterns in point clouds\nSometimes, it can be easier to see a pattern in a point cloud by adding a smoothing line to our scatterplots. The code below adapts the code in Exercise 4 to do this:\n\n# scatterplot with smoothing line\nlifts %&gt;%\n    ggplot(aes(x = BodyweightKg, y = SWR)) +\n    geom_point(alpha = 0.5) +\n    geom_smooth()\n\n\nLook back at your answer to Exercise 4 (c). Does the smoothing line assist you in seeing a pattern, or change your answer at all? Why or why not?\nBased on the scatterplot with the smoothing line added above, does there appear to be a linear relationship between body weight and SWR (i.e.¬†would a straight line do a decent job at summarizing the relationship between these two variables)? Why or why not?",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#exercise-6-correlation",
    "href": "activities/03_slr_introduction.html#exercise-6-correlation",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "Exercise 6: Correlation",
    "text": "Exercise 6: Correlation\nWe can quantify the linear relationship between two quantitative variables using a numerical summary known as correlation (sometimes known as a ‚Äúcorrelation coefficient‚Äù or ‚ÄúPearson‚Äôs correlation‚Äù). Correlation can range from -1 to 1, where a correlation of 0 indicates that there is no linear relationship between the two quantitative variables.\n\n\n\n\n\n\nCorrelation (underlying math)\n\n\n\n\n\nThe Pearson correlation coefficient, \\(r_{x, y}\\), of \\(x\\) and \\(y\\) is the (almost) average of products of the z-scores of variables \\(x\\) and \\(y\\):\n\\[\nr_{x, y} = \\frac{\\sum z_x z_y}{n - 1}\n\\]\n\n\n\nIn general, we will want to be able to describe (qualitatively) two aspects of correlation:\n\nStrength\n\n\nIs the correlation between x and y strong, or weak, i.e.¬†how closely do the points fit around a line? This has to do with how dispersed our point clouds are.\n\n\nDirection\n\n\nIs the correlation between x and y positive or negative, i.e.¬†does y go ‚Äúup‚Äù when x goes ‚Äúup‚Äù (positive), or does y go ‚Äúdown‚Äù when x goes ‚Äúup‚Äù (negative)?\n\nStronger correlations will be further from 0 (closer to -1 or 1), and positive and negative correlations will have the appropriate respective sign (above or below zero).\n\nRather than a smooth trend line, we can force the line we add to our scatterplots to be linear using geom_smooth(method = 'lm'), as below:\n\n\n# scatterplot with linear trend line\nlifts %&gt;%\n    ggplot(aes(x = BodyweightKg, y = SWR)) +\n    geom_point(alpha = 0.5) +\n    geom_smooth(method = \"lm\")\n\n\nBased on the above scatterplot, how would you describe the correlation between body weight and SWR, in terms of strength and direction?\nMake a guess as to what numerical value the correlation between body weight and SWR will have, based on your response to part (b).",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#exercise-7-computing-correlation-in-r",
    "href": "activities/03_slr_introduction.html#exercise-7-computing-correlation-in-r",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "Exercise 7: Computing correlation in R",
    "text": "Exercise 7: Computing correlation in R\nWe can compute the correlation between body weight and SWR using summarize and cor functions:\n\n# correlation\n\n# Note: the order in which you put your two quantitative variables into the cor\n# function doesn't matter! Try switching them around to confirm this for yourself\n# Because of the missing data, we need to include the use = \"complete.obs\" - otherwise the correlation would be computed as NA\nlifts %&gt;%\n    summarize(cor(SWR, BodyweightKg, use = \"complete.obs\"))\n\nIs the computed correlation close to what you guessed in Exercise 6 part (c)?",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#exercise-8-limitations-of-correlation",
    "href": "activities/03_slr_introduction.html#exercise-8-limitations-of-correlation",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "Exercise 8: Limitations of correlation",
    "text": "Exercise 8: Limitations of correlation\nWe previously noted that correlation was a numerical summary of the linear relationship between two variables. We‚Äôll now go through some examples of relationships between quantitative variables to demonstrate why it is incredibly important to visualize our data in addition to just computing numerical summaries!\nFor this exercise, we‚Äôll be working with the anscombe dataset, which is built in to R. To load this dataset into our environment, we run the following code:\n\n# load anscombe data\ndata(\"anscombe\")\n\nThe anscombe dataset contains four different pairs of quantitative variables:\n\nx1, y1\nx2, y2\nx3, y3\nx4, y4\n\nAdapt the code we used in Exercise 7 to compute the correlation between each of these four pairs of variables, below:\n\n# correlation between x1, y1\n\n# correlation between x2, y2\n\n# correlation between x3, y3\n\n# correlation between x4, y4\n\n\nWhat do you notice about each of these correlations (if the answer to this isn‚Äôt obvious, double-check your code)?\nDescribe these correlations in terms of strength and direction, using only the numerical summary to assist you in your description.\nDraw an example on the whiteboard or at your tables of what you think the point clouds for these pairs of variables might look like. There are only 11 observations, so you can draw all 11 points if you‚Äôd like!\nAdapt the code for scatterplots given previously in this activity to make four distinct scatterplots for each pair of quantitative variables in the anscombe dataset. You do not need to add a smooth trend line or a linear trend line to these plots.\n\n\n# scatterplot: x1, y1\n\n# scatterplot: x2, y2\n\n# scatterplot: x3, y3\n\n# scatterplot: x4, y4\n\n\nBased on the correlations you calculated and scatterplots you made, what is the message of this last exercise as it relates to the limits of correlation?",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#exercise-9-lines-of-best-fit-intuition-for-least-squares",
    "href": "activities/03_slr_introduction.html#exercise-9-lines-of-best-fit-intuition-for-least-squares",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "Exercise 9: Lines of best fit (intuition for least squares)",
    "text": "Exercise 9: Lines of best fit (intuition for least squares)\nIn this activity, we‚Äôve learned how to fit straight lines to data, to help us visualize the relationship between two quantitative variables. So far, ggplot has chosen the line for us. How does it know which line is ‚Äúbest‚Äù, and what does ‚Äúbest‚Äù even mean?\nFor this exercise, we‚Äôll consider the relationship between x1 and y1 in the anscombe dataset. Run the following code, which creates a scatterplot with a fitted line to our data using the function geom_abline:\n\n# scatterplot with a fitted line, whose slope is 0.4 and intercept is 3\nanscombe %&gt;%\n    ggplot(aes(x = x1, y = y1)) +\n    geom_point() +\n    geom_abline(slope = 0.4, intercept = 3, col = \"blue\", size = 1)\n\nDescribe the line that you see. Do you think the line is ‚Äúgood‚Äù? What are you using to define ‚Äúgood‚Äù?\nSome things to think about:\n\nHow many points are above the line?\nHow many points are below the line?\nAre the distances of the points above and below the line roughly similar, or is there meaningful difference?\n\nNow we‚Äôll add another line to our plot. Which line do you think is better suited for this data? Why? Be specific!\n\n# scatterplot with a fitted line, whose slope is 0.4 and intercept is 3\nanscombe %&gt;%\n    ggplot(aes(x = x1, y = y1)) +\n    geom_point() +\n    geom_abline(slope = 0.4, intercept = 3, col = \"blue\", size = 1) +\n    geom_abline(slope = 0.5, intercept = 4, col = \"orange\", size = 1)\n\nIt‚Äôs usually quite simple to note when a line is bad, but more difficult to quantify when a line is a good fit for our data. Consider the following line:\n\n# scatterplot with a fitted line, whose slope is 0.4 and intercept is 3\nanscombe %&gt;%\n    ggplot(aes(x = x1, y = y1)) +\n    geom_point() +\n    geom_abline(slope = -0.5, intercept = 10, col = \"red\", size = 1) \n\nIn the next activity, we‚Äôll formalize the principle of least squares, which will give us one particular definition of a line of best fit that is commonly used in statistics! We‚Äôll take advantage of the vertical distances between each point and the fitted line (residuals), which will help us define (mathematically) a line that best fits our data:\n\n# Before you run this code chunk, you will need to enter \n# the following command in the Console:\n# install.packages(\"broom\")\nlibrary(broom)\nmod &lt;- lm(y1 ~ x1, data = anscombe)\n\naugment(mod) %&gt;%\n    ggplot(aes(x = x1, y = y1)) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    geom_segment(aes(xend = x1, yend = .fitted), col = \"red\") +\n    geom_point()",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#reflection",
    "href": "activities/03_slr_introduction.html#reflection",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "Reflection",
    "text": "Reflection\nMuch of statistics is about making (hopefully) reasonable assumptions in attempt to summarize observed relationships in data. Today we started considering assumptions of linear relationships between quantitative variables.\nReview the learning objectives at the top of this file and today‚Äôs activity. How do you imagine assumptions of linearity might be useful in terms of quantifying relationships between quantitative variables? How do you imagine these assumptions could sometimes fall short, or even be unethical in certain cases?\n\nResponse: Put your response here.",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#exercise-10-correlation-and-extreme-values",
    "href": "activities/03_slr_introduction.html#exercise-10-correlation-and-extreme-values",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "Exercise 10: Correlation and extreme values",
    "text": "Exercise 10: Correlation and extreme values\nIn this exercise, we‚Äôll explore how correlation changes with the addition of extreme values, or observations. We‚Äôll begin by generating a toy dataset called dat with two quantitative variables, x and y. Run the code below to create the dataset.\nwhile not required, recall that you can look up function documentation in R using the ? in front of a function name to figure out what that function is doing!\n\n# create a toy dataset\nset.seed(1234)\nx &lt;- rnorm(100, mean = 5, sd = 2)\ny &lt;- -3 * x + rnorm(100, sd = 4)\ndat &lt;- data.frame(x = x, y = y)\n\n\nMake a scatterplot of x vs.¬†y.\n\n\n# scatterplot\n\n\nBased on your scatterplot, describe the correlation between x and y in terms of strength and direction.\nGuess the correlation (the numerical value) between x and y.\nCompute the correlation between x and y. Was your guess from part (c) close?\n\n\n# correlation\n\n\nSuppose we observe an additional observation with x = 15 and y = -45. We can create a new data frame, dat_new1, that contains this observation in addition to the original ones as follows:\n\n\n# creating dat_new1\nx1 &lt;- c(x, 15)\ny1 &lt;- c(y, -45)\ndat_new1 &lt;- data.frame(x = x1, y = y1)\n\n\nMake a scatterplot of x vs.¬†y for this new data frame, and compute the correlation between x and y. Did your correlation change very much with the addition of this observation? Hypothesize why or why not.\n\n\n# scatterplot\n\n# correlation\n\n\nSuppose instead of our additional observation having values x = 15 and y = -45, we instead observe x = 15 and y = -15. We can create a new data frame, dat_new2, that contains this observation in addition to the original ones as follows:\n\n\n# creating dat_new1\nx2 &lt;- c(x, 15)\ny2 &lt;- c(y, 45)\ndat_new2 &lt;- data.frame(x = x2, y = y2)\n\n\nMake a scatterplot of x vs.¬†y for this new data frame, and compute the correlation between x and y. Did your correlation change very much with the addition of this observation? Hypothesize why or why not.\n\n\n# scatterplot\n\n# correlation\n\n\nWhat do you think the takeaway message is of this exercise?\n\n\nChallenge Add linear trend lines to your scatterplots from parts (f) and (h). Does this give you any additional insight into why the correlations may have changed in different ways with the addition of a new observation?",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#exercise-1-get-to-know-the-data-1",
    "href": "activities/03_slr_introduction.html#exercise-1-get-to-know-the-data-1",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "Exercise 1: Get to know the data",
    "text": "Exercise 1: Get to know the data\n\nUse an appropriate function to look at the first few rows of the data.\n\n\nhead(lifts)\n\n# A tibble: 6 √ó 21\n  Name        Sex   Event Equipment   Age BodyweightKg Best3SquatKg Best3BenchKg\n  &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Natalya Po‚Ä¶ F     D     Raw        37           58.4          NA          NA  \n2 Fatima Rod‚Ä¶ F     SBD   Single-p‚Ä¶  NA           74.8          NA          NA  \n3 Josh Kelley M     SBD   Single-p‚Ä¶  NA           72.4         147.         97.5\n4 Timothy Ca‚Ä¶ M     D     Raw        16           72.9          NA          NA  \n5 M Moynihan  M     B     Raw        NA           67.5          NA         100  \n6 Lucas Wegr‚Ä¶ M     B     Raw        23.5        103.           NA         188. \n# ‚Ñπ 13 more variables: Best3DeadliftKg &lt;dbl&gt;, TotalKg &lt;dbl&gt;, Place &lt;chr&gt;,\n#   Dots &lt;dbl&gt;, Wilks &lt;dbl&gt;, Glossbrenner &lt;dbl&gt;, Goodlift &lt;dbl&gt;, Tested &lt;chr&gt;,\n#   Country &lt;chr&gt;, State &lt;chr&gt;, Date &lt;date&gt;, MeetCountry &lt;chr&gt;, MeetState &lt;chr&gt;\n\n\n\nCreate a new code chunk, and use an appropriate function to learn how much data we have (in terms of cases and variables).\n\n\ndim(lifts)\n\n[1] 100000     21\n\n\n\nA case represents an individual lifter at a single weightlifting competition.\nIt looks like some meets may be missing if they weren‚Äôt detected by the web scraper used by the maintainers of the Open Powerlifting database. They don‚Äôt describe in detail the process used for transferring PDFs of results to their database, so it‚Äôs unclear what errors in transcription might have resulted. Still, it‚Äôs worth taking a moment to appreciate the labor they put into making these results available for passionate powerlifters to explore.",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#exercise-2-mutating-our-data-1",
    "href": "activities/03_slr_introduction.html#exercise-2-mutating-our-data-1",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "Exercise 2: Mutating our data",
    "text": "Exercise 2: Mutating our data\nStrength-to-weight ratio (SWR) is defined as TotalKg/BodyweightKg. We can use the mutate() function from the dplyr package to create a new variable in our dataframe for SWR using the following code:\n\nlifts &lt;- lifts %&gt;% \n    mutate(SWR = TotalKg / BodyweightKg)\n\nAdapt the example above to create a new variable called SWR, where SWR is defined as TotalKg/BodyweightKg.",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#exercise-3-get-to-know-the-outcomeresponse-variable-1",
    "href": "activities/03_slr_introduction.html#exercise-3-get-to-know-the-outcomeresponse-variable-1",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "Exercise 3: Get to know the outcome/response variable",
    "text": "Exercise 3: Get to know the outcome/response variable\nLet‚Äôs get acquainted with the SWR variable.\n\nConstruct an appropriate plot to visualize the distribution of this variable, and compute appropriate numerical summaries.\n\n\nlifts %&gt;%\n    ggplot(aes(SWR)) +\n    geom_histogram(bins = 10, col = \"black\")\n\nWarning: Removed 8752 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\nlifts %&gt;% summarize(mean(SWR, na.rm = TRUE), min(SWR, na.rm = TRUE), max(SWR, na.rm = TRUE), sd(SWR, na.rm = TRUE))\n\n# A tibble: 1 √ó 4\n  `mean(SWR, na.rm = TRUE)` `min(SWR, na.rm = TRUE)` `max(SWR, na.rm = TRUE)`\n                      &lt;dbl&gt;                    &lt;dbl&gt;                    &lt;dbl&gt;\n1                      4.42                    0.183                     12.5\n# ‚Ñπ 1 more variable: `sd(SWR, na.rm = TRUE)` &lt;dbl&gt;\n\n\n\nWrite a good paragraph interpreting the plot and numerical summaries.\n\nStrength-to-weight (SWR) ratio ranges from 0.18 to 12.46, with a mean SWR of 4.4. SWR varies about 2.08 units above and below the mean. We observe that most SWRs appear to be centered between 4 and 7, with a slight right-skew to the data. The distribution of SWRs appears to be unimodal.",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#exercise-4-data-visualization---two-quantitative-variables-1",
    "href": "activities/03_slr_introduction.html#exercise-4-data-visualization---two-quantitative-variables-1",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "Exercise 4: Data visualization - two quantitative variables",
    "text": "Exercise 4: Data visualization - two quantitative variables\nWe‚Äôd like to visualize the relationship between body weight and the strength-to-weight ratio. A scatterplot (or informally, a ‚Äúpoint cloud‚Äù) allows us to do this! The code below creates a scatterplot of body weight vs.¬†SWR using ggplot().\n\n# scatterplot\n\n# The alpha = 0.5 in geom_point() adds transparency to the points\n# to make them easier to see. You can make this smaller for more transparency\nlifts %&gt;%\n    ggplot(aes(x = BodyweightKg, y = SWR)) +\n    geom_point(alpha = 0.5)\n\nWarning: Removed 8752 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\na & b. In our plot aesthetics, we now have two variables listed (an ‚Äúx‚Äù and a ‚Äúy‚Äù) as opposed to just a single variable. The ‚Äúgeom‚Äù for a scatterplot is geom_point. Otherwise, the code structure remains very similar!\n\nIn general, it seems as though higher body weights are associated with lower SWRs. Once body weight (in kg) is greater than 50, the relationship between body weight and SWR appears to be weakly negative, and roughly linear. The points are very dispersed, indicating that there is a good amount of variation in this relationship (hence the term ‚Äúweak‚Äù).",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#exercise-5-scatterplots---patterns-in-point-clouds-1",
    "href": "activities/03_slr_introduction.html#exercise-5-scatterplots---patterns-in-point-clouds-1",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "Exercise 5: Scatterplots - patterns in point clouds",
    "text": "Exercise 5: Scatterplots - patterns in point clouds\nSometimes, it can be easier to see a pattern in a point cloud by adding a smoothing line to our scatterplots. The code below adapts the code in Exercise 4 to do this:\n\n# scatterplot with smoothing line\nlifts %&gt;%\n    ggplot(aes(x = BodyweightKg, y = SWR)) +\n    geom_point(alpha = 0.5) +\n    geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 8752 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 8752 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nThis doesn‚Äôt change my answer much (but it may have changed yours, and that‚Äôs okay!). It does appear as though there is a weakly negative relationship between body weight and SWR, particularly once body weight is above a certain value.\nI would say that yes, a linear relationship here seems reasonable! Even though there is some curvature in the smoothed trend line early on, that is based on very few data points. Those data points with low body weights aren‚Äôt enough to convince me that the relationship couldn‚Äôt be roughly linear between body weight and SWR.",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#exercise-6-correlation-1",
    "href": "activities/03_slr_introduction.html#exercise-6-correlation-1",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "Exercise 6: Correlation",
    "text": "Exercise 6: Correlation\n\nI would describe the correlation between body weight and SWR as weak and negative.\nI‚Äôll guess -0.1, since the line is negative, and the points are very dispersed around the line!",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#exercise-7-computing-correlation-in-r-1",
    "href": "activities/03_slr_introduction.html#exercise-7-computing-correlation-in-r-1",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "Exercise 7: Computing correlation in R",
    "text": "Exercise 7: Computing correlation in R\n\n# correlation\n\n# Note: the order in which you put your two quantitative variables into the cor\n# function doesn't matter! Try switching them around to confirm this for yourself\n# Because of the missing data, we need to include the use = \"complete.obs\" - otherwise the correlation would be computed as NA\nlifts %&gt;%\n    summarize(cor(SWR, BodyweightKg, use = \"complete.obs\"))\n\n# A tibble: 1 √ó 1\n  `cor(SWR, BodyweightKg, use = \"complete.obs\")`\n                                           &lt;dbl&gt;\n1                                        -0.0392\n\n\nSo close to our guess!",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#exercise-8-limitations-of-correlation-1",
    "href": "activities/03_slr_introduction.html#exercise-8-limitations-of-correlation-1",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "Exercise 8: Limitations of correlation",
    "text": "Exercise 8: Limitations of correlation\n\n# correlation between x1, y1\nanscombe %&gt;% summarize(cor(x1, y1))\n\n  cor(x1, y1)\n1   0.8164205\n\n# correlation between x2, y2\nanscombe %&gt;% summarize(cor(x2, y2))\n\n  cor(x2, y2)\n1   0.8162365\n\n# correlation between x3, y3\nanscombe %&gt;% summarize(cor(x3, y3))\n\n  cor(x3, y3)\n1   0.8162867\n\n# correlation between x4, y4\nanscombe %&gt;% summarize(cor(x4, y4))\n\n  cor(x4, y4)\n1   0.8165214\n\n\n\nEach of these correlations are nearly the same!\nEach of these correlations is relatively strong, and positive, since 0.8 is positive and closer to 1 than 0.\n\n\n\n# scatterplot: x1, y1\nanscombe %&gt;%\n    ggplot(aes(x = x1, y = y1)) +\n    geom_point()\n\n\n\n\n\n\n\n# scatterplot: x2, y2\nanscombe %&gt;%\n    ggplot(aes(x = x2, y = y2)) +\n    geom_point()\n\n\n\n\n\n\n\n# scatterplot: x3, y3\nanscombe %&gt;%\n    ggplot(aes(x = x3, y = y3)) +\n    geom_point()\n\n\n\n\n\n\n\n# scatterplot: x4, y4\nanscombe %&gt;%\n    ggplot(aes(x = x4, y = y4)) +\n    geom_point()\n\n\n\n\n\n\n\n\n\nThe message of this exercise is that data visualization is important in addition to numerical summaries! Many different sets of points can have nearly the same correlation, but display very different patterns in point clouds upon closer inspection. Reporting correlation alone is not enough to summarize the relationship between two quantitative variables, and should be accompanied by a scatter plot!",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#exercise-9-lines-of-best-fit-intuition-for-least-squares-1",
    "href": "activities/03_slr_introduction.html#exercise-9-lines-of-best-fit-intuition-for-least-squares-1",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "Exercise 9: Lines of best fit (intuition for least squares)",
    "text": "Exercise 9: Lines of best fit (intuition for least squares)\n\n# scatterplot with a fitted line, whose slope is 0.4 and intercept is 3\nanscombe %&gt;%\n    ggplot(aes(x = x1, y = y1)) +\n    geom_point() +\n    geom_abline(slope = 0.4, intercept = 3, col = \"blue\", size = 1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nDescribe the line that you see. Do you think the line is ‚Äúgood‚Äù? What are you using to define ‚Äúgood‚Äù?\nSome things to think about:\n\nHow many points are above the line?\nHow many points are below the line?\nAre the distances of the points above and below the line roughly similar, or is there meaningful difference?\n\nNow we‚Äôll add another line to our plot. Which line do you think is better suited for this data? Why? Be specific!\n\n# scatterplot with a fitted line, whose slope is 0.4 and intercept is 3\nanscombe %&gt;%\n    ggplot(aes(x = x1, y = y1)) +\n    geom_point() +\n    geom_abline(slope = 0.4, intercept = 3, col = \"blue\", size = 1) +\n    geom_abline(slope = 0.5, intercept = 4, col = \"orange\", size = 1)\n\n\n\n\n\n\n\n\nIt‚Äôs usually quite simple to note when a line is bad, but more difficult to quantify when a line is a good fit for our data. Consider the following line:\n\n# scatterplot with a fitted line, whose slope is 0.4 and intercept is 3\nanscombe %&gt;%\n    ggplot(aes(x = x1, y = y1)) +\n    geom_point() +\n    geom_abline(slope = -0.5, intercept = 10, col = \"red\", size = 1) \n\n\n\n\n\n\n\n\nIn the next activity, we‚Äôll formalize the principle of least squares, which will give us one particular definition of a line of best fit that is commonly used in statistics! We‚Äôll take advantage of the vertical distances between each point and the fitted line (residuals), which will help us define (mathematically) a line that best fits our data:\n\nlibrary(broom)\nmod &lt;- lm(y1 ~ x1, data = anscombe)\n\naugment(mod) %&gt;%\n    ggplot(aes(x = x1, y = y1)) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    geom_segment(aes(xend = x1, yend = .fitted), col = \"red\") +\n    geom_point()\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/03_slr_introduction.html#exercise-10-correlation-and-extreme-values-1",
    "href": "activities/03_slr_introduction.html#exercise-10-correlation-and-extreme-values-1",
    "title": "Simple linear regression: Visualization and Introduction",
    "section": "Exercise 10: Correlation and extreme values",
    "text": "Exercise 10: Correlation and extreme values\n\n# create a toy dataset\nset.seed(1234)\nx &lt;- rnorm(100, mean = 5, sd = 2)\ny &lt;- -3 * x + rnorm(100, sd = 4)\ndat &lt;- data.frame(x = x, y = y)\n\n\n\n\n\n# scatterplot\ndat %&gt;% \n    ggplot(aes(x = x, y = y)) +\n    geom_point()\n\n\n\n\n\n\n\n\n\nThe correlation between x and y is moderately strong and negative.\nI‚Äôll guess -0.6, since the relationship is negative and is sort of in-between weak and strong.\n\n\n\n# correlation\ndat %&gt;% summarize(cor(x, y))\n\n   cor(x, y)\n1 -0.8295483\n\n\n\n\n\n\n# creating dat_new1\nx1 &lt;- c(x, 15)\ny1 &lt;- c(y, -45)\ndat_new1 &lt;- data.frame(x = x1, y = y1)\n\n\n\n\n\n# scatterplot\ndat_new1 %&gt;%\n    ggplot(aes(x1, y1)) +\n    geom_point()\n\n\n\n\n\n\n\n# correlation\ndat %&gt;% summarize(cor(x1, y1))\n\n  cor(x1, y1)\n1  -0.8573567\n\n\nOur correlation stayed roughly the same with the addition of this new point!\n\n\n\n\n# creating dat_new1\nx2 &lt;- c(x, 15)\ny2 &lt;- c(y, 45)\ndat_new2 &lt;- data.frame(x = x2, y = y2)\n\n\n\n\n\n# scatterplot\ndat_new2 %&gt;%\n    ggplot(aes(x2, y2)) +\n    geom_point()\n\n\n\n\n\n\n\n# correlation\ndat_new2 %&gt;% summarize(cor(x2, y2))\n\n  cor(x2, y2)\n1  -0.2924792\n\n\nThe correlation changes quite a bit with the addition of this new point! Something to note is that this new point does not follow the rough linear trend that the original points had, that the first point we considered adding also had. This line seems way off base, comparatively!\n\nThe takeaway message here is that even though both of these additional points might be considered ‚Äúoutliers‚Äù because they have extreme x values, one changes the relationship between x and y much more than the other. In this case, the second point we considered would be influential because it changes the observed relationship between all x‚Äôs and y‚Äôs much more than the first point we considered. Not all ‚Äúoutliers‚Äù are considered equal!\n\n\n\n\n\ndat_new1 %&gt;%\n    ggplot(aes(x1, y1)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\ndat_new2 %&gt;%\n    ggplot(aes(x2, y2)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Simple linear regression: Visualization and Introduction"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html",
    "href": "activities/01_foundations_welcome.html",
    "title": "Data Fundamentals",
    "section": "",
    "text": "Welcome to our first in-class activity! Today we will start thinking about fundamental ideas when working with data and get to know the people in this class.",
    "crumbs": [
      "Data Fundamentals"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#learning-goals",
    "href": "activities/01_foundations_welcome.html#learning-goals",
    "title": "Data Fundamentals",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of this lesson, you should be able to:\n\nDefine cases and variables\nApply the 5 W‚Äôs + H (who, what, when, where, why, and how) to data collection",
    "crumbs": [
      "Data Fundamentals"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#related-reading",
    "href": "activities/01_foundations_welcome.html#related-reading",
    "title": "Data Fundamentals",
    "section": "Related reading",
    "text": "Related reading\nIf you would like to read more after class, see the following sections of our textbook:\n\nWhat is Data?\nData Context",
    "crumbs": [
      "Data Fundamentals"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#introductions",
    "href": "activities/01_foundations_welcome.html#introductions",
    "title": "Data Fundamentals",
    "section": "Introductions",
    "text": "Introductions\nBefore we start talking about data, we will take time to get to know each other.\nTake 5 minutes to have a conversation with the people at your table:\n\nIntroduce yourselves however you see fit.\nWhat‚Äôs something that‚Äôs on your mind this fall?",
    "crumbs": [
      "Data Fundamentals"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#what-is-data",
    "href": "activities/01_foundations_welcome.html#what-is-data",
    "title": "Data Fundamentals",
    "section": "What is data?",
    "text": "What is data?\nIn a word: information!\nThe way that information is stored affects how we explore it.\nSpreadsheets are common for statistical explorations.\n\n\n\nStructure of tidy data: variables are in columns, observations are in rows, and values are in cells.\n\n\n\nObservations are also called cases or units of analysis.\n\nFigure above comes from Chapter 12 of R for Data Science.",
    "crumbs": [
      "Data Fundamentals"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#data-context",
    "href": "activities/01_foundations_welcome.html#data-context",
    "title": "Data Fundamentals",
    "section": "Data context",
    "text": "Data context\nWhen thinking about where data comes from, it is important to ask ourselves questions stemming from the 5 W‚Äôs + H.\n\nWho?\n\nWho is in the data?\nWhat is the observational unit?\nHow did they end up in the data?\nWere they selected randomly or were they in a particular location a particular time?\nWho collected the data? An agency, a consortium of researchers, an individual researcher? What motivations did they have?\n\nWhat?\n\nWhat is being measured or recorded on each unit?\nWhat are the characteristics, features, or variables that were collected?\n\nWhen?\n\nWhen was the data collected? One point in time? Over time?\nIf data quality degrades over time (e.g.¬†lab specimens), we should be concerned.\n\nWhere?\n\nWhere were the data collected? In one location? Multiple locations?\n\nWhy?\n\nWhy were these data collected? For profit? For academic research? Are there conflicts of interest?\n\nHow?\n\nHow were the data collected? What instruments and methods used for measurement? What questions were asked and how? Online survey? By phone? In person?",
    "crumbs": [
      "Data Fundamentals"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#sold-a-story",
    "href": "activities/01_foundations_welcome.html#sold-a-story",
    "title": "Data Fundamentals",
    "section": "Sold a Story",
    "text": "Sold a Story\n\nPodcast produced in 2022 by the American Public Media Group\nHow did a flawed method for teaching children to read persist in American education despite evidence that showed its inefficacy?\nEpisode 1 details the problem and the flawed approach: the ‚Äúthree-cueing‚Äù method. When faced with an unknown word readers were instructed to use 3 cues in the following priority:\n\nHighest priority: the context of the story (pictures on the page, other words in the sentence)\nMiddle priority: using grammar (is the unknown word a noun, verb, etc.? Past tense, present tense?)\nLowest priority: sounding out the word using the individual letters",
    "crumbs": [
      "Data Fundamentals"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#research-into-reading-strategies",
    "href": "activities/01_foundations_welcome.html#research-into-reading-strategies",
    "title": "Data Fundamentals",
    "section": "Research into reading strategies",
    "text": "Research into reading strategies\nWe are going to listen to two short segments of Sold a Story - Episode 2.\nThese segments describe two different research studies.\n\nFirst study\n\nConducted by Marie Clay - her ideas would form the basis of the three-cueing method\n\nSecond study\n\nConducted by reading research Bruce McCandliss",
    "crumbs": [
      "Data Fundamentals"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#marie-clays-study",
    "href": "activities/01_foundations_welcome.html#marie-clays-study",
    "title": "Data Fundamentals",
    "section": "Marie Clay‚Äôs study",
    "text": "Marie Clay‚Äôs study\nTimestamp: 14:07 - 16:10\nTranscript:\n\nIt was 1963, the same year schools in New Zealand started using those little books. Clay identified 100 children in Auckland in their first year of school. And she observed them for an entire year.\nClay: I went into classrooms. I recorded exactly what children were saying and doing. And this gave me new insights for building, um, almost a new theory of how our children were learning to read.\nClay observed those 100 kids closely. She wanted to know: what were the good readers doing? And what were the poor readers doing that was different?\n(Music)\nShe came away from that study and subsequent research with her theory. Her basic idea was that good readers are good problem solvers. They‚Äôre like detectives, searching for clues. She wrote: ‚ÄúYou will be familiar with the old game ‚ÄòTwenty Questions.‚Äô Reading is something like that game.‚Äù According to Clay‚Äôs theory, when good readers come to a word they don‚Äôt know, they ask themselves good questions. Like, what word would make sense here? For example, if a girl in a story is getting ready to ride a horse, and she puts something on her horse that starts with an ‚Äús‚Äù‚Ä¶the word must be ‚Äúsaddle.‚Äù\nClay also noticed there are things good readers don‚Äôt do. They don‚Äôt laboriously sound out words. They don‚Äôt get stuck on the letters. She thought good readers use the letters in words as one of their clues. But she was convinced that letters are not very good clues. Not that reliable and sometimes just kind of confusing. She concluded that good readers use the letters in words in an ‚Äúincidental‚Äù way. She thought they just skim the letters to confirm they‚Äôre getting the meaning of what they‚Äôre reading. And their last resort when figuring out a word is to sound it out.",
    "crumbs": [
      "Data Fundamentals"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#bruce-mccandlisss-study",
    "href": "activities/01_foundations_welcome.html#bruce-mccandlisss-study",
    "title": "Data Fundamentals",
    "section": "Bruce McCandliss‚Äôs study",
    "text": "Bruce McCandliss‚Äôs study\nTimestamp: 38:58 - 43:24\nTranscript:\n\nBruce McCandliss did a study in 2015 to try to understand how different teaching methods affect reading development. He and his colleagues made up a new written language and brought people into their labs to learn this language. They divided their subjects into two groups. One group was taught the relationships between the symbols and sounds in this new language. The other group was told to just look at the whole words ‚Äì the clusters of unfamiliar symbols ‚Äì and try to remember them.\n(Music)\nAt first, both groups were learning.\nMcCandliss: Learning occurs in both cases. Like, people can master, you know, 20, 30, 40, 50, 60 of these words.\nIn fact, the students who were memorizing the whole words did better at first. Learning was slower for the students focusing on letter-sound relationships. But those students were soon able to read more words than the group that hadn‚Äôt been taught letters and sounds. And when Bruce and his colleagues looked at what was going on inside the brains of their study participants, they saw something interesting. People who were sounding out the words were reading differently than the other group.\nMcCandliss: By really encouraging people to think about and attend to the nuances of the print and how they relate to the pronunciation, we see this activation pattern that looks a lot like what the expert reading circuitry looks like.\nIn other words, the people who focused on letter-sound relationships increased activity in areas of the brain that are associated with skilled reading.\nPeople who were not taught to focus on letters and sounds used a different neural network to read. A network that is not as efficient or effective at helping you map written words into your memory. Other experiments ‚Äì with adults and children ‚Äì have shown similar patterns of brain activity.\n(Music)\nHow a person is taught affects what areas of the brain they use to read. And you want to use the parts of your brain that are going to be most efficient and effective at helping you map words into your memory. Because that‚Äôs how you become a good reader. You‚Äôre not using your brain power to identify the words. You‚Äôre using your brain power to understand what you‚Äôre reading. And that‚Äôs the goal. Bruce McCandliss says teaching kids that they don‚Äôt have to look carefully at words and sound them out is putting many of them at risk of never getting there. Of never becoming good readers.\nMcCandliss: I think more and more people are starting to recognize that there‚Äôs a pretty significant number of kids out there that we‚Äôre neglecting their needs. And the kids struggle and they suffer, and at times I‚Äôve run reading clinics where the kids break down like the fourth word into a reading test and start crying and telling you that they‚Äôre, they‚Äôre a defective person who is stupid and doesn‚Äôt belong in school and hates school and never wants to do anything with reading ever.\n(Music)\nWhen I first started doing all this reporting on reading a few years ago, I didn‚Äôt realize how many people have a hard time learning how to read. I think it came pretty easily to me and to my kids, and I didn‚Äôt think about it much because I didn‚Äôt have to. But according to Reid Lyon, the guy who oversaw all that reading research at the NICHD, learning to read is a formidable challenge for a lot of people. He estimates that about 60% of kids need direct and explicit instruction. If they don‚Äôt get it at school, they might get it at home. But if they don‚Äôt get it, they‚Äôre not likely to become very good readers ‚Äì or spellers. And within that sixty percent of people who need good instruction, there‚Äôs a group of people who need a lot of good instruction. Because learning to read is really, really hard for them. It‚Äôs not about intelligence. There are very, very smart people who struggle to learn how to read. But what the research shows is that nearly everyone can learn how to do it ‚Äì if they are taught.",
    "crumbs": [
      "Data Fundamentals"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#group-exercise-comparing-the-studies-and-their-impact",
    "href": "activities/01_foundations_welcome.html#group-exercise-comparing-the-studies-and-their-impact",
    "title": "Data Fundamentals",
    "section": "Group exercise: comparing the studies and their impact",
    "text": "Group exercise: comparing the studies and their impact\nFeel free to write your responses anywhere - these responses are just for you. We‚Äôll discuss these as a class afterwards.\n\nImagine that you were organizing the data from Marie Clay‚Äôs and Bruce McCandliss‚Äôs studies into spreadsheets. What would each row in the underlying data set represent? What would each column of the data set represent?\nBrainstorm some strengths and weaknesses of Marie Clay‚Äôs and Bruce McCandliss‚Äôs studies. As you brainstorm, think about each of the 5 W‚Äôs + H. (Note: there‚Äôs a lot of contextual information about the studies that we don‚Äôt have from just the podcast. Focus on thinking about who, what, when, where, why, and how types of questions rather than on answering them for these 2 studies.)\nMarie Clay‚Äôs study formed the basis for the flawed ‚Äúthree-cueing‚Äù approach that was in widespread use across the US for decades despite evidence from studies like Bruce McCandliss‚Äôs. How do you think that this could happen?",
    "crumbs": [
      "Data Fundamentals"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#what-to-expect-in-this-course",
    "href": "activities/01_foundations_welcome.html#what-to-expect-in-this-course",
    "title": "Data Fundamentals",
    "section": "What to expect in this course",
    "text": "What to expect in this course\nClass activities\n\nShort discussion together to introduce ideas or review ideas from readings/videos (hopefully no more than 15 minutes)\nWork on practice exercises in small groups\n\nStarting Friday we will be working in RStudio almost every day\nOur preceptor Ryan and I will walk around to help with exercises\n\nSolutions for exercises will be posted at the bottom of each activity page on the course website after class\nActivities are for practice‚Äìnot turned in to be graded\n\nIf you understand the activities, you will have good understanding for the weekly practice problems, 3 quizzes, and the course project.\n\n\n\nCore value in this course: mistakes are needed for growth, and growth occurs when we reflect on feedback\n\nThis value is reflected in the ways that you can show improvement in the main assessments for our course.",
    "crumbs": [
      "Data Fundamentals"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#for-next-time",
    "href": "activities/01_foundations_welcome.html#for-next-time",
    "title": "Data Fundamentals",
    "section": "For next time",
    "text": "For next time\nHow to prepare for subsequent classes will always be posted on the Schedule page.",
    "crumbs": [
      "Data Fundamentals"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html",
    "href": "activities/02_foundations_univariate.html",
    "title": "Univariate visualization and summaries",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDescribe what a case (or unit of analysis) represents in a dataset.\nDescribe what a variable represents in a dataset.\nIdentify whether a variable is categorical or quantitative and what summarizations and visualizations are appropriate for that variable\nWrite R code to read in data and to summarize and visualize a single variable at a time.\nInterpret key features of barplots, boxplots, histograms, and density plots\nDescribe information about the distribution of a quantitative variable using the concepts of shape, center, spread, and outliers\nRelate summary statistics of data to the concepts of shape, center, spread, and outliers\n\n\n\n\nBefore class you should have read and/or watched:\n\nReading: Sections 2.1-2.4, 2.6 in the STAT 155 Notes\nVideos:\n\nUnivariate visualization and summarization (slides)\nR Code for Categorical Visualization and Summarization\nR Code for Quantitative Visualization and Summarization\n\n\n\n\n\nCases\nWhen Macalester advertises an average class size of 17, what do you think the cases in this dataset represent?\n\n\n\n\n\n\nResponse\n\n\n\n\n\nSuppose we had just had 2 classes: one with a class size of 20 and the other with a class size of 28. If the cases are classes, the average class size is 24.\nHowever, if the cases are students, the average class size looks like this:\n\n(20*20 + 28*28)/(20+28)\n\n[1] 24.66667\n\n\nThis is another viewpoint for what ‚Äúaverage class size‚Äù means from the student perspective.\nNote: From the student perspective (when cases are students), average class size will almost always be higher than when cases are classes.\n(Thanks to this post for the idea for this example.)\n\n\n\n\nUnivariate visualization\nIn August 2018, the data journalism group The Pudding published an article about the size of men‚Äôs and women‚Äôs jeans pockets (called Women‚Äôs Pockets are Inferior).\nWe‚Äôll explore this data to review univariate visualization.\n\n\n\n\n\n\nTips for navigating code\n\n\n\n\nWhenever you see a parenthesis (, you are using a function. Look at the text to the left of the ( to see the function name. You can think of function names as verbs. They do things to data.\nWhenever you see an arrow &lt;-, whatever is happening on the right is being stored in a ‚Äúbox‚Äù with the label on the left. (So the result of read_csv() is being stored in a ‚Äúbox‚Äù called pockets‚Äîpockets is the name/label of our dataset.)\n\n\n\n\nlibrary(readr)\n\npockets &lt;- read_csv(\"https://raw.githubusercontent.com/the-pudding/data/master/pockets/measurements.csv\")\n\nThe documentation (codebook) for this data is available here.\nThe menWomen variable is a categorical variable. We can use the code below to make a barplot to explore how many men‚Äôs and women‚Äôs jeans were examined.\n\n\n\n\n\n\nTips for navigating code\n\n\n\n\nInside parentheses (which indicate that a function is being called), look for lists of things separated by commas. The items in that list are inputs (also called arguments) for the function. These inputs supply essential information for the function.\nFor plots, look for + signs. These indicate ‚Äúlayers‚Äù of a plot, kind of like layers of a painting.\n\n\n\n\nlibrary(ggplot2)\n\nggplot(pockets, aes(x = menWomen)) +\n    geom_bar()\n\n\n\n\n\n\n\n\nWe can also tabulate categorical variables with the count() function from the dplyr data wrangling/manipulation package.\n\n\n\n\n\n\nTips for navigating code\n\n\n\n\nThe %&gt;% symbol is called a pipe. It takes the object before it and feeds it in as the first input to the function after it. Whenever you see the pipe symbol, you can replace it in your mind with the words ‚Äúand then‚Äù.\n\n\n\n\nlibrary(dplyr)\n\npockets %&gt;% \n    count(menWomen)\n\n# A tibble: 2 √ó 2\n  menWomen     n\n  &lt;chr&gt;    &lt;int&gt;\n1 men         40\n2 women       40\n\n\nThe minHeightFront variable gives the minimum height of the front pocket and is quantitative. For a single quantitative variable, we can make a boxplot, density plot, or histogram. Let‚Äôs look at a histogram:\n\nggplot(pockets, aes(x = minHeightFront)) +\n    geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWhen you interpret a plot of a quantitative variable, there are 4 aspects to keep in mind:\n\nShape\nCenter\nSpread\nOutliers\n\nShape: How are values distributed along the observed range? What does the distribution of the variable look like?\n\n\n\n\n\n\n\n\n\n\nCenter: What is a typical value of the variable?\n\nQuantified with summary statistics like the mean and median.\n\n\nsummary(pockets$minHeightFront)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.50   13.00   15.00   15.65   17.00   25.00 \n\n\n\nSpread: How spread out are the values? Are most values very close together or far apart?\n\nQuantified with summary statistics like the variance, standard deviation, range (difference between min and max), interquartile range (IQR) (difference between the 75th percentile and 25th percentile)\nInterpretation of the variance: it is the average (roughly) squared distance of each value to the mean. Units are the squared version of the original variable.\nInterpretation of the standard deviation: square root of the variance. Measures spread on the same scale as the original variable (same units as the original variable).\n\n\nvar(pockets$minHeightFront)\n\n[1] 12.27745\n\nsd(pockets$minHeightFront)\n\n[1] 3.50392\n\n\n\nOutliers: Are there any values that are particularly high or low relative to the rest?\n\nA good paragraph putting all of these aspects together:\n\nThe distribution of the minimum height of front pockets seems right skewed with values ranging from 9.5 to 25cm. The average pocket height is about 15cm (median). Pocket heights tend to deviate from this average by about 3.5cm (SD), and there don‚Äôt seem to be extreme outliers.\n\n\nFile organization: You can download a template file to work with here. Save this file in the ‚ÄúActivities‚Äù subfolder of your ‚ÄúSTAT155‚Äù folder.",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#learning-goals",
    "href": "activities/02_foundations_univariate.html#learning-goals",
    "title": "Univariate visualization and summaries",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDescribe what a case (or unit of analysis) represents in a dataset.\nDescribe what a variable represents in a dataset.\nIdentify whether a variable is categorical or quantitative and what summarizations and visualizations are appropriate for that variable\nWrite R code to read in data and to summarize and visualize a single variable at a time.\nInterpret key features of barplots, boxplots, histograms, and density plots\nDescribe information about the distribution of a quantitative variable using the concepts of shape, center, spread, and outliers\nRelate summary statistics of data to the concepts of shape, center, spread, and outliers",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#readings-and-videos",
    "href": "activities/02_foundations_univariate.html#readings-and-videos",
    "title": "Univariate visualization and summaries",
    "section": "",
    "text": "Before class you should have read and/or watched:\n\nReading: Sections 2.1-2.4, 2.6 in the STAT 155 Notes\nVideos:\n\nUnivariate visualization and summarization (slides)\nR Code for Categorical Visualization and Summarization\nR Code for Quantitative Visualization and Summarization",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#review",
    "href": "activities/02_foundations_univariate.html#review",
    "title": "Univariate visualization and summaries",
    "section": "",
    "text": "Cases\nWhen Macalester advertises an average class size of 17, what do you think the cases in this dataset represent?\n\n\n\n\n\n\nResponse\n\n\n\n\n\nSuppose we had just had 2 classes: one with a class size of 20 and the other with a class size of 28. If the cases are classes, the average class size is 24.\nHowever, if the cases are students, the average class size looks like this:\n\n(20*20 + 28*28)/(20+28)\n\n[1] 24.66667\n\n\nThis is another viewpoint for what ‚Äúaverage class size‚Äù means from the student perspective.\nNote: From the student perspective (when cases are students), average class size will almost always be higher than when cases are classes.\n(Thanks to this post for the idea for this example.)\n\n\n\n\nUnivariate visualization\nIn August 2018, the data journalism group The Pudding published an article about the size of men‚Äôs and women‚Äôs jeans pockets (called Women‚Äôs Pockets are Inferior).\nWe‚Äôll explore this data to review univariate visualization.\n\n\n\n\n\n\nTips for navigating code\n\n\n\n\nWhenever you see a parenthesis (, you are using a function. Look at the text to the left of the ( to see the function name. You can think of function names as verbs. They do things to data.\nWhenever you see an arrow &lt;-, whatever is happening on the right is being stored in a ‚Äúbox‚Äù with the label on the left. (So the result of read_csv() is being stored in a ‚Äúbox‚Äù called pockets‚Äîpockets is the name/label of our dataset.)\n\n\n\n\nlibrary(readr)\n\npockets &lt;- read_csv(\"https://raw.githubusercontent.com/the-pudding/data/master/pockets/measurements.csv\")\n\nThe documentation (codebook) for this data is available here.\nThe menWomen variable is a categorical variable. We can use the code below to make a barplot to explore how many men‚Äôs and women‚Äôs jeans were examined.\n\n\n\n\n\n\nTips for navigating code\n\n\n\n\nInside parentheses (which indicate that a function is being called), look for lists of things separated by commas. The items in that list are inputs (also called arguments) for the function. These inputs supply essential information for the function.\nFor plots, look for + signs. These indicate ‚Äúlayers‚Äù of a plot, kind of like layers of a painting.\n\n\n\n\nlibrary(ggplot2)\n\nggplot(pockets, aes(x = menWomen)) +\n    geom_bar()\n\n\n\n\n\n\n\n\nWe can also tabulate categorical variables with the count() function from the dplyr data wrangling/manipulation package.\n\n\n\n\n\n\nTips for navigating code\n\n\n\n\nThe %&gt;% symbol is called a pipe. It takes the object before it and feeds it in as the first input to the function after it. Whenever you see the pipe symbol, you can replace it in your mind with the words ‚Äúand then‚Äù.\n\n\n\n\nlibrary(dplyr)\n\npockets %&gt;% \n    count(menWomen)\n\n# A tibble: 2 √ó 2\n  menWomen     n\n  &lt;chr&gt;    &lt;int&gt;\n1 men         40\n2 women       40\n\n\nThe minHeightFront variable gives the minimum height of the front pocket and is quantitative. For a single quantitative variable, we can make a boxplot, density plot, or histogram. Let‚Äôs look at a histogram:\n\nggplot(pockets, aes(x = minHeightFront)) +\n    geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWhen you interpret a plot of a quantitative variable, there are 4 aspects to keep in mind:\n\nShape\nCenter\nSpread\nOutliers\n\nShape: How are values distributed along the observed range? What does the distribution of the variable look like?\n\n\n\n\n\n\n\n\n\n\nCenter: What is a typical value of the variable?\n\nQuantified with summary statistics like the mean and median.\n\n\nsummary(pockets$minHeightFront)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.50   13.00   15.00   15.65   17.00   25.00 \n\n\n\nSpread: How spread out are the values? Are most values very close together or far apart?\n\nQuantified with summary statistics like the variance, standard deviation, range (difference between min and max), interquartile range (IQR) (difference between the 75th percentile and 25th percentile)\nInterpretation of the variance: it is the average (roughly) squared distance of each value to the mean. Units are the squared version of the original variable.\nInterpretation of the standard deviation: square root of the variance. Measures spread on the same scale as the original variable (same units as the original variable).\n\n\nvar(pockets$minHeightFront)\n\n[1] 12.27745\n\nsd(pockets$minHeightFront)\n\n[1] 3.50392\n\n\n\nOutliers: Are there any values that are particularly high or low relative to the rest?\n\nA good paragraph putting all of these aspects together:\n\nThe distribution of the minimum height of front pockets seems right skewed with values ranging from 9.5 to 25cm. The average pocket height is about 15cm (median). Pocket heights tend to deviate from this average by about 3.5cm (SD), and there don‚Äôt seem to be extreme outliers.\n\n\nFile organization: You can download a template file to work with here. Save this file in the ‚ÄúActivities‚Äù subfolder of your ‚ÄúSTAT155‚Äù folder.",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-1-get-curious",
    "href": "activities/02_foundations_univariate.html#exercise-1-get-curious",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 1: Get curious",
    "text": "Exercise 1: Get curious\n\nHypothesize with each other: what themes do you think might come up often in Dear Abby letters?\nAfter brainstorming, take a quick glance at the original article from The Pudding to see what themes they explored.\nGo to the very end of the Pudding article to the section titled ‚ÄúData and Method‚Äù. In thinking about the who, what, when, where, why, and how of data context, what concerns/limitations surface with regards to using this data to learn about Americans‚Äô concerns over the decades?",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-2-importing-and-getting-to-know-the-data",
    "href": "activities/02_foundations_univariate.html#exercise-2-importing-and-getting-to-know-the-data",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 2: Importing and getting to know the data",
    "text": "Exercise 2: Importing and getting to know the data\n\n# Load package\nlibrary(readr)\n\n# Read in the course evaluation data\nabby &lt;- read_csv(\"https://mac-stat.github.io/data/dear_abby.csv\")\n\n\nClick on the Environment tab (generally in the upper right hand pane in RStudio). Then click the abby line. The abby data will pop up as a separate pane (like viewing a spreadsheet) ‚Äì check it out.\nIn this tidy dataset, what is the unit of observation? That is, what is represented in each row of the dataset?\nWhat term do we use for the columns of the dataset?\nTry out each function below. Identify what each function tells you about the abby data and note this in the ???:\n\n\n# ??? [what do both numbers mean?]\ndim(abby)\n\n# ???\nnrow(abby)\n\n# ???\nncol(abby)\n\n# ???\nhead(abby)\n\n# ???\nnames(abby)\n\n\nWe can learn what functions do by pulling up help pages. To do this, click inside the Console pane, and enter ?function_name. For example, to pull up a help page for the dim() function, we can type ?dim and hit Enter. Pull up the help page for the head() function.\n\nRead the Description.\nChallenge: Look at the Arguments and Examples sections to figure out how to display the first 10 rows of the evals data (instead of the default first 6 rows).",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-3-preparing-to-summarize-and-visualize-the-data",
    "href": "activities/02_foundations_univariate.html#exercise-3-preparing-to-summarize-and-visualize-the-data",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 3: Preparing to summarize and visualize the data",
    "text": "Exercise 3: Preparing to summarize and visualize the data\nIn the next exercises, we will be exploring themes in the Dear Abby questions and the overall ‚Äúmood‚Äù or sentiment of the questions. Before continuing, read the codebook for this dataset for some context about sentiment analysis, which gives us a measure of the mood/sentiment of a text.\n\nWhat sentiment variables do we have in the dataset? Are they quantitative or categorical?\nIf we were able to create a theme variable that took values like ‚Äúfriendship‚Äù, ‚Äúmarriage‚Äù, and ‚Äúrelationships‚Äù, would theme be quantitative or categorical?\nWhat visualizations are appropriate for looking at the distribution of a single quantitative variable? What about a single categorical variable?",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-4-exploring-themes-in-the-letters",
    "href": "activities/02_foundations_univariate.html#exercise-4-exploring-themes-in-the-letters",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 4: Exploring themes in the letters",
    "text": "Exercise 4: Exploring themes in the letters\nThe dplyr package provides many useful functions for managing data (like creating new variables, summarizing information). The stringr package provides tools for working with strings (text). We‚Äôll use these packages to search for words in the questions in order to (roughly) identify themes/subjects.\nThe code below searches for words related to mothers, fathers, marriage, and money and combines them into a single theme variable.\n\nInside mutate() the line moms = ifelse(str_detect(question_only, \"mother|mama|mom\"), \"mom\", \"no mom\") creates a new variable called moms. If any of the text ‚Äúmother‚Äù, ‚Äúmama‚Äù, or ‚Äúmom‚Äù (which covers ‚Äúmommy‚Äù) is found, then the variable takes the value ‚Äúmom‚Äù. Otherwise, the variable takes the value ‚Äúno mom‚Äù.\nThe dads, marriage, and money variables are created similarly.\nThe themes = str_c(moms, dads, marriage, money, sep = \"|\") line takes the 4 created variables and combines the text of those variables separated with a |. For example, one value of the themes variable is ‚Äúmom|no_dad|no_marriage|no_money‚Äù (which contains words about moms but not dads, marriage, or money).\n\n\nlibrary(dplyr)\nlibrary(stringr)\n\nabby &lt;- abby %&gt;% \n    mutate(\n        moms = ifelse(str_detect(question_only, \"mother|mama|mom\"), \"mom\", \"no mom\"),\n        dads = ifelse(str_detect(question_only, \"father|papa|dad\"), \"dad\", \"no dad\"),\n        marriage = ifelse(str_detect(question_only, \"marriage|marry|married\"), \"marriage\", \"no marriage\"),\n        money = ifelse(str_detect(question_only, \"money|finance\"), \"money\", \"no money\"),\n        themes = str_c(moms, dads, marriage, money, sep = \"|\")\n    )\n\n\nModify the code above however you wish to replace themes (e.g., replace ‚Äúmoms‚Äù with something else) or add new themes to search for. If you want to add a new subject to search for, copy and paste a line for an existing subject above the themes line, and modify the code like this:\n\nIf your subject is captured by multiple words: YOUR_SUBJECT = ifelse(str_detect(question_only, \"WORD1|WORD2|ETC\"), \"SUBJECT\", \"NO SUBJECT\"),\nIf your subject is captured by a single word: YOUR_SUBJECT = ifelse(str_detect(question_only, \"WORD\"), \"SUBJECT\", \"NO SUBJECT\"),\nTry to have no more than 6 subjects‚Äîotherwise we‚Äôll have too many themes, which will complicate exploration.\n\nThe code below makes a barplot of the themes variable using the ggplot2 visualization package. Before making the plot, make note of what you expect the plot might look like. (This might be hard‚Äìjust do your best!) Then compare to what you observe when you run the code chunk to make the plot. (Clearly defining your expectations first is good scientific practice to avoid confirmation bias.)\n\n\n# Load package\nlibrary(ggplot2)\n\n# barplot\nggplot(abby, aes(x = themes)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\nWe can follow up on the barplot with a simple numerical summary. Whereas the ggplot2 package is great for visualizations, dplyr is great for numerical summaries. The code below constructs a table of the number of questions with each theme. Make sure that these numerical summaries match up with what you saw in the barplot.\n\n\n# Construct a table of counts\nabby %&gt;% \n    count(themes)\n\n\nBefore proceeding, let‚Äôs break down the plotting code above. Run each chunk to see how the two lines of code above build up the plot in ‚Äúlayers‚Äù. Add comments (on the lines starting with #) to document what you notice.\n\n\n# ???\nggplot(abby, aes(x = themes))\n\n\n# ???\nggplot(abby, aes(x = themes)) +\n    geom_bar()\n\n\n# ???\nggplot(abby, aes(x = themes)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n# ???\nggplot(abby, aes(x = themes)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n    theme_classic()",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-5-exploring-sentiment",
    "href": "activities/02_foundations_univariate.html#exercise-5-exploring-sentiment",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 5: Exploring sentiment",
    "text": "Exercise 5: Exploring sentiment\nWe‚Äôll look at the distribution of the afinn_overall sentiment variable and associated summary statistics.\n\nThe code below creates a boxplot of this variable. In the comment, make note of how this code is simliar to the code for the barplot above. As in the previous exercise, before running the code chunk to create the plot, make note of what you expect the boxplot to look like.\n\n\n# ???\nggplot(abby, aes(x = afinn_overall)) +\n    geom_boxplot()\n\n\nChallenge: Using the code for the barplot and boxplot as a guide, try to make a histogram and a density plot of the overall average ratings.\n\nWhat information is given by the tallest bar of the histogram?\nHow would you describe the shape of the distribution?\n\n\n\n# Histogram\n\n# Density plot\n\n\nWe can compute summary statistics (numerical summaries) for a quantitative variable using the summary() function or with the summarize() function from the dplyr package. (1st Qu. and 3rd Qu. stand for first and third quartile.) After inspecting these summaries, look back to your boxplot, histogram, and density plot. Which plots show which summaries most clearly?\n\n\n# Summary statistics\n# Using summary() - convenient for computing many summaries in one command\n# Does not show the standard deviation\nsummary(abby$afinn_overall)\n\n# Using summarize() from dplyr\n# Note that we use %&gt;% to pipe the data into the summarize() function\n# We need to use na.rm = TRUE because there are missing values (NAs)\nabby %&gt;% \n    summarize(mean(afinn_overall, na.rm = TRUE), median(afinn_overall, na.rm = TRUE), sd(afinn_overall, na.rm = TRUE))\n\n\nWrite a good paragraph describing the information in the histogram (or density plot) by discussing shape, center, spread, and outliers. Incorporate the numerical summaries from part c.",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-6-box-plots-vs.-histograms-vs.-density-plots",
    "href": "activities/02_foundations_univariate.html#exercise-6-box-plots-vs.-histograms-vs.-density-plots",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 6: Box plots vs.¬†histograms vs.¬†density plots",
    "text": "Exercise 6: Box plots vs.¬†histograms vs.¬†density plots\nWe took 3 different approaches to plotting the quantitative average course variable above. They all have pros and cons.\n\nWhat is one pro about the boxplot in comparison to the histogram and density plot?\nWhat is one con about the boxplot in comparison to the histogram and density plots?\nIn this example, which plot do you prefer and why?",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-7-explore-outliers",
    "href": "activities/02_foundations_univariate.html#exercise-7-explore-outliers",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 7: Explore outliers",
    "text": "Exercise 7: Explore outliers\nGiven that Dear Abby column is an advice column, it seems natural that the sentiment of the questions would lean more negative. What‚Äôs going on with the questions that have particularly positive sentiments?\nWe can use the filter() function in the dplyr package to look at the . Based on the plots of afinn_overall that you made in Exercise 5, pick a threshold for the afinn_overall variable‚Äîwe‚Äôll say that questions with an overall sentiment score above this threshold are high outliers. Fill in this number where it says YOUR_THRESHOLD below.\n\nabby %&gt;% \n    filter(afinn_overall &gt; YOUR_THRESHOLD) %&gt;% \n    pull(question_only)\n\nWhat do you notice? Why might these questions have such high sentiment scores?",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-8-returning-to-our-context-looking-ahead",
    "href": "activities/02_foundations_univariate.html#exercise-8-returning-to-our-context-looking-ahead",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 8: Returning to our context, looking ahead",
    "text": "Exercise 8: Returning to our context, looking ahead\nIn this activity, we explored data on Dear Abby question, with a focus on exploring a single variable at a time.\n\nIn big picture terms, what have we learned about Dear Abby questions?\nWhat further curiosities do you have about the data?",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-9-different-ways-to-think-about-data-visualization",
    "href": "activities/02_foundations_univariate.html#exercise-9-different-ways-to-think-about-data-visualization",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 9: Different ways to think about data visualization",
    "text": "Exercise 9: Different ways to think about data visualization\nIn working with and visualizing data, it‚Äôs important to keep in mind what a data point represents. It can reflect the experience of a real person. It might reflect the sentiment in a piece of art. It might reflect history. We‚Äôve taken one very narrow and technical approach to data visualization. Check out the following examples, and write some notes about anything you find interesting.\n\nDear Data\nW.E.B. DuBois\nDecolonizing Data Viz\nPhase Change Project (by Prof Kim, Mac research students)",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-10-rendering-your-work",
    "href": "activities/02_foundations_univariate.html#exercise-10-rendering-your-work",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 10: Rendering your work",
    "text": "Exercise 10: Rendering your work\nSave this file, and then click the ‚ÄúRender‚Äù button in the menu bar for this pane (blue arrow pointing right). This will create an HTML file containing all of the directions, code, and responses from this activity. A preview of the HTML will appear in the browser.\n\nScroll through and inspect the document to see how your work was translated into this HTML format. Neat!\nClose the browser tab.\nGo to the ‚ÄúBackground Jobs‚Äù pane in RStudio and click the Stop button to end the rendering process.\nNavigate to your ‚ÄúActivities‚Äù subfolder within your ‚ÄúSTAT155‚Äù folder and locate the HTML file. You can open it again in your browser to double check.",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#reflection",
    "href": "activities/02_foundations_univariate.html#reflection",
    "title": "Univariate visualization and summaries",
    "section": "Reflection",
    "text": "Reflection\nGo to the top of this file and review the learning objectives for this lesson. Which objectives do you have a good handle on, are at least familiar with, or are struggling with? What feels challenging right now? What are some wins from the day?\n\nResponse: Put your response here.",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-11-read-in-and-get-to-know-the-weather-data",
    "href": "activities/02_foundations_univariate.html#exercise-11-read-in-and-get-to-know-the-weather-data",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 11: Read in and get to know the weather data",
    "text": "Exercise 11: Read in and get to know the weather data\nDaily weather data are available for 3 locations in Perth, Australia.\n\nView the codebook here.\nComplete the code below to read in the data.\n\n\n# Replace the ??? with your own name for the weather data\n# Replace the ___ with the correct function\n??? &lt;- ___(\"https://mac-stat.github.io/data/weather_3_locations.csv\")",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-12-exploring-the-data-structure",
    "href": "activities/02_foundations_univariate.html#exercise-12-exploring-the-data-structure",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 12: Exploring the data structure",
    "text": "Exercise 12: Exploring the data structure\nCheck out the basic features of the weather data.\n\n# Examine the first six cases\n\n# Find the dimensions of the data\n\nWhat does a case represent in this data?",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-13-exploring-rainfall",
    "href": "activities/02_foundations_univariate.html#exercise-13-exploring-rainfall",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 13: Exploring rainfall",
    "text": "Exercise 13: Exploring rainfall\nThe raintoday variable contains information about rainfall.\n\nIs this variable quantitative or categorical?\nCreate an appropriate visualization, and compute appropriate numerical summaries.\nWhat do you learn about rainfall in Perth?\n\n\n# Visualization\n\n# Numerical summaries",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-14-exploring-temperature",
    "href": "activities/02_foundations_univariate.html#exercise-14-exploring-temperature",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 14: Exploring temperature",
    "text": "Exercise 14: Exploring temperature\nThe maxtemp variable contains information on the daily high temperature.\n\nIs this variable quantitative or categorical?\nCreate an appropriate visualization, and compute appropriate numerical summaries.\nWhat do you learn about high temperatures in Perth?\n\n\n# Visualization\n\n# Numerical summaries",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-15-customizing-challenge",
    "href": "activities/02_foundations_univariate.html#exercise-15-customizing-challenge",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 15: Customizing! (CHALLENGE)",
    "text": "Exercise 15: Customizing! (CHALLENGE)\nThough you will naturally absorb some RStudio code throughout the semester, being an effective statistical thinker and ‚Äúprogrammer‚Äù does not require that we memorize all code. That would be impossible! In contrast, using the foundation you built today, do some digging online to learn how to customize your visualizations.\n\nFor the histogram below, add a title and more meaningful axis labels. Specifically, title the plot ‚ÄúDistribution of max temperatures in Perth‚Äù, change the x-axis label to ‚ÄúMaximum temperature‚Äù and y-axis label to ‚ÄúNumber of days‚Äù. HINT: Do a Google search for something like ‚Äúadd axis labels ggplot‚Äù.\n\n\n# Add a title and axis labels\nggplot(weather, aes(x = maxtemp)) + \n    geom_histogram()\n\n\nAdjust the code below in order to color the bars green. NOTE: Color can be an effective tool, but here it is simply gratuitous.\n\n\n# Make the bars green\nggplot(weather, aes(x = raintoday)) + \n    geom_bar()\n\n\nCheck out the ggplot2 cheat sheet. Try making some of the other kinds of univariate plots outlined there.\nWhat else would you like to change about your plot? Try it!",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-1-get-curious-1",
    "href": "activities/02_foundations_univariate.html#exercise-1-get-curious-1",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 1: Get curious",
    "text": "Exercise 1: Get curious\n\nResults of brainstorming themes will vary\nFrom the ‚ÄúData and Method‚Äù section at the end of the Pudding article, we see this paragraph:\n\n\nThe writers of these questions likely skew roughly 2/3 female (according to Pauline Phillips, who mentions the demographics of responses to a survey she disseminated in 1987), and consequently, their interests are overrepresented; we‚Äôve been unable to find other demographic data surrounding their origins. There is, doubtless, a level of editorializing here: only a fraction of the questions that people have written in have seen publication, because agony aunts (the writers of advice columns) must selectively filter what gets published. Nevertheless, the concerns of the day seem to be represented, such as the HIV/AIDS crisis in the 1980s. Additionally, we believe that the large sample of questions in our corpus (20,000+) that have appeared over recent decades gives a sufficient directional sense of broad trends.\n\n\nWriters of the questions are predominately female. The 2/3 proportion was estimated in 1987, so it would be useful to understand shifts in demographics over time.\nWhat questions were chosen to be answered on the column? Likely a small fraction of what got submitted. What themes tended to get cut out?",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-2-importing-and-getting-to-know-the-data-1",
    "href": "activities/02_foundations_univariate.html#exercise-2-importing-and-getting-to-know-the-data-1",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 2: Importing and getting to know the data",
    "text": "Exercise 2: Importing and getting to know the data\n\nNote how clicking the abby data causes both a popup pane and the command View(abby) to appear in the Console. In fact, the View() function is the underlying command that opens a dataset pane. (View() should always be entered in the Console and NOT your Quarto document.)\nEach row / case corresponds to a single question.\nColumns = variables\nTry out each function below. Identify what each function tells you about the abby data and note this in the ???:\n\n\n# First number = number of rows / cases\n# Second number = number of columns / variables\ndim(abby)\n\n[1] 20034    16\n\n# Number of rows (cases)\nnrow(abby)\n\n[1] 20034\n\n# Number of columns (variables)\nncol(abby)\n\n[1] 16\n\n# View first few rows of the dataset (6 rows, by default)\nhead(abby)\n\n# A tibble: 6 √ó 16\n   year month day   url     title letterId question_only afinn_overall afinn_pos\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;\n1  1985     1 01    proque‚Ä¶ WOMA‚Ä¶        1 \"i have been‚Ä¶           -30         5\n2  1985     1 01    proque‚Ä¶ WOMA‚Ä¶        1 \"this is for‚Ä¶           -30         5\n3  1985     1 02    proque‚Ä¶ LAME‚Ä¶        1 \"our 16-year‚Ä¶             1         3\n4  1985     1 03    proque‚Ä¶ 'NOR‚Ä¶        1 \"i was a hap‚Ä¶            -3         7\n5  1985     1 04    proque‚Ä¶ IT'S‚Ä¶        1 \"you be the ‚Ä¶            13        31\n6  1985     1 04    proque‚Ä¶ IT'S‚Ä¶        1 \"a further w‚Ä¶            13        31\n# ‚Ñπ 7 more variables: afinn_neg &lt;dbl&gt;, bing_pos &lt;dbl&gt;, moms &lt;chr&gt;, dads &lt;chr&gt;,\n#   marriage &lt;chr&gt;, money &lt;chr&gt;, themes &lt;chr&gt;\n\n# Get all column (variable) names\nnames(abby)\n\n [1] \"year\"          \"month\"         \"day\"           \"url\"          \n [5] \"title\"         \"letterId\"      \"question_only\" \"afinn_overall\"\n [9] \"afinn_pos\"     \"afinn_neg\"     \"bing_pos\"      \"moms\"         \n[13] \"dads\"          \"marriage\"      \"money\"         \"themes\"       \n\n\n\nWe can display the first 10 rows with head(abby, n = 10).",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-3-preparing-to-summarize-and-visualize-the-data-1",
    "href": "activities/02_foundations_univariate.html#exercise-3-preparing-to-summarize-and-visualize-the-data-1",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 3: Preparing to summarize and visualize the data",
    "text": "Exercise 3: Preparing to summarize and visualize the data\n\nThe sentiment variables are afinn_overall, afinn_pos, afinn_neg, and bing_pos, and they are quantitative. The afinn variables don‚Äôt have units but we can still get a sense of the scale by remembering that each word gets a score between -5 and 5. The bing_pos variable doesn‚Äôt have units because it‚Äôs a fraction, but we know that it ranges from 0 to 1.\ntheme would be categorical.\nAppropriate visualizations:\n\nsingle quantitative variable: boxplot, histogram, density plot\nsingle categorical variable: barplot",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-4-exploring-themes-in-the-letters-1",
    "href": "activities/02_foundations_univariate.html#exercise-4-exploring-themes-in-the-letters-1",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 4: Exploring themes in the letters",
    "text": "Exercise 4: Exploring themes in the letters\n\nCode will vary\nExpectations about the plot will vary\n\n\nggplot(abby, aes(x = themes)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\nCounts in the table below match the barplot\n\n\n# Construct a table of counts\nabby %&gt;% \n    count(themes)\n\n# A tibble: 16 √ó 2\n   themes                                 n\n   &lt;chr&gt;                              &lt;int&gt;\n 1 mom|dad|marriage|money                67\n 2 mom|dad|marriage|no money            567\n 3 mom|dad|no marriage|money            109\n 4 mom|dad|no marriage|no money         906\n 5 mom|no dad|marriage|money            121\n 6 mom|no dad|marriage|no money         839\n 7 mom|no dad|no marriage|money         293\n 8 mom|no dad|no marriage|no money     2462\n 9 no mom|dad|marriage|money             41\n10 no mom|dad|marriage|no money         350\n11 no mom|dad|no marriage|money          96\n12 no mom|dad|no marriage|no money      760\n13 no mom|no dad|marriage|money         360\n14 no mom|no dad|marriage|no money     2967\n15 no mom|no dad|no marriage|money      865\n16 no mom|no dad|no marriage|no money  9231\n\n\n\nWhat do the plot layers do?\n\n\n# Just sets up the \"canvas\" of the plot with axis labels\nggplot(abby, aes(x = themes))\n\n\n\n\n\n\n\n\n\n# Adds the bars\nggplot(abby, aes(x = themes)) +\n    geom_bar()\n\n\n\n\n\n\n\n\n\n# Rotates the x axis labels\nggplot(abby, aes(x = themes)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\n# Changes the visual theme of the plot with a white background and removes gridlines\nggplot(abby, aes(x = themes)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n    theme_classic()",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-5-exploring-course-overall-ratings",
    "href": "activities/02_foundations_univariate.html#exercise-5-exploring-course-overall-ratings",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 5: Exploring course overall ratings",
    "text": "Exercise 5: Exploring course overall ratings\nNow we‚Äôll look at the distribution of the avg_rating variable and associated summary statistics.\n\n\nWe might expect the mean of this variable is less than zero given that more negative words might be appear in questions on an advice column.\nThe code has a similar structure to the barplot in that there is an initial ggplot() layer which sets the canvas, then a + to add a layer, then the final layer geom_boxplot() (like geom_bar()) which tells R what type of plot to make.\n\n\n\nggplot(abby, aes(x = afinn_overall)) +\n    geom_boxplot()\n\nWarning: Removed 490 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\nWe replace geom_boxplot() with geom_histogram() and geom_density().\n\nThe tallest bar of the histogram indicates that over 7500 questions had an overall afinn sentiment score between around -8 to 0.(The -8 to 0 comes from eyeballing where the tallest bar is placed on the x-axis, and the height of this bar indicates how many cases fall into that bin.)\nThe shape of the distribution: roughly symmetric\n\n\n\n# Histogram\nggplot(abby, aes(x = afinn_overall)) +\n    geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 490 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n# Density plot\nggplot(abby, aes(x = afinn_overall)) +\n    geom_density()\n\nWarning: Removed 490 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\n\nBoxplot shows min, max, median, 1st and 3rd quartile easily. (It shows median, 1st and 3rd quartile directly as lines)\nHistogram and density plot show min and max but the mean and median aren‚Äôt shown directly‚Äìwe have to roughly guess based on the peak of the distribution\n\n\n\n# Summary statistics\nsummary(abby$afinn_overall)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n-140.000   -6.000   -1.000   -1.401    3.000  100.000      490 \n\nabby %&gt;% \n    summarize(mean(afinn_overall, na.rm = TRUE), median(afinn_overall, na.rm = TRUE), sd(afinn_overall, na.rm = TRUE))\n\n# A tibble: 1 √ó 3\n  mean(afinn_overall, na.rm = TR‚Ä¶¬π median(afinn_overall‚Ä¶¬≤ sd(afinn_overall, na‚Ä¶¬≥\n                             &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n1                            -1.40                     -1                   11.1\n# ‚Ñπ abbreviated names: ¬π‚Äã`mean(afinn_overall, na.rm = TRUE)`,\n#   ¬≤‚Äã`median(afinn_overall, na.rm = TRUE)`, ¬≥‚Äã`sd(afinn_overall, na.rm = TRUE)`\n\n\n\nThe distribution of sentiment scores is roughly symmetric with a mean of -1.4 and a similar median of -1. The median and mean are quite similar because the distribution is fairly symmetric. The standard deviation of the sentiment scores is about 11.08 which tells us how much variation there is from the center of the distribution. 11.08 is somewhat high given the IQR of -6 to 3 (which is a span of 9 units).",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-6-box-plots-vs.-histograms-vs.-density-plots-1",
    "href": "activities/02_foundations_univariate.html#exercise-6-box-plots-vs.-histograms-vs.-density-plots-1",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 6: Box plots vs.¬†histograms vs.¬†density plots",
    "text": "Exercise 6: Box plots vs.¬†histograms vs.¬†density plots\n\nBoxplots very clearly show key summary statistics like median, 1st and 3rd quartile\nBoxplots can oversimplify by not showing the shape of the distribution.",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-7-explore-outliers-1",
    "href": "activities/02_foundations_univariate.html#exercise-7-explore-outliers-1",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 7: Explore outliers",
    "text": "Exercise 7: Explore outliers\nThere are some positive words in the questions that seem to pull up the sentiment score a lot despite the negative overall tone. From this we can see the limitations of a basic sentiment analysis in which the sentiment of each word is considered in isolation.\n\nabby %&gt;% \n    filter(afinn_overall &gt; 50) %&gt;% \n    pull(question_only) %&gt;% \n    head() # Just to look at first 6\n\n[1] \"i am a 36-year-old college dropout whose lifelong ambition was to be a physician. i have a very good job selling pharmaceutical supplies, but my heart is still in the practice of medicine. i do volunteer work at the local hospital on my time off, and people tell me i would have made a wonderful doctor.\\nif i go back to college and get my degree, then go to medical school, do my internship and finally get into the actual practice of medicine, it will take me seven years! but, abby, in seven years i will be 43 years old. what do you think?\\nunfulfilled in philly\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n[2] \"we have an only child--a grown daughter we dearly love--and when we pass on, we want to leave her our entire estate, which is considerable.\\nthe thing that troubles us is this: our daughter is married to a very unworthy character. for years he has taken advantage of her sweet, forgiving, generous nature because he knows she worships him. we are sure that whatever we leave our daughter will be spent on this dirty dog.\\nhow can we prevent this from happening?\\nbewildered\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n[3] \"both of our sons have been married for about 15 years. their wives were of normal weight when they married our sons, but one daughter-in- law weighs about 300 pounds and the other weighs about 225. their ages are 35 and 37. both our sons are good-looking, and neither is fat.\\nour daughters-in-law seem to have no pride in their appearance, which upsets everyone in the family, except themselves. they are fat, they know it and they don't care! when they first began to put on weight, they tried various diets, pills, doctors, etc., but they both gave up and decided to \\\"accept\\\" themselves as they are.\\nthey wear the wrong kind of clothes (shorts and blue jeans) without any apologies.\\nour problem (my husband's and mine) is how do we cope with this? we are ashamed to be around them. our sons have accepted the situation, but we seem unable to.\\nperhaps we need more help than the girls. any suggestions?\\nupset in florida\"                                                                                                                                                                                                                                                                                                                                                                                                                                           \n[4] \"the letter from \\\"concerned mom,\\\" who was trying to teach her 5-year-old not to accept gifts from strangers, prompts this letter.\\na gentleman friend of mine recently stood in line behind a mother and her young daughter at a bank. the child remarked on the visor he was wearing, as it had the name of a popular pizza imprinted on it.\\nmy friend, who is the public relations director for this pizza firm, wanted the child to have the visor but, instead of giving it to the child, he handed the visor to her mother and said to the child: \\\"i'm giving this to your mother to give to you, because she's probably told you never to accept gifts from a stranger. you won't ever do that, will you?\\\"\\nwhat a thoughtful way to be friendly while reinforcing a message mothers cannot stress enough.\\nsue in wichita, kan.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n[5] \"in january, i sent an original manuscript as a gift to woody allen. i had hand-bound the pages, and decorated the binding with baroque pearls and amethyst. i enclosed my name, address and telephone number. i had hoped that woody would send me a note or call me, or at the very least, instruct his secretary to do so.\\nto date, i haven't received even an acknowledgment that my gift was received. is it unrealistic of me to expect a thank-you from a famous person?\\ndisappointed in california.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n[6] \"will you please, please discourage high school and college graduates from sending graduation invitations to every distant relative they and their parents ever heard of? we all know that sending \\\"invitations\\\" to people we hardly know is a flagrant, shameless bid for a gift. and if, in a moment of weakness, one does send a gift, a barrage of birth announcements and invitations to weddings, showers and more graduations is sure to follow.\\ni am a 75-year-old widow, living on social security and very little else. i just received a high school graduation invitation from the granddaughter of a third cousin whom i have not seen in so long i wouldn't even recognize her. (i have never even met her granddaughter.)\\ni have many relatives in this town, but i never hear from them unless they are celebrating something that requires a gift. i have no car, yet they \\\"invite\\\" me to every imaginable event, knowing full well i can't possibly attend. this is just shameless begging.\\ni am not cheap. i just sent a generous graduation gift to a neighbor girl who used to stop by every day to bring in my mail and newspaper and ask if i needed any errands run.\\ndon't suggest that i send \\\"a nice card\\\" to the relatives who send me invitations to events they know i can't attend. we both know a card is not what these spongers want.\\nsick of them in iowa city\"",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-8-returning-to-our-context-looking-ahead-1",
    "href": "activities/02_foundations_univariate.html#exercise-8-returning-to-our-context-looking-ahead-1",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 8: Returning to our context, looking ahead",
    "text": "Exercise 8: Returning to our context, looking ahead\n\nAnswers will vary",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-11-read-in-and-get-to-know-the-weather-data-1",
    "href": "activities/02_foundations_univariate.html#exercise-11-read-in-and-get-to-know-the-weather-data-1",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 11: Read in and get to know the weather data",
    "text": "Exercise 11: Read in and get to know the weather data\n\nweather &lt;- read_csv(\"https://raw.githubusercontent.com/Mac-STAT/data/main/weather_3_locations.csv\")\n\nRows: 2367 Columns: 24\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr   (6): location, windgustdir, winddir9am, winddir3pm, raintoday, raintom...\ndbl  (17): mintemp, maxtemp, rainfall, evaporation, sunshine, windgustspeed,...\ndate  (1): date\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-12-exploring-the-data-structure-1",
    "href": "activities/02_foundations_univariate.html#exercise-12-exploring-the-data-structure-1",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 12: Exploring the data structure",
    "text": "Exercise 12: Exploring the data structure\nCheck out the basic features of the weather data.\n\n# Examine the first six cases\nhead(weather)\n\n# A tibble: 6 √ó 24\n  date       location  mintemp maxtemp rainfall evaporation sunshine windgustdir\n  &lt;date&gt;     &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      \n1 2020-01-01 Wollongo‚Ä¶    17.1    23.1        0          NA       NA SSW        \n2 2020-01-02 Wollongo‚Ä¶    17.7    24.2        0          NA       NA SSW        \n3 2020-01-03 Wollongo‚Ä¶    19.7    26.8        0          NA       NA NE         \n4 2020-01-04 Wollongo‚Ä¶    20.4    35.5        0          NA       NA SSW        \n5 2020-01-05 Wollongo‚Ä¶    19.8    21.4        0          NA       NA SSW        \n6 2020-01-06 Wollongo‚Ä¶    18.3    22.9        0          NA       NA NE         \n# ‚Ñπ 16 more variables: windgustspeed &lt;dbl&gt;, winddir9am &lt;chr&gt;, winddir3pm &lt;chr&gt;,\n#   windspeed9am &lt;dbl&gt;, windspeed3pm &lt;dbl&gt;, humidity9am &lt;dbl&gt;,\n#   humidity3pm &lt;dbl&gt;, pressure9am &lt;dbl&gt;, pressure3pm &lt;dbl&gt;, cloud9am &lt;dbl&gt;,\n#   cloud3pm &lt;dbl&gt;, temp9am &lt;dbl&gt;, temp3pm &lt;dbl&gt;, raintoday &lt;chr&gt;,\n#   risk_mm &lt;dbl&gt;, raintomorrow &lt;chr&gt;\n\n# Find the dimensions of the data\ndim(weather)\n\n[1] 2367   24\n\n\nA case represents a day of the year in a particular area (Hobart, Uluru, Wollongong as seen by the location variable).",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-13-exploring-rainfall-1",
    "href": "activities/02_foundations_univariate.html#exercise-13-exploring-rainfall-1",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 13: Exploring rainfall",
    "text": "Exercise 13: Exploring rainfall\nThe raintoday variable contains information about rainfall.\n\nraintoday is categorical (No, Yes)\nIt is more common to have no rain.\n\n\n# Visualization\nggplot(weather, aes(x = raintoday)) +\n    geom_bar()\n\n\n\n\n\n\n\n# Numerical summaries\nweather %&gt;% \n    count(raintoday)\n\n# A tibble: 3 √ó 2\n  raintoday     n\n  &lt;chr&gt;     &lt;int&gt;\n1 No         1864\n2 Yes         446\n3 &lt;NA&gt;         57",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-14-exploring-temperature-1",
    "href": "activities/02_foundations_univariate.html#exercise-14-exploring-temperature-1",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 14: Exploring temperature",
    "text": "Exercise 14: Exploring temperature\nThe maxtemp variable contains information on the daily high temperature.\n\nmaxtemp is quantitative\nThe typical max temperature is around 23 degrees Celsius (with an average of 23.62 and a median of 22 degrees). The max temperatures ranged from 8.6 to 45.4 degrees. Finally, on the typical day, the max temp falls about 7.8 degrees from the mean. There are multiple modes in the distribution of max temperature‚Äîthis likely reflects the different cities in the dataset.\n\n\n# Visualization\nggplot(weather, aes(x = maxtemp)) + \n    geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 34 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n# Numerical summaries\nsummary(weather$maxtemp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   8.60   18.10   22.00   23.62   27.40   45.40      34 \n\n# There are missing values (NAs) in this variable, so we add\n# the na.rm = TRUE argument\nweather %&gt;% \n    summarize(sd(maxtemp, na.rm = TRUE))\n\n# A tibble: 1 √ó 1\n  `sd(maxtemp, na.rm = TRUE)`\n                        &lt;dbl&gt;\n1                        7.80",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/02_foundations_univariate.html#exercise-15-customizing-challenge-1",
    "href": "activities/02_foundations_univariate.html#exercise-15-customizing-challenge-1",
    "title": "Univariate visualization and summaries",
    "section": "Exercise 15: Customizing! (CHALLENGE)",
    "text": "Exercise 15: Customizing! (CHALLENGE)\n\n\n\n\nggplot(weather, aes(x = maxtemp)) + \n    geom_histogram() + \n    labs(x = \"Maximum temperature\", y = \"Number of days\", title = \"Distribution of max temperatures in Perth\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 34 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Make the bars green\nggplot(weather, aes(x = raintoday)) + \n    geom_bar(fill = \"green\")",
    "crumbs": [
      "Univariate visualization and summaries"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html",
    "href": "activities/04_slr_formalization.html",
    "title": "Simple linear regression: formalizing concepts",
    "section": "",
    "text": "You can download a template file to work with here.\nFile organization: Save this file in the ‚ÄúActivities‚Äù subfolder of your ‚ÄúSTAT155‚Äù folder.\n\n\n\nBy the end of this lesson, you should be able to:\n\nDifferentiate between a response / outcome variable and a predictor / explanatory variable\nWrite a model formula for a simple linear regression model with a quantitative predictor\nWrite R code to fit a linear regression model\nInterpret the intercept and slope coefficients in a simple linear regression model with a quantitative predictor\nCompute expected / predicted / fitted values and residuals from a linear regression model formula\nInterpret predicted values and residuals in the context of the data\nExplain the connection between residuals and the least squares criterion\n\n\n\n\nBefore class you should have read and/or watched:\n\nReading: Sections 2.8, 3.1-3.3, 3.6 in the STAT 155 Notes\nVideos:\n\nSummarizing the Relationships between Two Quantitative Variables (Time: 12:12)\nIntroduction to Linear Models (Time: 10:57)\nMethod of Least Squares (Time: 5:10)\nInterpretation of Intercept and Slope (Time: 11:09)\nR Code for Fitting a Linear Model (Time: 11:07)\n\n\n\n\n\nLast class we were looking at powerlifting data.\n\nGuiding question: Are bigger (heavier) lifters stronger for their weight (SWR = total lifted/bodyweight)?\n\n\n# Load packages and import data\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nlifts &lt;- read_csv(\"https://mac-stat.github.io/data/powerlifting.csv\")\n\nlifts &lt;- lifts %&gt;% \n    mutate(SWR = TotalKg/BodyweightKg)\n\n\nggplot(lifts, aes(x = BodyweightKg, y = SWR)) +\n    geom_point() +\n    geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 8752 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 8752 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nObservations:\n\nThe relationship looks slightly curved: positively correlated at first and then negatively correlated afterward.\nThe strong positive slope at the beginning and strong negative slope at the end are caused by a few outlier observations and are less trustworthy.\n\nThe height of the gray band around the blue trend represents uncertainty about the value of the blue line at that location. The band is thicker towards the ends of the plot.\n\nThere may be a slightly positive relationship between 50kg and 80kg, but afterward the relationship looks clearly negative.\n\nA natural next question to ask is:\n\nHow steep are the upsloping and downsloping parts of this trend?\n\nBecause our pre-class material focused on single lines as models, let‚Äôs focus on the downslope.\n\nWe can focus on lifters above 80kg using the filter() function from the dplyr package for data wrangling. filter() selectively keeps rows based on a condition:\n\nlifts_subs &lt;- lifts %&gt;% \n    filter(BodyweightKg &gt; 80)\n\nLet‚Äôs look at the relationship again with a linear smoothing line (a line of best fit):\n\nggplot(lifts_subs, aes(x = BodyweightKg, y = SWR)) +\n    geom_point() +\n    geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 3863 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 3863 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe line is governed by a formula that looks like:\n\\[\ny = a + bx\n\\]\nWe can rewrite that formula for our context, where SWR is our outcome or response variable (the variable we‚Äôre interested in explaining) and BodyweightKg is a predictor or explanatory variable:\n\\[\n\\text{SWR} = a + b \\,\\text{BodyweightKg}\n\\]\n‚ÄúBest‚Äù is determined by the least squares criterion: we find the values of \\(a\\) and \\(b\\) such that we minimize the sum of squared residuals. A residual is the vertical distance from a point to the line:\n\\[\n\\text{residual} = \\text{observed y} - \\text{predicted y}\n\\]\nIn other words, a residual quantifies the error in our model. The idea with the least squares criterion is that we want our line to minimize these errors.\n\nNote that we will be writing the formula for our line in a slightly different way. Instead of:\n\\[\n\\text{SWR} = a + b \\,\\text{BodyweightKg}\n\\]\nWe will be writing model formula as such:\n\\[\nE[\\text{SWR} \\mid \\text{BodyweightKg}] = \\beta_0 + \\beta_1\\text{BodyweightKg}\n\\]\n\nThe \\(E[]\\) is read as ‚Äúexpected value‚Äù which is another way to describe the average value.\n\nExpected value is important in describing our line because our line describes the trend in our data on average. It does not describe what to expect for any individual case.\n\nThe \\(E[\\text{SWR} \\mid \\text{BodyweightKg}]\\) part is read as ‚Äúexpected value of SWR given body weight‚Äù. This is saying that the average value of SWR is function of body weight.\nThe expected value shows up in the way that we interpret the intercept and slope in our line:\n\nIntercept \\(\\beta_0\\): The average SWR for someone who weights 0kg. (Not meaningful! But the intercept is important for placing our line vertically)\nSlope \\(\\beta_1\\): Every 1kg increase in body weight is linked with a \\(\\beta_1\\) change in SWR on average.\n\n\nLet‚Äôs see these interpretations with numbers:\n\nmod &lt;- lm(SWR ~ BodyweightKg, data = lifts_subs)\n\ncoef(summary(mod))\n\n                Estimate   Std. Error   t value      Pr(&gt;|t|)\n(Intercept)   6.44448472 0.0571735021 112.71803  0.000000e+00\nBodyweightKg -0.02046372 0.0005710373 -35.83604 1.538229e-277\n\n\n\nThe ‚ÄúEstimate‚Äù column contains the estimated coefficients in our model (the intercept and slope).\n\nFull model formula: \\(E[\\text{SWR} \\mid \\text{BodyweightKg}] = 6.44 - 0.02\\,\\text{BodyweightKg}\\)\nIntercept \\(\\beta_0\\): The average SWR for someone who weights 0kg is 6.44. (Not meaningful! No one weighs 0kg!)\nSlope \\(\\beta_1\\): Every 1kg increase in body weight is linked with a 0.02 decrease in SWR. So a 10kg increase in body weight is linked with a 0.2 decrease in SWR on average. Interpreting this coefficient addresses our guiding question.\n\nKey features of slope interpretation:\n\nUnits: Provided units of kg for the predictor. Note that there are no units for SWR because we‚Äôre dividing kilograms lifted by kilograms of body weight.\nAverages vs.¬†individuals: On average lifters who differ by 1kg in body weight will differ in SWR by 0.02, but any given pair of lifters who differ by 1kg in body weight will not always differ in SWR by 0.02.\nCausal language: We want to be careful to not say ‚ÄúIncreasing your body weight by 1 kg will decrease your SWR by 0.02 on average‚Äù because there may be other factors at play.\n\nFor example, maybe heavier lifters tend to also use equipment that helps them lift more while lighter lifters don‚Äôt.\nThis might make heavier lifters seem stronger than they are.\nWhen accounting for equipment usage, the relationship between body weight and SWR might change.\n\n\n\n\nWe can use our estimated model formula to get predictions and residuals.\nWhat would we expect the SWR to be for a lifter who weighs 100kg? (On average, what is the SWR for a 100kg lifter?)\n\n6.44 - 0.02 * 100\n\n[1] 4.44\n\n\n\nOn average, a lifter who weighs 100kg is expected to have a SWR of 4.44.\n\nSuppose that Alice is a 100kg lifter who has a SWR of 5.5. What would her residual be?\n\n# residual = observed y - predicted y\n5.5 - 4.44\n\n[1] 1.06\n\n\n\nWe underpredict Alice‚Äôs SWR by 1.06.",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#learning-goals",
    "href": "activities/04_slr_formalization.html#learning-goals",
    "title": "Simple linear regression: formalizing concepts",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDifferentiate between a response / outcome variable and a predictor / explanatory variable\nWrite a model formula for a simple linear regression model with a quantitative predictor\nWrite R code to fit a linear regression model\nInterpret the intercept and slope coefficients in a simple linear regression model with a quantitative predictor\nCompute expected / predicted / fitted values and residuals from a linear regression model formula\nInterpret predicted values and residuals in the context of the data\nExplain the connection between residuals and the least squares criterion",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#readings-and-videos",
    "href": "activities/04_slr_formalization.html#readings-and-videos",
    "title": "Simple linear regression: formalizing concepts",
    "section": "",
    "text": "Before class you should have read and/or watched:\n\nReading: Sections 2.8, 3.1-3.3, 3.6 in the STAT 155 Notes\nVideos:\n\nSummarizing the Relationships between Two Quantitative Variables (Time: 12:12)\nIntroduction to Linear Models (Time: 10:57)\nMethod of Least Squares (Time: 5:10)\nInterpretation of Intercept and Slope (Time: 11:09)\nR Code for Fitting a Linear Model (Time: 11:07)",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#review",
    "href": "activities/04_slr_formalization.html#review",
    "title": "Simple linear regression: formalizing concepts",
    "section": "",
    "text": "Last class we were looking at powerlifting data.\n\nGuiding question: Are bigger (heavier) lifters stronger for their weight (SWR = total lifted/bodyweight)?\n\n\n# Load packages and import data\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nlifts &lt;- read_csv(\"https://mac-stat.github.io/data/powerlifting.csv\")\n\nlifts &lt;- lifts %&gt;% \n    mutate(SWR = TotalKg/BodyweightKg)\n\n\nggplot(lifts, aes(x = BodyweightKg, y = SWR)) +\n    geom_point() +\n    geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 8752 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 8752 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nObservations:\n\nThe relationship looks slightly curved: positively correlated at first and then negatively correlated afterward.\nThe strong positive slope at the beginning and strong negative slope at the end are caused by a few outlier observations and are less trustworthy.\n\nThe height of the gray band around the blue trend represents uncertainty about the value of the blue line at that location. The band is thicker towards the ends of the plot.\n\nThere may be a slightly positive relationship between 50kg and 80kg, but afterward the relationship looks clearly negative.\n\nA natural next question to ask is:\n\nHow steep are the upsloping and downsloping parts of this trend?\n\nBecause our pre-class material focused on single lines as models, let‚Äôs focus on the downslope.\n\nWe can focus on lifters above 80kg using the filter() function from the dplyr package for data wrangling. filter() selectively keeps rows based on a condition:\n\nlifts_subs &lt;- lifts %&gt;% \n    filter(BodyweightKg &gt; 80)\n\nLet‚Äôs look at the relationship again with a linear smoothing line (a line of best fit):\n\nggplot(lifts_subs, aes(x = BodyweightKg, y = SWR)) +\n    geom_point() +\n    geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 3863 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 3863 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe line is governed by a formula that looks like:\n\\[\ny = a + bx\n\\]\nWe can rewrite that formula for our context, where SWR is our outcome or response variable (the variable we‚Äôre interested in explaining) and BodyweightKg is a predictor or explanatory variable:\n\\[\n\\text{SWR} = a + b \\,\\text{BodyweightKg}\n\\]\n‚ÄúBest‚Äù is determined by the least squares criterion: we find the values of \\(a\\) and \\(b\\) such that we minimize the sum of squared residuals. A residual is the vertical distance from a point to the line:\n\\[\n\\text{residual} = \\text{observed y} - \\text{predicted y}\n\\]\nIn other words, a residual quantifies the error in our model. The idea with the least squares criterion is that we want our line to minimize these errors.\n\nNote that we will be writing the formula for our line in a slightly different way. Instead of:\n\\[\n\\text{SWR} = a + b \\,\\text{BodyweightKg}\n\\]\nWe will be writing model formula as such:\n\\[\nE[\\text{SWR} \\mid \\text{BodyweightKg}] = \\beta_0 + \\beta_1\\text{BodyweightKg}\n\\]\n\nThe \\(E[]\\) is read as ‚Äúexpected value‚Äù which is another way to describe the average value.\n\nExpected value is important in describing our line because our line describes the trend in our data on average. It does not describe what to expect for any individual case.\n\nThe \\(E[\\text{SWR} \\mid \\text{BodyweightKg}]\\) part is read as ‚Äúexpected value of SWR given body weight‚Äù. This is saying that the average value of SWR is function of body weight.\nThe expected value shows up in the way that we interpret the intercept and slope in our line:\n\nIntercept \\(\\beta_0\\): The average SWR for someone who weights 0kg. (Not meaningful! But the intercept is important for placing our line vertically)\nSlope \\(\\beta_1\\): Every 1kg increase in body weight is linked with a \\(\\beta_1\\) change in SWR on average.\n\n\nLet‚Äôs see these interpretations with numbers:\n\nmod &lt;- lm(SWR ~ BodyweightKg, data = lifts_subs)\n\ncoef(summary(mod))\n\n                Estimate   Std. Error   t value      Pr(&gt;|t|)\n(Intercept)   6.44448472 0.0571735021 112.71803  0.000000e+00\nBodyweightKg -0.02046372 0.0005710373 -35.83604 1.538229e-277\n\n\n\nThe ‚ÄúEstimate‚Äù column contains the estimated coefficients in our model (the intercept and slope).\n\nFull model formula: \\(E[\\text{SWR} \\mid \\text{BodyweightKg}] = 6.44 - 0.02\\,\\text{BodyweightKg}\\)\nIntercept \\(\\beta_0\\): The average SWR for someone who weights 0kg is 6.44. (Not meaningful! No one weighs 0kg!)\nSlope \\(\\beta_1\\): Every 1kg increase in body weight is linked with a 0.02 decrease in SWR. So a 10kg increase in body weight is linked with a 0.2 decrease in SWR on average. Interpreting this coefficient addresses our guiding question.\n\nKey features of slope interpretation:\n\nUnits: Provided units of kg for the predictor. Note that there are no units for SWR because we‚Äôre dividing kilograms lifted by kilograms of body weight.\nAverages vs.¬†individuals: On average lifters who differ by 1kg in body weight will differ in SWR by 0.02, but any given pair of lifters who differ by 1kg in body weight will not always differ in SWR by 0.02.\nCausal language: We want to be careful to not say ‚ÄúIncreasing your body weight by 1 kg will decrease your SWR by 0.02 on average‚Äù because there may be other factors at play.\n\nFor example, maybe heavier lifters tend to also use equipment that helps them lift more while lighter lifters don‚Äôt.\nThis might make heavier lifters seem stronger than they are.\nWhen accounting for equipment usage, the relationship between body weight and SWR might change.\n\n\n\n\nWe can use our estimated model formula to get predictions and residuals.\nWhat would we expect the SWR to be for a lifter who weighs 100kg? (On average, what is the SWR for a 100kg lifter?)\n\n6.44 - 0.02 * 100\n\n[1] 4.44\n\n\n\nOn average, a lifter who weighs 100kg is expected to have a SWR of 4.44.\n\nSuppose that Alice is a 100kg lifter who has a SWR of 5.5. What would her residual be?\n\n# residual = observed y - predicted y\n5.5 - 4.44\n\n[1] 1.06\n\n\n\nWe underpredict Alice‚Äôs SWR by 1.06.",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#exercise-1-get-to-know-the-data",
    "href": "activities/04_slr_formalization.html#exercise-1-get-to-know-the-data",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Exercise 1: Get to know the data",
    "text": "Exercise 1: Get to know the data\nCreate a new code chunk to look at the first few rows of the data and learn how much data (in terms of cases and variables) we have.\n\nWhat does a case represent?\nHow many and what kinds of variables do we have?\nThinking about the who, what, when, where, why, and how of this data, which of the 5W‚Äôs + H seem most relevant to our investigations? Explain your thoughts.",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#exercise-2-get-to-know-the-outcomeresponse-variable",
    "href": "activities/04_slr_formalization.html#exercise-2-get-to-know-the-outcomeresponse-variable",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Exercise 2: Get to know the outcome/response variable",
    "text": "Exercise 2: Get to know the outcome/response variable\nLet‚Äôs get acquainted with the riders_registered variable.\n\nConstruct an appropriate plot to visualize the distribution of this variable, and compute appropriate numerical summaries.\nWrite a good paragraph interpreting the plot and numerical summaries.",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#exercise-3-explore-the-relationship-between-ridership-and-temperature",
    "href": "activities/04_slr_formalization.html#exercise-3-explore-the-relationship-between-ridership-and-temperature",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Exercise 3: Explore the relationship between ridership and temperature",
    "text": "Exercise 3: Explore the relationship between ridership and temperature\nWe‚Äôd like to understand how daily ridership among registered users relates with the temperature that it feels like that day (temp_feel).\n\nWhat type of plot would be appropriate to visualize this relationship? Sketch and describe what you expect this plot to look like.\nCreate an appropriate plot using ggplot(). How does the plot compare to what you predicted?\nAdd the following two lines after your plot to add a linear (blue) and curved (red) smoothing line. What do you notice? Is a simple linear regression model appropriate for this data?\n\n\n# Add a red straight line of best fit and a blue curve of best fit\nYOUR_PLOT +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n    geom_smooth(color = \"blue\", se = FALSE)",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#exercise-4-filtering-our-data",
    "href": "activities/04_slr_formalization.html#exercise-4-filtering-our-data",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Exercise 4: Filtering our data",
    "text": "Exercise 4: Filtering our data\nThe relationship between registered riders and temperature looks linear below 80 degrees. We can use the filter() function from the dplyr package to subset our cases. (We‚Äôll learn techniques soon for handling this nonlinear relationship.)\nIf we wanted to only keep cases where registered ridership was greater than 2000, we would use the following code:\n\n# The %&gt;% is called a \"pipe\" and feeds what comes before it\n# into what comes after (bikes data is \"fed into\" the filter() function)\nNEW_DATASET_NAME &lt;- bikes %&gt;% \n    filter(riders_registered &gt; 2000)\n\nAdapt the example above to create a new dataset called bikes_sub that only keeps cases where the felt temperature is less than 80 degrees.",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#exercise-5-model-fitting-and-coefficient-interpretation",
    "href": "activities/04_slr_formalization.html#exercise-5-model-fitting-and-coefficient-interpretation",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Exercise 5: Model fitting and coefficient interpretation",
    "text": "Exercise 5: Model fitting and coefficient interpretation\nLet‚Äôs fit a simple linear regression model and examine the results. Step through code chunk slowly, and make note of new code.\n\n# Construct and save the model as bike_mod\n# What's the purpose of \"riders_registered ~ temp_feel\"?\n# What's the purpose of \"data = bikes_sub\"?\nbike_mod &lt;- lm(riders_registered ~ temp_feel, data = bikes_sub)\n\n\n# A long summary of the model stored in bike_mod\nsummary(bike_mod)\n\n\n# A simplified model summary\ncoef(summary(bike_mod))\n\n\nUsing the model summary output, complete the following model formula:\nE[riders_registered | temp_feel] = ___ + ___ * temp_feel\nInterpret the intercept in terms of the data context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases. Is the intercept meaningful in this situation?\nInterpret the slope in terms of the data context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases.",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#exercise-6-predictions-and-residuals",
    "href": "activities/04_slr_formalization.html#exercise-6-predictions-and-residuals",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Exercise 6: Predictions and residuals",
    "text": "Exercise 6: Predictions and residuals\nOn August 17, 2012, the temp_feel was 53.816 degrees and there were 5665 riders. We can get data for this day using the filter() and select() dplyr functions. Note, but don‚Äôt worry about the syntax ‚Äì we haven‚Äôt learned this yet:\n\nbikes_sub %&gt;% \n    filter(date == \"2012-08-17\") %&gt;% \n    select(riders_registered, temp_feel) \n\n\nPeak back at the scatterplot. Identify which point corresponds to August 17, 2012. Is it close to the trend? Were there more riders than expected or fewer than expected?\nUse your model formula from the previous exercise to predict the ridership on August 17, 2012 from the temperature on that day. (That is, where do days with this temperature fall on the model trend line? How many registered riders would we expect on a 53.816 degree day?)\nCheck your part b calculation using the predict() function. Take careful note of the syntax ‚Äì there‚Äôs a lot going on!\n\n\n# What is the purpose of newdata = ___???\npredict(bike_mod, newdata = data.frame(temp_feel = 53.816))\n\n\nCalculate the residual or prediction error. How far does the observed ridership fall from the model prediction?\nresidual = observed y - predicted y = ???\nAre positive residuals above or below the trend line? When we have positive residuals, does the model over- or under-estimate ridership? Repeat these questions for negative residuals.\nFor an 85 degree day, how many registered riders would we expect? Do you think it‚Äôs a good idea to make this prediction? (Revisit the visualization and filtering we did in Exercises 3 and 4.)",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#exercise-7-changing-temperature-units-challenge",
    "href": "activities/04_slr_formalization.html#exercise-7-changing-temperature-units-challenge",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Exercise 7: Changing temperature units (CHALLENGE)",
    "text": "Exercise 7: Changing temperature units (CHALLENGE)\nSuppose we had measured temperature in degrees Celsius rather than degrees Fahrenheit. How do you think our intercept and slope estimates, and their coefficient interpretations, would change?",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#reflection",
    "href": "activities/04_slr_formalization.html#reflection",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Reflection",
    "text": "Reflection\nStatistics is a particular kind of language and collection of tools for channeling curiosity to improve our world.\nReview the learning objectives at the top of this file and the flow of today‚Äôs activity. How do the concepts we practiced today facilitate curiosity?\n\nResponse: Put your response here.",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#render-your-work",
    "href": "activities/04_slr_formalization.html#render-your-work",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Render your work",
    "text": "Render your work\n\nClick the ‚ÄúRender‚Äù button in the menu bar for this pane (blue arrow pointing right). This will create an HTML file containing all of the directions, code, and responses from this activity. A preview of the HTML will appear in the browser.\nScroll through and inspect the document to check that your work translated to the HTML format correctly.\nClose the browser tab.\nGo to the ‚ÄúBackground Jobs‚Äù pane in RStudio and click the Stop button to end the rendering process.\nNavigate to your ‚ÄúActivities‚Äù subfolder within your ‚ÄúSTAT155‚Äù folder and locate the HTML file. You can open it again in your browser to double check.",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#exercise-8-ridership-and-windspeed",
    "href": "activities/04_slr_formalization.html#exercise-8-ridership-and-windspeed",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Exercise 8: Ridership and windspeed",
    "text": "Exercise 8: Ridership and windspeed\nLet‚Äôs pull together everything that you‚Äôve practiced in the preceding exercises to investigate the relationship between riders_registered and windspeed. Go back to using the bikes dataset (instead of bikes_sub) because we no longer need to only keep days less than 80 degrees.\n\n# Construct and interpret a visualization of this relationship\n# Include a representation of the relationship trend\n\n\n# Use lm to construct a model of riders_registered vs windspeed\n# Save this as bike_mod2\n\n\n# Get a short summary of this model\n\n\nSummarize your observations from the visualizations.\nWrite out a formula for the model trend.\nInterpret both the intercept and the windspeed coefficient. (Note: What does a negative slope indicate?)\nUse this model to predict the ridership on August 17, 2012 and calculate the corresponding residual. (Note: You‚Äôll first need to find the windspeed on this date!)",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#exercise-9-data-drills-filter-select-summarize",
    "href": "activities/04_slr_formalization.html#exercise-9-data-drills-filter-select-summarize",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Exercise 9: Data drills (filter, select, summarize)",
    "text": "Exercise 9: Data drills (filter, select, summarize)\nThis exercise is designed to help you keep building your dplyr skills. These skills are important to data cleaning and digging, which in turn is important to really making meaning of our data. We‚Äôll work with a simpler set of 10 data points:\n\nnew_bikes &lt;- bikes %&gt;% \n    select(date, temp_feel, humidity, riders_registered, day_of_week) %&gt;% \n    head(10)\n\n\nVerb 1: summarize\nThus far, in the dplyr grammar you‚Äôve seen 3 verbs or action words: summarize(), select(), filter(). Try out the following code and then summarize the point of the summarize() function:\n\nnew_bikes %&gt;% \n    summarize(mean(temp_feel), mean(humidity))\n\n\n\nVerb 2: select\nTry out the following code and then summarize the point of the select() function:\n\nnew_bikes %&gt;%\n    select(date, temp_feel)\n\n\nnew_bikes %&gt;% \n    select(-date, -temp_feel)\n\n\n\nVerb 3: filter\nTry out the following code and then summarize the point of the filter() function:\n\nnew_bikes %&gt;% \n    filter(riders_registered &gt; 850)\n\n\nnew_bikes %&gt;% \n    filter(day_of_week == \"Sat\")\n\n\nnew_bikes %&gt;% \n    filter(riders_registered &gt; 850, day_of_week == \"Sat\")",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#exercise-10-your-turn",
    "href": "activities/04_slr_formalization.html#exercise-10-your-turn",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Exercise 10: Your turn",
    "text": "Exercise 10: Your turn\nUse dplyr verbs to complete each task below.\n\n# Keep only information about the humidity and day of week\n\n# Keep only information about the humidity and day of week using a different approach\n\n# Keep only information for Sundays\n\n# Keep only information for Sundays with temperatures below 50\n\n# Calculate the maximum and minimum temperatures",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#exercise-1-get-to-know-the-data-1",
    "href": "activities/04_slr_formalization.html#exercise-1-get-to-know-the-data-1",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Exercise 1: Get to know the data",
    "text": "Exercise 1: Get to know the data\n\ndim(bikes)\n\n[1] 731  15\n\nhead(bikes)\n\n# A tibble: 6 √ó 15\n  date       season  year month day_of_week weekend holiday temp_actual\n  &lt;date&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;lgl&gt;   &lt;chr&gt;         &lt;dbl&gt;\n1 2011-01-01 winter  2011 Jan   Sat         TRUE    no             57.4\n2 2011-01-02 winter  2011 Jan   Sun         TRUE    no             58.8\n3 2011-01-03 winter  2011 Jan   Mon         FALSE   no             46.5\n4 2011-01-04 winter  2011 Jan   Tue         FALSE   no             46.8\n5 2011-01-05 winter  2011 Jan   Wed         FALSE   no             48.7\n6 2011-01-06 winter  2011 Jan   Thu         FALSE   no             47.1\n# ‚Ñπ 7 more variables: temp_feel &lt;dbl&gt;, humidity &lt;dbl&gt;, windspeed &lt;dbl&gt;,\n#   weather_cat &lt;chr&gt;, riders_casual &lt;dbl&gt;, riders_registered &lt;dbl&gt;,\n#   riders_total &lt;dbl&gt;\n\n\n\nA case represents a day of the year.\nWe have 15 variables broadly concerning weather, day of week information, whether the day is a holiday.\nLots of answers are reasonable here! When and where seem to be particularly relevant because this is for a rideshare based in Washington DC with data from 2011-2012. Ridership likely changes a lot from city to city and over time.",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#exercise-2-get-to-know-the-outcomeresponse-variable-1",
    "href": "activities/04_slr_formalization.html#exercise-2-get-to-know-the-outcomeresponse-variable-1",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Exercise 2: Get to know the outcome/response variable",
    "text": "Exercise 2: Get to know the outcome/response variable\nThe distribution of the riders_registered variable looks fairly symmetric. On average there are about 3600 registered riders per day (mean = 3656, median = 3662). On any given day, the number of registered riders is about 1560 from the mean. There seem to be a small number of low outliers (minimum ridership was 20).\n\nggplot(bikes, aes(x = riders_registered)) +\n    geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(bikes, aes(y = riders_registered)) +\n    geom_boxplot()\n\n\n\n\n\n\n\nsummary(bikes$riders_registered)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n     20    2497    3662    3656    4776    6946 \n\nbikes %&gt;% \n    summarize(sd(riders_registered))\n\n# A tibble: 1 √ó 1\n  `sd(riders_registered)`\n                    &lt;dbl&gt;\n1                   1560.",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#exercise-3-explore-the-relationship-between-ridership-and-temperature-1",
    "href": "activities/04_slr_formalization.html#exercise-3-explore-the-relationship-between-ridership-and-temperature-1",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Exercise 3: Explore the relationship between ridership and temperature",
    "text": "Exercise 3: Explore the relationship between ridership and temperature\nWe‚Äôd like to understand how daily ridership among registered users relates with the temperature that it feels like that day (temp_feel).\n\nScatterplot (outcome and predictor are both quantitative)\n\n\n\nggplot(bikes, aes(x = temp_feel, y = riders_registered)) +\n    geom_point()\n\n\n\n\n\n\n\n\n\nIf we only displayed the red line of best fit on the plot, we might miss the slight downward trend at the highest temperatures that we can see more clearly with the blue curve of best fit. A linear model is not appropriate if fit to the whole range of the data, but there does seem to be a linear relationship between ridership and temperature below 80 degrees Fahrenheit.\n\n\n# Add a red straight line of best fit and a blue curve of best fit\nggplot(bikes, aes(x = temp_feel, y = riders_registered)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n    geom_smooth(color = \"blue\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#exercise-4-filtering-our-data-1",
    "href": "activities/04_slr_formalization.html#exercise-4-filtering-our-data-1",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Exercise 4: Filtering our data",
    "text": "Exercise 4: Filtering our data\n\n# The %&gt;% is called a \"pipe\" and feeds what comes before it\n# into what comes after (bikes data is \"fed into\" the filter() function)\nbikes_sub &lt;- bikes %&gt;% \n    filter(temp_feel &lt; 80)",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#exercise-5-model-fitting-and-coefficient-interpretation-1",
    "href": "activities/04_slr_formalization.html#exercise-5-model-fitting-and-coefficient-interpretation-1",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Exercise 5: Model fitting and coefficient interpretation",
    "text": "Exercise 5: Model fitting and coefficient interpretation\nLet‚Äôs fit a simple linear regression model and examine the results. Step through code chunk slowly, and make note of new code.\n\n# Construct and save the model as bike_mod\n# What's the purpose of \"riders_registered ~ temp_feel\"?\n# What's the purpose of \"data = bikes_sub\"?\nbike_mod &lt;- lm(riders_registered ~ temp_feel, data = bikes_sub)\n\n\n# A long summary of the model stored in bike_mod\nsummary(bike_mod)\n\n\nCall:\nlm(formula = riders_registered ~ temp_feel, data = bikes_sub)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3681.8  -928.3   -98.6   904.9  3496.7 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2486.412    421.379  -5.901 7.37e-09 ***\ntemp_feel      86.493      6.464  13.380  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1267 on 428 degrees of freedom\nMultiple R-squared:  0.2949,    Adjusted R-squared:  0.2933 \nF-statistic:   179 on 1 and 428 DF,  p-value: &lt; 2.2e-16\n\n\n\n# A simplified model summary\ncoef(summary(bike_mod))\n\n               Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -2486.41180 421.379174 -5.900652 7.368345e-09\ntemp_feel      86.49251   6.464247 13.380135 2.349753e-34\n\n\n\nE[riders_registered | temp_feel] = -2486.41180 + 86.49251 * temp_feel\nIntercept interpretation: On days that feel like 0 degrees Fahrenheit, we can expect an average of -2486.41180 riders‚Äîa negative number of riders doesn‚Äôt make sense! This results because of extrapolation‚Äî0 degrees is so far below the minimum temperature in the data. We only have information on the relationship between ridership and temperature in the ~40-100 degree range and have no idea what that relationship looks like outside that range.\nSlope interpretation: Every 1 degree increase in feeling temperature is associated with an average of about 86 more riders.",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#exercise-6-predictions-and-residuals-1",
    "href": "activities/04_slr_formalization.html#exercise-6-predictions-and-residuals-1",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Exercise 6: Predictions and residuals",
    "text": "Exercise 6: Predictions and residuals\nOn August 17, 2012, the temp_feel was 53.816 degrees and there were 5665 riders. We can get data for this day using the filter() and select() dplyr functions. Note, but don‚Äôt worry about the syntax ‚Äì we haven‚Äôt learned this yet:\n\nbikes_sub %&gt;% \n    filter(date == \"2012-08-17\") %&gt;% \n    select(riders_registered, temp_feel) \n\n# A tibble: 1 √ó 2\n  riders_registered temp_feel\n              &lt;dbl&gt;     &lt;dbl&gt;\n1              5665      53.8\n\n\n\nMore riders than expected ‚Äì the point is far above the trend line\n-2486.41180 + 86.49251 * 53.816 = 2168.269\nWe get the same result with predict():\n\n\n# What is the purpose of newdata = ___???\npredict(bike_mod, newdata = data.frame(temp_feel = 53.816))\n\n       1 \n2168.269 \n\n\n\nresidual = 5665 - 2168.269 = 3496.731. On August 17, 2012, there were 3496.731 more riders than would be expected from our model.\n\nPositive residuals are above the trend line‚Äîwe under-estimate ridership.\nNegative residuals are below the trend line‚Äîwe over-estimate ridership.\n\nOn an 85 degree day, we would predict 4865.452 riders. Even though we can compute this prediction, it‚Äôs not a good idea because of extrapolation‚Äìthe data that we used to fit our model was filtered to days less than 80 degrees.\n\n\n-2486.41180 + 86.49251 * 85\n\n[1] 4865.452\n\npredict(bike_mod, newdata = data.frame(temp_feel = 85))\n\n       1 \n4865.451",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#exercise-7-changing-temperature-units-challenge-1",
    "href": "activities/04_slr_formalization.html#exercise-7-changing-temperature-units-challenge-1",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Exercise 7: Changing temperature units (CHALLENGE)",
    "text": "Exercise 7: Changing temperature units (CHALLENGE)\nIf we had measured temperature in degrees Celsius rather than degrees Fahrenheit, both the intercept and slope should change. The intercept would now represent 0 degrees Celsius (32 degrees Fahrenheit) and a one unit change in temperature is now 1 degree Celsius (1.8 degrees Fahrenheit).",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#exercise-8-ridership-and-windspeed-1",
    "href": "activities/04_slr_formalization.html#exercise-8-ridership-and-windspeed-1",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Exercise 8: Ridership and windspeed",
    "text": "Exercise 8: Ridership and windspeed\nLet‚Äôs pull together everything that you‚Äôve practiced in the preceding exercises to investigate the relationship between riders_registered and windspeed. Go back to using the bikes dataset (instead of bikes_sub) because we no longer need to only keep days less than 80 degrees.\n\n# Construct and interpret a visualization of this relationship\n# Include a representation of the relationship trend\nggplot(bikes, aes(x = windspeed, y = riders_registered)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n    geom_smooth(color = \"blue\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Use lm to construct a model of riders_registered vs windspeed\n# Save this as bike_mod2\nbike_mod2 &lt;- lm(riders_registered ~ windspeed, data = bikes)\n\n# Get a short summary of this model\ncoef(summary(bike_mod2))\n\n              Estimate Std. Error   t value      Pr(&gt;|t|)\n(Intercept) 4490.09761  149.65992 30.002005 2.023179e-129\nwindspeed    -65.34145   10.86299 -6.015053  2.844453e-09\n\n\n\nThere‚Äôs a weak, negative relationship ‚Äì ridership tends to be smaller on windier days.\nE[riders_registered | windspeed] = 4490.09761 - 65.34145 windspeed\n\nIntercept: On days with no wind, we‚Äôd expect around 4490 riders. (0 is a little below the minimum of the observed data, but not by much! So extrapolation in interpreting the intercept isn‚Äôt a huge concern.)\nSlope: Every 1mph increase in windspeed is associated with a ridership decrease of 65 riders on average.\n\nSee the code below to predict ridership on August 17, 2012 and calculate the corresponding residual. Note that this residual is smaller than the residual from the temperature model (that residual was 3496.731). This indicates that August 17 was more of an outlier in ridership given the temperature than the windspeed.\n\n\nbikes %&gt;% \n    filter(date == \"2012-08-17\") %&gt;% \n    select(riders_registered, windspeed)\n\n# A tibble: 1 √ó 2\n  riders_registered windspeed\n              &lt;dbl&gt;     &lt;dbl&gt;\n1              5665      15.5\n\n# prediction\n4490.09761 - 65.34145 * 15.50072\n\n[1] 3477.258\n\n# residual \n5665 - 3477.258\n\n[1] 2187.742",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#exercise-9-data-drills-filter-select-summarize-1",
    "href": "activities/04_slr_formalization.html#exercise-9-data-drills-filter-select-summarize-1",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Exercise 9: Data drills (filter, select, summarize)",
    "text": "Exercise 9: Data drills (filter, select, summarize)\nThis exercise is designed to help you keep building your dplyr skills. These skills are important to data cleaning and digging, which in turn is important to really making meaning of our data. We‚Äôll work with a simpler set of 10 data points:\n\nnew_bikes &lt;- bikes %&gt;% \n    select(date, temp_feel, humidity, riders_registered, day_of_week) %&gt;% \n    head(10)\n\n\nVerb 1: summarize\nsummarize() calculates numerical summaries of variables (columns).\n\nnew_bikes %&gt;% \n    summarize(mean(temp_feel), mean(humidity))\n\n# A tibble: 1 √ó 2\n  `mean(temp_feel)` `mean(humidity)`\n              &lt;dbl&gt;            &lt;dbl&gt;\n1              52.0            0.544\n\n\n\n\nVerb 2: select\nselect() selects variables (columns).\n\nnew_bikes %&gt;%\n    select(date, temp_feel)\n\n# A tibble: 10 √ó 2\n   date       temp_feel\n   &lt;date&gt;         &lt;dbl&gt;\n 1 2011-01-01      64.7\n 2 2011-01-02      63.8\n 3 2011-01-03      49.0\n 4 2011-01-04      51.1\n 5 2011-01-05      52.6\n 6 2011-01-06      53.0\n 7 2011-01-07      50.8\n 8 2011-01-08      46.6\n 9 2011-01-09      42.5\n10 2011-01-10      45.6\n\n\n\nnew_bikes %&gt;% \n    select(-date, -temp_feel)\n\n# A tibble: 10 √ó 3\n   humidity riders_registered day_of_week\n      &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n 1    0.806               654 Sat        \n 2    0.696               670 Sun        \n 3    0.437              1229 Mon        \n 4    0.590              1454 Tue        \n 5    0.437              1518 Wed        \n 6    0.518              1518 Thu        \n 7    0.499              1362 Fri        \n 8    0.536               891 Sat        \n 9    0.434               768 Sun        \n10    0.483              1280 Mon        \n\n\n\n\nVerb 3: filter\nfilter() keeps only days (rows) that meet the given condition(s).\n\nnew_bikes %&gt;% \n    filter(riders_registered &gt; 850)\n\n# A tibble: 7 √ó 5\n  date       temp_feel humidity riders_registered day_of_week\n  &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n1 2011-01-03      49.0    0.437              1229 Mon        \n2 2011-01-04      51.1    0.590              1454 Tue        \n3 2011-01-05      52.6    0.437              1518 Wed        \n4 2011-01-06      53.0    0.518              1518 Thu        \n5 2011-01-07      50.8    0.499              1362 Fri        \n6 2011-01-08      46.6    0.536               891 Sat        \n7 2011-01-10      45.6    0.483              1280 Mon        \n\n\n\nnew_bikes %&gt;% \n    filter(day_of_week == \"Sat\")\n\n# A tibble: 2 √ó 5\n  date       temp_feel humidity riders_registered day_of_week\n  &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n1 2011-01-01      64.7    0.806               654 Sat        \n2 2011-01-08      46.6    0.536               891 Sat        \n\n\n\nnew_bikes %&gt;% \n    filter(riders_registered &gt; 850, day_of_week == \"Sat\")\n\n# A tibble: 1 √ó 5\n  date       temp_feel humidity riders_registered day_of_week\n  &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n1 2011-01-08      46.6    0.536               891 Sat",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/04_slr_formalization.html#exercise-10-your-turn-1",
    "href": "activities/04_slr_formalization.html#exercise-10-your-turn-1",
    "title": "Simple linear regression: formalizing concepts",
    "section": "Exercise 10: Your turn",
    "text": "Exercise 10: Your turn\nUse dplyr verbs to complete each task below.\n\n# Keep only information about the humidity and day of week\nnew_bikes %&gt;% \n    select(humidity, day_of_week)\n\n# A tibble: 10 √ó 2\n   humidity day_of_week\n      &lt;dbl&gt; &lt;chr&gt;      \n 1    0.806 Sat        \n 2    0.696 Sun        \n 3    0.437 Mon        \n 4    0.590 Tue        \n 5    0.437 Wed        \n 6    0.518 Thu        \n 7    0.499 Fri        \n 8    0.536 Sat        \n 9    0.434 Sun        \n10    0.483 Mon        \n\n# Keep only information about the humidity and day of week using a different approach\nnew_bikes %&gt;% \n    select(-date, -temp_feel, -riders_registered)\n\n# A tibble: 10 √ó 2\n   humidity day_of_week\n      &lt;dbl&gt; &lt;chr&gt;      \n 1    0.806 Sat        \n 2    0.696 Sun        \n 3    0.437 Mon        \n 4    0.590 Tue        \n 5    0.437 Wed        \n 6    0.518 Thu        \n 7    0.499 Fri        \n 8    0.536 Sat        \n 9    0.434 Sun        \n10    0.483 Mon        \n\n# Keep only information for Sundays\nnew_bikes %&gt;% \n    filter(day_of_week == \"Sun\")\n\n# A tibble: 2 √ó 5\n  date       temp_feel humidity riders_registered day_of_week\n  &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n1 2011-01-02      63.8    0.696               670 Sun        \n2 2011-01-09      42.5    0.434               768 Sun        \n\n# Keep only information for Sundays with temperatures below 50\nnew_bikes %&gt;% \n    filter(day_of_week == \"Sun\", temp_feel &lt; 50)\n\n# A tibble: 1 √ó 5\n  date       temp_feel humidity riders_registered day_of_week\n  &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n1 2011-01-09      42.5    0.434               768 Sun        \n\n# Calculate the maximum and minimum temperatures\nnew_bikes %&gt;% \n    summarize(min(temp_feel), max(temp_feel))\n\n# A tibble: 1 √ó 2\n  `min(temp_feel)` `max(temp_feel)`\n             &lt;dbl&gt;            &lt;dbl&gt;\n1             42.5             64.7",
    "crumbs": [
      "Simple linear regression: formalizing concepts"
    ]
  },
  {
    "objectID": "activities/06_slr_transformations.html",
    "href": "activities/06_slr_transformations.html",
    "title": "Simple linear regression: Transformations",
    "section": "",
    "text": "You can download a template file to work with here.\nFile organization: Save this file in the ‚ÄúActivities‚Äù subfolder of your ‚ÄúSTAT155‚Äù folder.\n\n\n\nBy the end of this lesson, you should be able to:\n\nDistinguish between the different motivations for transformations of variables (interpretation, regression assumptions, etc.)\nDetermine when a particular transformation (center, scale, or log) may be appropriate\nInterpret regression coefficients after a transformation has taken place\n\n\n\n\nPlease watch the following video before class.\n\nVideo: Simple Linear Regression: Transformations\n\nThe following reading is optional.\n\nSection 3.8.4 in the STAT 155 Notes covers log transformations, and the ‚Äúladder of power,‚Äù which we will not cover in class.",
    "crumbs": [
      "Simple linear regression: Transformations"
    ]
  },
  {
    "objectID": "activities/06_slr_transformations.html#learning-goals",
    "href": "activities/06_slr_transformations.html#learning-goals",
    "title": "Simple linear regression: Transformations",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDistinguish between the different motivations for transformations of variables (interpretation, regression assumptions, etc.)\nDetermine when a particular transformation (center, scale, or log) may be appropriate\nInterpret regression coefficients after a transformation has taken place",
    "crumbs": [
      "Simple linear regression: Transformations"
    ]
  },
  {
    "objectID": "activities/06_slr_transformations.html#readings-and-videos",
    "href": "activities/06_slr_transformations.html#readings-and-videos",
    "title": "Simple linear regression: Transformations",
    "section": "",
    "text": "Please watch the following video before class.\n\nVideo: Simple Linear Regression: Transformations\n\nThe following reading is optional.\n\nSection 3.8.4 in the STAT 155 Notes covers log transformations, and the ‚Äúladder of power,‚Äù which we will not cover in class.",
    "crumbs": [
      "Simple linear regression: Transformations"
    ]
  },
  {
    "objectID": "activities/06_slr_transformations.html#exercise-1-location-transformations",
    "href": "activities/06_slr_transformations.html#exercise-1-location-transformations",
    "title": "Simple linear regression: Transformations",
    "section": "Exercise 1: Location transformations",
    "text": "Exercise 1: Location transformations\nLocation transformations are ones that shift a predictor variable up or down by a fixed amount. Using a location transformation is sometimes also called centering a predictor.\nWe‚Äôll use the homes data in this exercise.\n\nFit a linear regression model of Price as a function of Living.Area, and call this model home_mod.\n\n\n# Fit the model\n\n\n# Display model summary output\n\n\nInterpret the intercept and the coefficient for Living.Area. Is the interpretation of the intercept meaningful?\nWe can use a location transformation on Living.Area to ‚Äústart‚Äù it at a more reasonable value. We can see from the summarize() code below that the smallest house is 616 square feet, so let‚Äôs center this predictor at 600 square feet. There is no code to fill in here, but make note of the mutate() syntax.\n\n\nhomes %&gt;% \n    summarize(min(Living.Area))\n\n# What is mutate() doing???\nhomes &lt;- homes %&gt;%\n    mutate(Living.Area.Shifted = Living.Area-600)\n\n\nWe can actually determine the coefficients of the Price ~ Living.Area.Shifted model by hand.\n\nFirst, write out in general terms (without specific numbers) how we would interpret the intercept and slope in this model.\nUse these general interpretations as well as the summary output of home_mod to determine what these new coefficients should be.\n\nNow check your answer to part d by fitting the model.\n\n\n# Fit a model of Price vs. Living.Area.Shifted\n\n\n# Display model summary output",
    "crumbs": [
      "Simple linear regression: Transformations"
    ]
  },
  {
    "objectID": "activities/06_slr_transformations.html#exercise-2-scale-transformations",
    "href": "activities/06_slr_transformations.html#exercise-2-scale-transformations",
    "title": "Simple linear regression: Transformations",
    "section": "Exercise 2: Scale transformations",
    "text": "Exercise 2: Scale transformations\nIn this exercise, we‚Äôll explore the relationship between four-year graduation rate and admissions rate of colleges.\nIn the code chunk below, construct a visualization comparing graduation rate (our outcome variable) and admissions rate (our predictor of interest). Remember that your outcome variable should be on the y-axis, in general!\n\n# Scatterplot of graduation rate vs. admissions rate\n\n\nDescribe the relationship you observe between the two quantitative variables, in terms of correlation (weak/strong, positive/negative). Does the relationship appear to be roughly linear?\nWrite a linear regression model formula of the form E[Y | X] = ‚Ä¶ (filling in Y and X appropriately).\nFit this model in R, and report (don‚Äôt interpret yet!) the slope coefficient and intercept coefficient estimates.\n\n\n# Linear regression model with GradRate as the outcome, AdmisRate as predictor of interest\n\n\nIntercept Estimate: Your response here\n\n\nSlope Estimate: Your response here\n\n\nConsidering the units of AdmisRate, what does it mean for AdmisRate to change by one unit? What are the units for AdmisRate (and GradRate, for that matter!)?\nSuppose I want the interpretation of my slope coefficient for AdmisRate in my linear model to be in terms a ‚Äú1% increase in admissions rate.‚Äù To achieve this, we could mutate our AdmisRate variable to range from 0 to 100. Let‚Äôs do that for GradRate too (just because!):\n\n\n# Mutate\ncollege &lt;- college %&gt;%\n  mutate(AdmisRate = AdmisRate * ___,\n         GradRate = ___ * ___)\n\n\nFit a new linear regression model with the updated AdmisRate and GradRate variables as your predictor of interest and outcome, respectively. Again, report the intercept and slope estimate from your model.\n\n\n# Linear regression model with updated GradRate as the outcome, updated AdmisRate as predictor of interest\n\n\nIntercept Estimate: Your response here\n\n\nSlope Estimate: Your response here\n\nHow have your intercept and slope estimates changed from the previous model, if at all?\n\nInterpret the regression coefficient that corresponds to the estimated linear relationship between admissions and graduation rates, in the context of the problem. Make sure to use non-causal language, include units, and talk about averages rather than individual cases.",
    "crumbs": [
      "Simple linear regression: Transformations"
    ]
  },
  {
    "objectID": "activities/06_slr_transformations.html#exercise-3-log-transformations",
    "href": "activities/06_slr_transformations.html#exercise-3-log-transformations",
    "title": "Simple linear regression: Transformations",
    "section": "Exercise 3: Log transformations",
    "text": "Exercise 3: Log transformations\nThe Big Mac Index has been published by The Economist since 1986 as a metric for comparing purchasing power between countries, giving rise to the phrase Burgernomics. It was developed (sort of jokingly) as a way to explain exchange rates in digestible terms.\nAs an example, suppose a Big Mac in Switzerland costs 6.70 Swiss franc, and in the U.S. a Big Mac costs 5.58 USD. Then the Big Mac Index is 6.70/5.58 = 1.20, and is the implied exchange rate between Swiss franc and USD.\nIf you‚Äôd like to read more about the Big Mac index, here‚Äôs an article in The Economist (this may be behind a pay-wall for you, you can read up to 5 free articles in the Economist per month).\nFor this exercise, we‚Äôll explore the relationship between average teaching salary in a country and the amount of time someone needs to work to be able to afford a Big Mac. The variables we‚Äôll consider are:\n\nbigmac_mins: average minutes to earn 1 Big Mac\ngross_annual_teacher_income: average gross teacher salary in 1 year (USD)\n\n\nCreate an appropriate visualization that displays the relationship between average minutes to earn a Big Mac and gross annual, average teaching salary, and describe what you observe.\n\n\n# Visualization: Big Mac minutes vs. gross annual teacher income\n\n\nExplain why correlation might not be an appropriate numerical summary for the relationship between the two variables you plotted above.\nFit a linear regression model with bigmac_mins as the outcome and gross_annual_teacher_income as the predictor of interest, and interpret the coefficient for gross_annual_teacher_income, in context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases.\n\n\n# Linear regression code\n\n\nPlot residuals vs.¬†fitted values for the model you fit, and describe what you observe. Are there any noticeable patterns in the residuals? Describe them!\n\n\n# Residuals vs. fitted values plot\n\n\nFor which observations do the residuals from the linear regression model appear to be relatively large (i.e.¬†for which observations would predictions fall farthest from observed outcomes)? What possible consequences would this have for people using this model to predict the amount of time it takes for them to earn enough money to afford a Big Mac?\n\nWe‚Äôll now consider a log transformation of teaching salary. In the code chunk below, create a new variable called log_sal that contains the logged values of gross_annual_teacher_income.\n\n# Creating new variable log_sal\nbigmac &lt;- bigmac %&gt;%\n  mutate(log_sal = log(___))\n\n\nCreate an appropriate visualization that displays the relationship between average minutes to earn a Big Mac and logged gross annual, average teaching salary, and describe what you observe. Does correlation seem like it may be an appropriate numerical summary for the relationship between these two variables? Explain why or why not.\nFit a linear regression model with bigmac_mins as the outcome and log_sal as the predictor of interest, and interpret the coefficient for log_sal, in context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases.\nPlot residuals vs.¬†fitted values for the model you fit, and describe what you observe. Are there any noticeable patterns in the residuals? Describe them!\n\n\n# Residuals vs. fitted values plot",
    "crumbs": [
      "Simple linear regression: Transformations"
    ]
  },
  {
    "objectID": "activities/06_slr_transformations.html#reflection",
    "href": "activities/06_slr_transformations.html#reflection",
    "title": "Simple linear regression: Transformations",
    "section": "Reflection",
    "text": "Reflection\nTwo of the main motivations for transforming variables in our regression models is to (1) intentionally change the interpretation of regression coefficients, and (2) to better satisfy linear regression assumptions (e.g.¬†remove ‚Äúpatterns‚Äù from our residual plots). The first is nearly always justified by the scientific context of the research questions you are trying to answer, while the second is a bit more muddy.\nThink about the pros and cons of transforming your variables to satisfy linear regression assumptions. Is there a limit to how much you would be willing to transform your variables? Would transforming too much leave you with un-interpretable regression coefficients?\n\nResponse: Put your response here.",
    "crumbs": [
      "Simple linear regression: Transformations"
    ]
  },
  {
    "objectID": "activities/06_slr_transformations.html#exercise-1-location-transformations-1",
    "href": "activities/06_slr_transformations.html#exercise-1-location-transformations-1",
    "title": "Simple linear regression: Transformations",
    "section": "Exercise 1: Location transformations",
    "text": "Exercise 1: Location transformations\nLocation transformations are ones that shift a predictor variable up or down by a fixed amount. Using a location transformation is sometimes also called centering a predictor.\nWe‚Äôll use the homes data in this exercise.\n\nFit a linear regression model of Price as a function of Living.Area, and call this model home_mod.\n\n\n# Fit the model\nhome_mod &lt;- lm(Price ~ Living.Area, data = homes)\n\n# Display model summary output\ncoef(summary(home_mod))\n\n              Estimate  Std. Error   t value      Pr(&gt;|t|)\n(Intercept) 13439.3940 4992.352849  2.691996  7.171207e-03\nLiving.Area   113.1225    2.682341 42.173065 9.486240e-268\n\n\n\n\nInterpretation of slope: Every 1 square foot increase in living area is associated with an expected / average increase in house price of $113.12.\nInterpretation of intercept: The average/expected house price for a house with zero square feet is $13,439.39. Can a house ever be zero square feet??? Nope! The intercept is meaningless in this case.\n\n\n\n\nhomes %&gt;% \n    summarize(min(Living.Area))\n\n# A tibble: 1 √ó 1\n  `min(Living.Area)`\n               &lt;dbl&gt;\n1                616\n\n# mutate() creates a new variable called Living.Area.Shifted that is equal to Living.Area - 600\nhomes &lt;- homes %&gt;%\n    mutate(Living.Area.Shifted = Living.Area-600)\n\n\n\nIn general terms, the intercept in this model should represent the average house price when Living.Area.Shifted is 0‚Äîin other words when Living.Area is 600 square feet. From the coefficient estimates in home_mod, we can calculate the expected / predicted house price for 600 square foot homes: 13439.394 + (113.123*600) = 81312.89. So we‚Äôre expecting the new intercept to be $81312.89.\nThe slope in this model represents the average price change for each unit change in Living.Area.Shifted (which is the same as a unit change in Living.Area). Based on this, the slope should be the same as in home_mod ($113.12 per square foot).\n\nLines up with work in part d!\n\n\n# Fit a model of Price vs. Living.Area.Shifted\nhome_mod_centered &lt;- lm(Price ~ Living.Area.Shifted, data = homes)\n\n# Display model summary output\ncoef(summary(home_mod_centered))\n\n                      Estimate  Std. Error  t value      Pr(&gt;|t|)\n(Intercept)         81312.9191 3515.879467 23.12733 2.638371e-103\nLiving.Area.Shifted   113.1225    2.682341 42.17307 9.486240e-268",
    "crumbs": [
      "Simple linear regression: Transformations"
    ]
  },
  {
    "objectID": "activities/06_slr_transformations.html#exercise-2-scale-transformations-1",
    "href": "activities/06_slr_transformations.html#exercise-2-scale-transformations-1",
    "title": "Simple linear regression: Transformations",
    "section": "Exercise 2: Scale transformations",
    "text": "Exercise 2: Scale transformations\nIn the code chunk below, construct a visualization comparing graduation rate (our outcome variable) and admissions rate (our predictor of interest). Remember that your outcome variable should be on the y-axis, in general!\n\n# Scatterplot of graduation rate vs. admissions rate\nggplot(college, aes(x = AdmisRate, y = GradRate)) +\n    geom_point() +\n    geom_smooth(se = FALSE) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nThe correlation between admissions and graduation rates appears to be weakly negative. Notably, there are hard boundaries to admissions and graduation rates, since both must fall between 0 and 100%! A few colleges hit up against these boundaries. I would say that, with the exception of the observations that have either 0% graduation rates or 0% admission rates, the relationship does appear to be roughly linear.\nE[GradRate | AdmisRate] = \\(\\beta_0\\) + \\(\\beta_1\\) AdmisRate\n\n\n\n# Linear regression model with GradRate as the outcome, AdmisRate as predictor of interest\nmod &lt;- lm(GradRate ~ AdmisRate, data = college)\nsummary(mod)\n\n\nCall:\nlm(formula = GradRate ~ AdmisRate, data = college)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68409 -0.13681  0.01296  0.15550  0.66204 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.68409    0.01759   38.89   &lt;2e-16 ***\nAdmisRate   -0.34613    0.02330  -14.85   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2088 on 1649 degrees of freedom\nMultiple R-squared:  0.118, Adjusted R-squared:  0.1175 \nF-statistic: 220.6 on 1 and 1649 DF,  p-value: &lt; 2.2e-16\n\n\n\nIntercept Estimate: 0.68409\n\n\nSlope Estimate: -0.34613\n\n\nOne unit of AdmisRate corresponds to a 100% change in admissions rates! The same goes for graduation rate. This is a huge change (in fact, the largest change possible).\nSuppose I want the interpretation of my slope coefficient for AdmisRate in my linear model to be in terms a ‚Äú1% increase in admissions rate.‚Äù To achieve this, we could mutate our AdmisRate variable to range from 0 to 100. Let‚Äôs do that for GradRate too (just because!):\n\n\n# Mutate\ncollege &lt;- college %&gt;%\n  mutate(AdmisRate = AdmisRate * 100,\n         GradRate = GradRate * 100)\n\n\nFit a new linear regression model with the updated AdmisRate and GradRate variables as your predictor of interest and outcome, respectively. Again, report the intercept and slope estimate from your model.\n\n\n# Linear regression model with updated GradRate as the outcome, updated AdmisRate as predictor of interest\nmod_new &lt;- lm(GradRate ~ AdmisRate, data = college)\nsummary(mod_new)\n\n\nCall:\nlm(formula = GradRate ~ AdmisRate, data = college)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-68.409 -13.681   1.296  15.550  66.204 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  68.4088     1.7592   38.89   &lt;2e-16 ***\nAdmisRate    -0.3461     0.0233  -14.85   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.88 on 1649 degrees of freedom\nMultiple R-squared:  0.118, Adjusted R-squared:  0.1175 \nF-statistic: 220.6 on 1 and 1649 DF,  p-value: &lt; 2.2e-16\n\n\n\nIntercept Estimate: 68.4088\n\n\nSlope Estimate: -0.3461\n\nOur intercept estimate is now 100x larger, and our slope estimate has remained the same! The slope remained the same because we multiplied our outcome and our predictor of interest by the same value, and the intercept is 100x larger because we multiplied our outcome by 100 (recall that the intercept is the average expected outcome when ‚Äúx‚Äù is zero).\n\nOn average, we expect colleges that differ in admissions rate by 1% to have 0.35% different graduation rates, with colleges with higher admissions rates having lower graduation rates.",
    "crumbs": [
      "Simple linear regression: Transformations"
    ]
  },
  {
    "objectID": "activities/06_slr_transformations.html#exercise-3-log-transformations-1",
    "href": "activities/06_slr_transformations.html#exercise-3-log-transformations-1",
    "title": "Simple linear regression: Transformations",
    "section": "Exercise 3: Log transformations",
    "text": "Exercise 3: Log transformations\n\n\n\n\n# Visualization: Big Mac minutes vs. gross annual teacher income\nbigmac %&gt;%\n  ggplot(aes(x = gross_annual_teacher_income, y = bigmac_mins)) +\n  geom_point() \n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nAs annual teacher income gets higher, time it takes in minutes to earn a Big Mac decreases, though the relationship does not appear linear. The amount of time it takes to earn a Big Mac is very high when income is below about 10,000 where it sharply decreases, and then decreases at a much lower rate when income is above around 20,000.\n\nCorrelation is a summary of the linear relationship between two quantitative variables, and this relationship does not appear to be linear!\n\n\n\n# Linear regression code\nmod &lt;- lm(bigmac_mins ~ gross_annual_teacher_income, data = bigmac)\nsummary(mod)\n\n\nCall:\nlm(formula = bigmac_mins ~ gross_annual_teacher_income, data = bigmac)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.649  -9.556  -1.784   4.512  43.715 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  5.801e+01  3.104e+00   18.69  &lt; 2e-16 ***\ngross_annual_teacher_income -9.092e-04  9.591e-05   -9.48 6.16e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.4 on 66 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.5766,    Adjusted R-squared:  0.5701 \nF-statistic: 89.86 on 1 and 66 DF,  p-value: 6.164e-14\n\n\nOn average, we expect a one dollar increase in gross annual teacher income to be associated with a decrease in the number of minutes it takes to earn a Big Mac by 9 x 10^(-4) minutes. Stated differently, we expect a ten-thousand dollar increase in gross annual teacher income to be associated with a decrease in the number of minutes it takes to earn a Big Mac by 9 minutes (note that here I did a scale transformation of gross annual teacher income to get this interpretation, which might make more sense when looking at the scale of salary!).\n\n\n\n\n# Residuals vs. fitted values plot\nggplot(mod, aes(x = .fitted, y = .resid)) + \n    geom_point() + \n    geom_hline(yintercept = 0) +\n    geom_smooth(se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe residuals vs.¬†fitted values plot shows a very clear, nonlinear pattern! As fitted values increase, residuals decrease for a while, and then sharply increase once fitted values are higher than around 40 minutes. The spread of residuals around zero also varies, with greater spread for higher fitted values.\n\nThe residuals appear to be large for people with negative fitted values and those with very high fitted values. Recall that a linear model does not ‚Äúknow‚Äù that number of minutes to earn a Big Mac can‚Äôt be negative, in context. If we look at the fitted line from our linear model on a scatterplot (see below)‚Ä¶\n\n\nbigmac %&gt;%\n    ggplot(aes(x = gross_annual_teacher_income, y = bigmac_mins)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWe observe that negative fitted values and very large fitted values occur when annual teacher income is greater than around 70,000 and less than 10,000, respectively. This implies that the model does a worse job at predicting the number of minutes to earn a Big Mac in countries where annual teacher income is either very high or very low.\nWe‚Äôll now consider a log transformation of teaching salary. In the code chunk below, create a new variable called log_sal that contains the logged values of gross_annual_teacher_income.\n\n# Creating new variable log_sal\nbigmac &lt;- bigmac %&gt;%\n    mutate(log_sal = log(gross_annual_teacher_income))\n\n\n\n\n\nbigmac %&gt;%\n    ggplot(aes(log_sal, bigmac_mins)) +\n    geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe relationship between logged annual teaching salary and minutes to earn a Big Mac appears roughly linear, with a weakly negative relationship. Correlation is likely an appropriate numerical summary for the relationship between these two quantitative variables, as the relationship is roughly linear!\n\n\n\n\nmod_log &lt;- lm(bigmac_mins ~ log_sal, data = bigmac)\nsummary(mod_log)\n\n\nCall:\nlm(formula = bigmac_mins ~ log_sal, data = bigmac)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.817  -6.951  -1.241   6.032  41.357 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  210.875     14.687   14.36   &lt;2e-16 ***\nlog_sal      -18.142      1.502  -12.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.21 on 66 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.6886,    Adjusted R-squared:  0.6838 \nF-statistic: 145.9 on 1 and 66 DF,  p-value: &lt; 2.2e-16\n\n\nEach 1 unit increase in logged salary is associated with a 18.14 minute decrease in time to earn a Big Mac on average.\nWe can also use a property of logarithms to interpret the slope of -18.14 in a different way. Suppose we have two salaries: Salary1 and Salary2. If Salary2 is 10% higher than Salary1, then Salary2/Salary1 = 1.1. It is a property of logarithms that log(Salary2/Salary1) = log(Salary2) - log(Salary1). In this case log(Salary2/Salary1) = log(Salary2) - log(Salary1) = log(1.1) = 0.09531018. So a 10% increase in salary is a 0.09 unit increase in the log scale:\n\n# Multiplicative difference of 1.1, or 10% between salaries gives us the \nlog(1.1) * -18.142\n\n[1] -1.729117\n\n\nWhile a 1 unit increase in log salary is associated with an average decrease of 18 Big Mac minutes, a 0.0953 unit increase in log salary (which corresponds to a 10% multiplicative increase), is associated with a 1.7 minute decrease in Big Mac minutes.\nUnderlying math:\nCase 1: Salary = x\n   E[bigmacmin_1] = beta0 + beta1 log(x)\nCase 2: Salary = m*x\n   E[bigmacmin_2] = beta0 + beta1 log(m*x)\n\nE[bigmacmin_2] - E[bigmacmin_1] = beta1 log(m)\n\n\n\n\n# Residuals vs. fitted values plot\nggplot(mod_log, aes(x = .fitted, y = .resid)) + \n    geom_point() + \n    geom_hline(yintercept = 0) +\n    geom_smooth(se = FALSE)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe residuals seem to lie roughly around zero for all possible fitted values, though the spread is still noticably larger for larger fitted values compared to smaller ones. This implies that the linearity assumption is likely satisfied for this model, but equal variance may be a concern.",
    "crumbs": [
      "Simple linear regression: Transformations"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html",
    "href": "activities/08_mlr_intro.html",
    "title": "Introduction to multiple regression",
    "section": "",
    "text": "You can download a template file to work with here.\nFile organization: Save this file in the ‚ÄúActivities‚Äù subfolder of your ‚ÄúSTAT155‚Äù folder.\n\n\n\nBy the end of this lesson, you should be familiar with:\n\nsome limitations of simple linear regression\nthe general goals behind multiple linear regression\nstrategies for visualizing and interpreting multiple linear regression models of \\(y\\) vs 2 predictors, 1 quantitative and 1 categorical\n\n\n\n\nToday is a day to discover ideas, so no readings or videos to go through before class.\n\n\n\nEXAMPLE 1\nLet‚Äôs explore some data on penguins.\nFirst, enter install.packages(\"palmerpenguins\") in the console (not Rmd).\nThen load the penguins data.\nYou can find a codebook for these data by typing ?penguins in your console (not Rmd).\n\n# Load packages\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load data\nlibrary(palmerpenguins)\ndata(penguins)\npenguins &lt;- penguins %&gt;% \n    filter(species != \"Adelie\", bill_length_mm &lt; 57)\n\n# Check it out\nhead(penguins)\n\n# A tibble: 6 √ó 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Gentoo  Biscoe           46.1          13.2               211        4500\n2 Gentoo  Biscoe           50            16.3               230        5700\n3 Gentoo  Biscoe           48.7          14.1               210        4450\n4 Gentoo  Biscoe           50            15.2               218        5700\n5 Gentoo  Biscoe           47.6          14.5               215        5400\n6 Gentoo  Biscoe           46.5          13.5               210        4550\n# ‚Ñπ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nOur goal is to build a model that we can use to get good predictions of penguins‚Äô flipper (‚Äúarm‚Äù) lengths.\nConsider 2 simple linear regression models of flipper_length_mm by penguin sex and species:\n\nsummary(lm(flipper_length_mm ~ sex, penguins))$r.squared\n\n[1] 0.1204716\n\nsummary(lm(flipper_length_mm ~ species, penguins))$r.squared\n\n[1] 0.7013577\n\n\nHow might we improve our predictions of flipper_length_mm using only these 2 predictors?\nWhat do you think our new R-squared would be?\nEXAMPLE 2\nConsider a simple linear regression model of flipper_length_mm by bill_length_mm:\n\npenguins %&gt;% \n    ggplot(aes(y = flipper_length_mm, x = bill_length_mm)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThoughts? What‚Äôs going on here?\nHow does this highlight the limitations of a simple linear regression model?\nEXAMPLE 3\nThe cps dataset contains employment information collected by the U.S. Current Population Survey (CPS) in 2018.\nWe can use these data to explore wages among 18-34 year olds.\nThe original codebook is here.\n\n# Import data\ncps &lt;- read_csv(\"https://mac-stat.github.io/data/cps_2018.csv\") %&gt;% \n    select(-education, -hours) %&gt;% \n    filter(age &gt;= 18, age &lt;= 34) %&gt;% \n    filter(wage &lt; 250000)\n\nRows: 10000 Columns: 8\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (4): marital, industry, health, education_level\ndbl (4): wage, age, education, hours\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# Check it out\nhead(cps)\n\n# A tibble: 6 √ó 6\n   wage   age marital industry   health    education_level\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;          \n1 75000    33 single  management fair      bachelors      \n2 33000    19 single  management very_good bachelors      \n3 43000    33 married management good      bachelors      \n4 50000    32 single  management excellent HS             \n5 14400    28 single  service    excellent HS             \n6 33000    31 married management very_good bachelors      \n\n\nWe can use a simple linear regression model to summarize the relationship of wage with marital status:\n\n# Build the model\nwage_mod &lt;- lm(wage ~ marital, data = cps)\n\n# Summarize the model\ncoef(summary(wage_mod))\n\n               Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)    46145.23    921.062  50.10002 0.000000e+00\nmaritalsingle -17052.37   1127.177 -15.12839 5.636068e-50\n\n\nWhat do you / don‚Äôt you conclude from this model?\nHow does it highlight the limitations of a simple linear regression model?\nReflection: Why are multiple regression models so useful?\nWe can put more than 1 predictor into a regression model! Adding predictors to models‚Ä¶\n\nPredictive viewpoint: Helps us better predict the response\nDescriptive viewpoint: Helps us better understand the isolated (causal) effect of a variable by holding constant confounders\n\nMultiple linear regression model formula\nIn general, a multiple linear regression model of \\(y\\) with multiple predictors \\((x_1, x_2, ..., x_p)\\) is represented by the following formula:\n\\[E[y \\mid x_1, x_2, ..., x_p] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p\\]",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#learning-goals",
    "href": "activities/08_mlr_intro.html#learning-goals",
    "title": "Introduction to multiple regression",
    "section": "",
    "text": "By the end of this lesson, you should be familiar with:\n\nsome limitations of simple linear regression\nthe general goals behind multiple linear regression\nstrategies for visualizing and interpreting multiple linear regression models of \\(y\\) vs 2 predictors, 1 quantitative and 1 categorical",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#readings-and-videos",
    "href": "activities/08_mlr_intro.html#readings-and-videos",
    "title": "Introduction to multiple regression",
    "section": "",
    "text": "Today is a day to discover ideas, so no readings or videos to go through before class.",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#motivation",
    "href": "activities/08_mlr_intro.html#motivation",
    "title": "Introduction to multiple regression",
    "section": "",
    "text": "EXAMPLE 1\nLet‚Äôs explore some data on penguins.\nFirst, enter install.packages(\"palmerpenguins\") in the console (not Rmd).\nThen load the penguins data.\nYou can find a codebook for these data by typing ?penguins in your console (not Rmd).\n\n# Load packages\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load data\nlibrary(palmerpenguins)\ndata(penguins)\npenguins &lt;- penguins %&gt;% \n    filter(species != \"Adelie\", bill_length_mm &lt; 57)\n\n# Check it out\nhead(penguins)\n\n# A tibble: 6 √ó 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Gentoo  Biscoe           46.1          13.2               211        4500\n2 Gentoo  Biscoe           50            16.3               230        5700\n3 Gentoo  Biscoe           48.7          14.1               210        4450\n4 Gentoo  Biscoe           50            15.2               218        5700\n5 Gentoo  Biscoe           47.6          14.5               215        5400\n6 Gentoo  Biscoe           46.5          13.5               210        4550\n# ‚Ñπ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nOur goal is to build a model that we can use to get good predictions of penguins‚Äô flipper (‚Äúarm‚Äù) lengths.\nConsider 2 simple linear regression models of flipper_length_mm by penguin sex and species:\n\nsummary(lm(flipper_length_mm ~ sex, penguins))$r.squared\n\n[1] 0.1204716\n\nsummary(lm(flipper_length_mm ~ species, penguins))$r.squared\n\n[1] 0.7013577\n\n\nHow might we improve our predictions of flipper_length_mm using only these 2 predictors?\nWhat do you think our new R-squared would be?\nEXAMPLE 2\nConsider a simple linear regression model of flipper_length_mm by bill_length_mm:\n\npenguins %&gt;% \n    ggplot(aes(y = flipper_length_mm, x = bill_length_mm)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThoughts? What‚Äôs going on here?\nHow does this highlight the limitations of a simple linear regression model?\nEXAMPLE 3\nThe cps dataset contains employment information collected by the U.S. Current Population Survey (CPS) in 2018.\nWe can use these data to explore wages among 18-34 year olds.\nThe original codebook is here.\n\n# Import data\ncps &lt;- read_csv(\"https://mac-stat.github.io/data/cps_2018.csv\") %&gt;% \n    select(-education, -hours) %&gt;% \n    filter(age &gt;= 18, age &lt;= 34) %&gt;% \n    filter(wage &lt; 250000)\n\nRows: 10000 Columns: 8\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (4): marital, industry, health, education_level\ndbl (4): wage, age, education, hours\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# Check it out\nhead(cps)\n\n# A tibble: 6 √ó 6\n   wage   age marital industry   health    education_level\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;          \n1 75000    33 single  management fair      bachelors      \n2 33000    19 single  management very_good bachelors      \n3 43000    33 married management good      bachelors      \n4 50000    32 single  management excellent HS             \n5 14400    28 single  service    excellent HS             \n6 33000    31 married management very_good bachelors      \n\n\nWe can use a simple linear regression model to summarize the relationship of wage with marital status:\n\n# Build the model\nwage_mod &lt;- lm(wage ~ marital, data = cps)\n\n# Summarize the model\ncoef(summary(wage_mod))\n\n               Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)    46145.23    921.062  50.10002 0.000000e+00\nmaritalsingle -17052.37   1127.177 -15.12839 5.636068e-50\n\n\nWhat do you / don‚Äôt you conclude from this model?\nHow does it highlight the limitations of a simple linear regression model?\nReflection: Why are multiple regression models so useful?\nWe can put more than 1 predictor into a regression model! Adding predictors to models‚Ä¶\n\nPredictive viewpoint: Helps us better predict the response\nDescriptive viewpoint: Helps us better understand the isolated (causal) effect of a variable by holding constant confounders\n\nMultiple linear regression model formula\nIn general, a multiple linear regression model of \\(y\\) with multiple predictors \\((x_1, x_2, ..., x_p)\\) is represented by the following formula:\n\\[E[y \\mid x_1, x_2, ..., x_p] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p\\]",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#exercise-1-visualizing-the-relationship",
    "href": "activities/08_mlr_intro.html#exercise-1-visualizing-the-relationship",
    "title": "Introduction to multiple regression",
    "section": "Exercise 1: Visualizing the relationship",
    "text": "Exercise 1: Visualizing the relationship\nWe‚Äôve learned how to visualize the relationship of flipper_length_mm by bill_length_mm alone:\n\npenguins %&gt;% \n    ggplot(aes(y = flipper_length_mm, x = bill_length_mm)) + \n    geom_point()\n\n\nTHINK: How might we change the scatterplot points to also indicate information about penguin species? (There‚Äôs more than 1 approach!)\nTry out your idea by modifying the code below. If you get stuck, talk with the tables around you!\n\n\npenguins %&gt;%\n    ggplot(aes(y = flipper_length_mm, x = bill_length_mm, ___ = ___)) +\n    geom_point()",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#exercise-2-visualizing-the-model",
    "href": "activities/08_mlr_intro.html#exercise-2-visualizing-the-model",
    "title": "Introduction to multiple regression",
    "section": "Exercise 2: Visualizing the model",
    "text": "Exercise 2: Visualizing the model\nWe‚Äôve also learned that a simple linear regression model of flipper_length_mm by bill_length_mm alone can be represented by a line:\n\npenguins %&gt;% \n    ggplot(aes(y = flipper_length_mm, x = bill_length_mm)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", se = FALSE)\n\n\nTHINK: Reflecting on your plot of flipper_length_mm by bill_length_mm and species in Exercise 1, how do you think a multiple regression model of flipper_length_mm using both of these predictors would be represented?\nCheck your intuition below by modifying the code below to include species in this plot, as you did in Exercise 1.\n\n\npenguins %&gt;%\n    ggplot(aes(y = flipper_length_mm, x = bill_length_mm, ___ = ___)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#exercise-3-intuition",
    "href": "activities/08_mlr_intro.html#exercise-3-intuition",
    "title": "Introduction to multiple regression",
    "section": "Exercise 3: Intuition",
    "text": "Exercise 3: Intuition\nYour plot in Exercise 2 demonstrated that the multiple linear regression model of flipper_length_mm by bill_length_mm and species is represented by 2 lines.\nLet‚Äôs interpret those lines!\nFor each question, provide an answer along with evidence from the model lines that supports your answer.\n\nWhat‚Äôs the relationship between flipper_length_mm and species, no matter a penguin‚Äôs bill_length_mm?\nWhat‚Äôs the relationship between flipper_length_mm and bill_length_mm, no matter a penguin‚Äôs species?\nDoes the rate of increase in flipper_length_mm with bill_length_mm differ between the two species?",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#exercise-4-model-formula",
    "href": "activities/08_mlr_intro.html#exercise-4-model-formula",
    "title": "Introduction to multiple regression",
    "section": "Exercise 4: Model formula",
    "text": "Exercise 4: Model formula\nOf course, there‚Äôs a formula behind the multiple regression model.\nWe can obtain this using the usual lm() function.\n\n# Build the model\npenguin_mod &lt;- lm(flipper_length_mm ~ bill_length_mm + species, data = penguins)\n\n# Summarize the model\ncoef(summary(penguin_mod))\n\n\nIn the lm() function, how did we communicate that we wanted to model flipper_length_mm by both bill_length_mm and species?\nComplete the following model formula:\nE[flipper_length_mm | bill_length_mm, speciesGentoo] = ___ + ___ * bill_length_mm + ___ * speciesGentoo",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#exercise-5-sub-model-formulas",
    "href": "activities/08_mlr_intro.html#exercise-5-sub-model-formulas",
    "title": "Introduction to multiple regression",
    "section": "Exercise 5: Sub-model formulas",
    "text": "Exercise 5: Sub-model formulas\nOk. We now have a single formula for the model.\nAnd we observed earlier that this formula is represented by two lines: one describing the relationship between flipper_length_mm and bill_length_mm for Chinstrap penguins and the other for Gentoo penguins.\nLet‚Äôs bring these ideas together.\nUtilize the model formula to obtain the equations of these two lines, i.e.¬†to obtain the sub-model formulas for the 2 species. Hint: Plug speciesGentoo = 0 and speciesGentoo = 1.\nChinstrap: flipper_length_mm = ___ + ___ bill_length_mm\nGentoo: flipper_length_mm = ___ + ___ bill_length_mm",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#exercise-6-coefficients-physical-interpretation",
    "href": "activities/08_mlr_intro.html#exercise-6-coefficients-physical-interpretation",
    "title": "Introduction to multiple regression",
    "section": "Exercise 6: coefficients ‚Äì physical interpretation",
    "text": "Exercise 6: coefficients ‚Äì physical interpretation\nReflecting on Exercise 5, let‚Äôs interpret what the model coefficients tell us about the physical properties of the two 2 sub-model lines. Choose the correct option given in parentheses:\n\nThe intercept coefficient, 127.75, is the intercept of the line for (Chinstrap / Gentoo) penguins.\nThe bill_length_mm coefficient, 1.40, is the (intercept / slope) of both lines.\nThe speciesGentoo coefficient, 22.85, indicates that the (intercept / slope) of the line for Gentoo is 22.85mm higher than the (intercept / slope) of the line for Chinstrap. Similarly, since the lines are parallel, the line for Gentoo is 22.85mm higher than the line for Chinstrap at any bill_length_mm.",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#exercise-7-coefficients-contextual-interpretation",
    "href": "activities/08_mlr_intro.html#exercise-7-coefficients-contextual-interpretation",
    "title": "Introduction to multiple regression",
    "section": "Exercise 7: coefficients ‚Äì contextual interpretation",
    "text": "Exercise 7: coefficients ‚Äì contextual interpretation\nNext, interpret each coefficient in a contextually meaningful way. What do they tell us about penguin flipper lengths?!\n\nInterpret 127.75 (intercept of the Chinstrap line).\nInterpret 1.40 (slope of both lines). For both Chinstrap and Gentoo penguins, we expect‚Ä¶\nInterpret 22.85. At any bill_length_mm, we expect‚Ä¶",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#exercise-8-prediction",
    "href": "activities/08_mlr_intro.html#exercise-8-prediction",
    "title": "Introduction to multiple regression",
    "section": "Exercise 8: Prediction",
    "text": "Exercise 8: Prediction\nNow that we better understand the model, let‚Äôs use it to predict flipper lengths!\nRecall the model summary and visualization:\n\ncoef(summary(penguin_mod))\n\npenguins %&gt;% \n    ggplot(aes(y = flipper_length_mm, x = bill_length_mm, color = species)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", se = FALSE)\n\n\nPredict the flipper length of a Chinstrap penguin with a 50mm long bill. Make sure your calculation is consistent with the plot.\n\n\n127.75 + 1.40*___ + 22.85*___\n\n\nPredict the flipper length of a Gentoo penguin with a 50mm long bill. Make sure your calculation is consistent with the plot.\n\n\n127.75 + 1.40*___ + 22.85*___\n\n\nUse the predict() function to confirm your predictions in parts a and b.\n\n\n# Confirm the calculation in part a\npredict(penguin_mod,\n        newdata = data.frame(bill_length_mm = ___, species = \"___\"))\n\n# Confirm the calculation in part b\npredict(penguin_mod,\n        newdata = data.frame(bill_length_mm = ___, species = \"___\"))",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#exercise-9-r-squared",
    "href": "activities/08_mlr_intro.html#exercise-9-r-squared",
    "title": "Introduction to multiple regression",
    "section": "Exercise 9: R-squared",
    "text": "Exercise 9: R-squared\nFinally, recall that improving our predictions was one motivation for multiple linear regression (using 2 predictors instead of 1).\nTo this end, consider the R-squared values of the simple linear regression models that use just one predictor at a time:\n\nsummary(lm(flipper_length_mm ~ bill_length_mm, data = penguins))$r.squared\n\nsummary(lm(flipper_length_mm ~ species, data = penguins))$r.squared\n\n\nIf you had to use only 1 of our 2 predictors, which would give the better predictions of flipper_length_mm?\nWhat do you guess is the R-squared of our multiple regression model that uses both of these predictors? Why?\nCheck your intuition. How does the R-squared of our multiple regression model compare to that of the 2 separate simple linear regression models?\n\n\nsummary(penguin_mod)$r.squared",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#reflection",
    "href": "activities/08_mlr_intro.html#reflection",
    "title": "Introduction to multiple regression",
    "section": "Reflection",
    "text": "Reflection\nYou‚Äôve now explored your first multiple regression model! Thus you likely have a lot of questions about what‚Äôs to come. What are they?\n\nResponse: Put your response here.",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#exercise-1-visualizing-the-relationship-1",
    "href": "activities/08_mlr_intro.html#exercise-1-visualizing-the-relationship-1",
    "title": "Introduction to multiple regression",
    "section": "Exercise 1: Visualizing the relationship",
    "text": "Exercise 1: Visualizing the relationship\n\nno wrong answer\nThere are multiple options!\n\n\npenguins %&gt;% \n    ggplot(aes(y = flipper_length_mm, x = bill_length_mm, color = species)) + \n    geom_point()\n\n\n\n\n\n\n\npenguins %&gt;% \n    ggplot(aes(y = flipper_length_mm, x = bill_length_mm, shape = species)) + \n    geom_point()",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#exercise-2-visualizing-the-model-1",
    "href": "activities/08_mlr_intro.html#exercise-2-visualizing-the-model-1",
    "title": "Introduction to multiple regression",
    "section": "Exercise 2: Visualizing the model",
    "text": "Exercise 2: Visualizing the model\n\nno wrong answer\n\n\n\npenguins %&gt;% \n    ggplot(aes(y = flipper_length_mm, x = bill_length_mm, color = species)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#exercise-3-intuition-1",
    "href": "activities/08_mlr_intro.html#exercise-3-intuition-1",
    "title": "Introduction to multiple regression",
    "section": "Exercise 3: Intuition",
    "text": "Exercise 3: Intuition\n\nGentoo tend to have longer flippers.\nFlipper length is positively associated with bill length.\nNo.¬†the lines are parallel / have the same slopes.",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#exercise-4-model-formula-1",
    "href": "activities/08_mlr_intro.html#exercise-4-model-formula-1",
    "title": "Introduction to multiple regression",
    "section": "Exercise 4: Model formula",
    "text": "Exercise 4: Model formula\n\n# Build the model\npenguin_mod &lt;- lm(flipper_length_mm ~ bill_length_mm + species, data = penguins)\n\n# Summarize the model\ncoef(summary(penguin_mod))\n\n                 Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept)    127.753693  6.1174521 20.88348 1.194472e-50\nbill_length_mm   1.402367  0.1249665 11.22194 1.194932e-22\nspeciesGentoo   22.848036  0.7938292 28.78206 1.981732e-70\n\n\n\nbill_length_mm + species\nE[flipper_length_mm | bill_length_mm, speciesGentoo] = 127.75 + 1.40 * bill_length_mm + 22.85 * speciesGentoo",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#exercise-5-sub-model-formulas-1",
    "href": "activities/08_mlr_intro.html#exercise-5-sub-model-formulas-1",
    "title": "Introduction to multiple regression",
    "section": "Exercise 5: Sub-model formulas",
    "text": "Exercise 5: Sub-model formulas\nChinstrap: flipper_length_mm = 127.75 + 1.40 bill_length_mm\nGentoo: flipper_length_mm = (127.75 + 22.85) + 1.40 bill_length_mm = 150.6 + 1.40 bill_length_mm",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#exercise-6-coefficients-physical-interpretation-1",
    "href": "activities/08_mlr_intro.html#exercise-6-coefficients-physical-interpretation-1",
    "title": "Introduction to multiple regression",
    "section": "Exercise 6: coefficients ‚Äì physical interpretation",
    "text": "Exercise 6: coefficients ‚Äì physical interpretation\n\nThe intercept coefficient, 127.75, is the intercept of the line for Chinstrap penguins.\nThe bill_length_mm coefficient, 1.40, is the slope of both lines.\nThe speciesGentoo coefficient, 22.85, indicates that the intercept of the line for Gentoo is 22.85mm higher than the intercept of the line for Chinstrap. Similarly, since the lines are parallel, the line for Gentoo is 22.85mm higher than the line for Chinstrap at any bill_length_mm.",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#exercise-7-coefficients-contextual-interpretation-1",
    "href": "activities/08_mlr_intro.html#exercise-7-coefficients-contextual-interpretation-1",
    "title": "Introduction to multiple regression",
    "section": "Exercise 7: coefficients ‚Äì contextual interpretation",
    "text": "Exercise 7: coefficients ‚Äì contextual interpretation\n\nFor Chinstrap penguins with 0mm bills (silly), we expect a flipper length of 127.75mm.\nFor both Chinstrap and Gentoo penguins, flipper lengths increase by 1.40mm on average for every additional mm in bill length.\nAt any bill_length_mm, we expect the a Gentoo penguin to have 22.85mm longer flippers than a Chinstrap, on average.",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#exercise-8-prediction-1",
    "href": "activities/08_mlr_intro.html#exercise-8-prediction-1",
    "title": "Introduction to multiple regression",
    "section": "Exercise 8: Prediction",
    "text": "Exercise 8: Prediction\n\n# a\n127.75 + 1.40*50 + 22.85*0\n\n[1] 197.75\n\n# b\n127.75 + 1.40*50 + 22.85*1\n\n[1] 220.6\n\n# c\npredict(penguin_mod,\n        newdata = data.frame(bill_length_mm = 50, \n                             species = \"Chinstrap\"))\n\n      1 \n197.872 \n\npredict(penguin_mod,\n        newdata = data.frame(bill_length_mm = 50, \n                             species = \"Gentoo\"))\n\n       1 \n220.7201",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/08_mlr_intro.html#exercise-9-r-squared-1",
    "href": "activities/08_mlr_intro.html#exercise-9-r-squared-1",
    "title": "Introduction to multiple regression",
    "section": "Exercise 9: R-squared",
    "text": "Exercise 9: R-squared\n\n# Single predictor: bill length\nsummary(lm(flipper_length_mm ~ bill_length_mm, data = penguins))$r.squared\n\n[1] 0.02881163\n\n# Single predictor: species\nsummary(lm(flipper_length_mm ~ species, data = penguins))$r.squared\n\n[1] 0.7013577\n\n# Both predictors\nsummary(penguin_mod)$r.squared\n\n[1] 0.8219244\n\n\n\nspecies\nno wrong answer\nIt‚Äôs higher than the R-squared when we use either predictor alone! We can interpret the R-squared of the model with both predictors (0.8219244): 82% of the variation in flipper length is explained by our model with bill length and species as predictors.",
    "crumbs": [
      "Introduction to multiple regression"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html",
    "href": "activities/10_mlr_confounding.html",
    "title": "Confounding variables",
    "section": "",
    "text": "Slides with comments on Quiz 1\nYou can download a template file to work with here.\nFile organization: Save this file in the ‚ÄúActivities‚Äù subfolder of your ‚ÄúSTAT155‚Äù folder.\n\n\n\nBy the end of this lesson, you should be familiar with:\n\nconfounding variables\nhow to control for confounding variables in our models\nhow to represent the role of confounding variables using causal diagrams\n\n\n\n\nBefore class you should have read and watched:\n\nSections 3.9.2 in the STAT 155 Notes\nConfounding (and other causal diagrams)\n\nWatch from 0:00 - 6:54",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#learning-goals",
    "href": "activities/10_mlr_confounding.html#learning-goals",
    "title": "Confounding variables",
    "section": "",
    "text": "By the end of this lesson, you should be familiar with:\n\nconfounding variables\nhow to control for confounding variables in our models\nhow to represent the role of confounding variables using causal diagrams",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#readings-and-videos",
    "href": "activities/10_mlr_confounding.html#readings-and-videos",
    "title": "Confounding variables",
    "section": "",
    "text": "Before class you should have read and watched:\n\nSections 3.9.2 in the STAT 155 Notes\nConfounding (and other causal diagrams)\n\nWatch from 0:00 - 6:54",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-1-review",
    "href": "activities/10_mlr_confounding.html#exercise-1-review",
    "title": "Confounding variables",
    "section": "Exercise 1: Review",
    "text": "Exercise 1: Review\nThe peaks data includes information on hiking trails in the 46 ‚Äúhigh peaks‚Äù in the Adirondack mountains of northern New York state:\n\n# Load useful packages and data\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\npeaks &lt;- read_csv(\"https://mac-stat.github.io/data/high_peaks.csv\") %&gt;%\n    mutate(ascent = ascent / 1000)\n\n# Check it out \nhead(peaks)\n\n# A tibble: 6 √ó 7\n  peak           elevation difficulty ascent length  time rating   \n  &lt;chr&gt;              &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    \n1 Mt. Marcy           5344          5   3.17   14.8  10   moderate \n2 Algonquin Peak      5114          5   2.94    9.6   9   moderate \n3 Mt. Haystack        4960          7   3.57   17.8  12   difficult\n4 Mt. Skylight        4926          7   4.26   17.9  15   difficult\n5 Whiteface Mtn.      4867          4   2.54   10.4   8.5 easy     \n6 Dix Mtn.            4857          5   2.8    13.2  10   moderate \n\n\nBelow is a model of the time (in hours) that it takes to complete a hike by the hike‚Äôs length (in miles), vertical ascent(in 1000s of feet), and rating (easy, moderate, or difficult):\n\npeaks_model &lt;- lm(time ~ length + ascent + rating, data = peaks)\ncoef(summary(peaks_model))\n\nInterpret the length and ratingeasy coefficients in the model formula below by using our strategy:\n\nStrategy: When interpreting a coefficient for a variable x, compare two units whose values of x differ by 1 but who are identical for all other variables.\n\nE[time | length, ascent, rating] = 6.511 + 0.459 length + 0.187 ascent - 3.169 ratingeasy - 2.477 ratingmoderate\n\nSynthesis:\n\nInterpreting the coefficient \\(\\beta_Q\\) for a quantitative variable Q:\n\nHolding all other variables constant, each unit increase in Q is associated with \\(\\beta_Q\\) change (note if it‚Äôs an increase or decrease) in Y on average.\n\nInterpreting the coefficient \\(\\beta_C\\) for an indicator variable:\n\nHolding all other variables constant, the average outcome for the group referenced by this indicator (group for whom indicator = 1), is \\(\\beta_C\\) higher/lower than that of the reference group.",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-2-confounders",
    "href": "activities/10_mlr_confounding.html#exercise-2-confounders",
    "title": "Confounding variables",
    "section": "Exercise 2: Confounders",
    "text": "Exercise 2: Confounders\n\nResearch question: Is there a wage gap, hence possibly discrimination, by marital status among 18-34 year olds?\n\nTo explore, we can revisit the cps data with employment information collected by the U.S. Current Population Survey (CPS) in 2018. View the codebook here.\n\n# Import data\ncps &lt;- read_csv(\"https://mac-stat.github.io/data/cps_2018.csv\") %&gt;% \n    filter(age &gt;= 18, age &lt;= 34) %&gt;% \n    filter(wage &lt; 250000)\n\nRows: 10000 Columns: 8\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (4): marital, industry, health, education_level\ndbl (4): wage, age, education, hours\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Check it out\nhead(cps)\n\n# A tibble: 6 √ó 8\n   wage   age education marital industry   health    hours education_level\n  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;          \n1 75000    33        16 single  management fair         40 bachelors      \n2 33000    19        16 single  management very_good    40 bachelors      \n3 43000    33        16 married management good         40 bachelors      \n4 50000    32        12 single  management excellent    40 HS             \n5 14400    28        12 single  service    excellent    40 HS             \n6 33000    31        16 married management very_good    45 bachelors      \n\n\nRecall that a simple linear regression model of wage by marital suggests that single workers make $17,052 less than married workers:\n\nwage_model_1 &lt;- lm(wage ~ marital, data = cps)\ncoef(summary(wage_model_1))\n\nThat‚Äôs a big gap!!\nBUT this model ignores important confounding variables that might help explain this gap.\nA confounding variable is a cause of both the predictor of interest (marital) and of the response variable (wage).\nWe can represent this idea with a causal diagram:\n\n\n\n\n\n\n\n\n\nAnother definition of a confounding variable is one that\n\nis a cause of the outcome (wage)\nis associated with the main variable of interest (marital status)\nNOT caused by the variable of interest\n\nWe can represent this on the causal diagram with a line from the confounder to the variable of interest (instead of an arrow):\n\n\n\n\n\n\n\n\n\nName at least 2 potential confounders.",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-3-controlling-for-confounders",
    "href": "activities/10_mlr_confounding.html#exercise-3-controlling-for-confounders",
    "title": "Confounding variables",
    "section": "Exercise 3: Controlling for confounders",
    "text": "Exercise 3: Controlling for confounders\nWhen exploring the relationship between response a response variable y (wage) and some predictor x (marital), there are often confounding variables for which we want to control or adjust.\n\nSometimes, we can control (adjust) for confounding variables through a carefully designed experiment. For example, in comparing the effectiveness (y) of 2 different cold remedies (x), we might want to control for the age, general health, and severity of symptoms among the participants. How might we do that?\nBUT we‚Äôre often working with observational, not experimental, data. Why? Well, explain what an experiment might look like if we wanted to explore the relationship between wage (y) and marital status (x) while controlling for age.",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-4-age",
    "href": "activities/10_mlr_confounding.html#exercise-4-age",
    "title": "Confounding variables",
    "section": "Exercise 4: Age",
    "text": "Exercise 4: Age\nWe‚Äôre in luck.\nWe can control (adjust) for confounding variables by including them in our model!\nThat‚Äôs one of the superpowers of multiple linear regression.\nLet‚Äôs start simple, by controlling for age in our model of wages by marital status:\n\n# Construct the model\nwage_model_2 &lt;- lm(wage ~ marital + age, cps)\ncoef(summary(wage_model_2))\n\n\nVisualize this model by modifying the code below. Note: The last line makes sure that the geom_smooth matches our model assumptions.\n\n\nggplot(cps, aes(y = ___, x = ___, color = ___)) +\n    geom____(size = 0.1, alpha = 0.5) +\n    geom_line(aes(y = wage_model_2$fitted.values), linewidth = 1.25)\n\n\nSuppose 2 workers are the same age, but one is married and one is single. By how much do we expect the single worker‚Äôs wage to differ from the married worker‚Äôs wage? (How does this compare to the $17,052 marital gap among all workers?)\nHow can we interpret the maritalsingle coefficient?",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-5-more-confounders",
    "href": "activities/10_mlr_confounding.html#exercise-5-more-confounders",
    "title": "Confounding variables",
    "section": "Exercise 5: More confounders",
    "text": "Exercise 5: More confounders\nLet‚Äôs control for even more potential confounders!\nModel wages by marital status while controlling for age and years of education:\n\nwage_model_3 &lt;- lm(wage ~ marital + age + education, cps)\ncoef(summary(wage_model_3))\n\n\nWith so many variables, this is a tough model to visualize. If you had to draw it, how would the model trend appear: 1 point, 2 points, 2 lines, 1 plane, or 2 planes? Explain your rationale. Hint: pay attention to whether your predictors are quantitative or categorical.\nGiven our research question, which coefficient is of primary interest? Interpret this coefficient.\nInterpret the two other coefficients in this model.",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-6-even-more",
    "href": "activities/10_mlr_confounding.html#exercise-6-even-more",
    "title": "Confounding variables",
    "section": "Exercise 6: Even more",
    "text": "Exercise 6: Even more\nLet‚Äôs control for another potential confounder, the job industry in which one works (categorical):\n\nwage_model_4 &lt;- lm(wage ~ marital + age + education + industry, cps)\ncoef(summary(wage_model_4))\n\nIf we had to draw it, this model would appear as 12 planes.\nThe original plane explains the relationship between wage and the 2 quantitative predictors, age and education.\nThen this plane is split into 12 (2*6) individual planes, 1 for each possible combination of marital status (2 possibilities) and industry (6 possibilities).\n\nInterpret the main coefficient of interest for our research question.\nWhen controlling for a worker‚Äôs age, marital status, and education level, which industry tends to have the highest wages? The lowest? Note: the following table shows the 6 industries:\n\n\ncps %&gt;% count(industry)",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-7-biggest-model-yet",
    "href": "activities/10_mlr_confounding.html#exercise-7-biggest-model-yet",
    "title": "Confounding variables",
    "section": "Exercise 7: Biggest model yet",
    "text": "Exercise 7: Biggest model yet\nBuild a model that helps us explore wage by marital status while controlling for: age, education, job industry, typical number of work hours, and health status.\nStore this model as wage_model_5.",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-8-reflection",
    "href": "activities/10_mlr_confounding.html#exercise-8-reflection",
    "title": "Confounding variables",
    "section": "Exercise 8: Reflection",
    "text": "Exercise 8: Reflection\nTake two workers ‚Äì one is married and the other is single.\nThe models above provided the following insights into the typical difference in wages for these two groups:\n\n\n\nModel\nAssume the two people have the same‚Ä¶\nWage difference\n\n\n\n\nwage_model_1\nNA\n-$17,052\n\n\nwage_model_2\nage\n-$7,500\n\n\nwage_model_3\nage, education\n-$6,478\n\n\nwage_model_4\nage, education, industry\n-$5,893\n\n\nwage_model_5\nage, education, industry, hours, health\n-$4,993\n\n\n\n\nThough not the case in every analysis, the marital coefficient got closer and closer to 0 as we controlled for more confounders. Explain the significance of this phenomenon, in context - what does it mean?\nDo you still find the wage gap for single vs married people to be meaningfully ‚Äúlarge‚Äù? Can you think of any remaining factors that might explain part of this remaining gap? Or do you think we‚Äôve found evidence of inequitable wage practices for single vs married workers?",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-9-a-new-extreme-example",
    "href": "activities/10_mlr_confounding.html#exercise-9-a-new-extreme-example",
    "title": "Confounding variables",
    "section": "Exercise 9: A new (extreme) example",
    "text": "Exercise 9: A new (extreme) example\nFor a more extreme example of why it‚Äôs important to control for confounding variables, let‚Äôs return to the diamonds data:\n\n# Import and wrangle the data\ndata(diamonds)\ndiamonds &lt;- diamonds %&gt;% \n    mutate(\n        cut = factor(cut, ordered = FALSE),\n        color = factor(color, ordered = FALSE),\n        clarity = factor(clarity, ordered = FALSE)\n    ) %&gt;% \n    select(price, clarity, cut, color, carat)\n\nOur goal is to explore how the price of a diamond depends upon its clarity (a measure of quality).\nClarity is classified as follows, in order from best to worst:\n\n\n\nclarity\ndescription\n\n\n\n\nIF\nflawless (no internal imperfections)\n\n\nVVS1\nvery very slightly imperfect\n\n\nVVS2\n‚Äù ‚Äù\n\n\nVS1\nvery slightly imperfect\n\n\nVS2\n‚Äù ‚Äù\n\n\nSI1\nslightly imperfect\n\n\nSI2\n‚Äù ‚Äù\n\n\nI1\nimperfect\n\n\n\n\nCheck out a model of price by clarity. What clarity has the highest average price? The lowest? (This is surprising!)\n\n\ndiamond_model_1 &lt;- lm(price ~ clarity, data = diamonds)\n\n# Get a model summary\ncoef(summary(diamond_model_1))\n\n\nWhat confounding variable might explain these results? What‚Äôs your rationale?",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-10-size",
    "href": "activities/10_mlr_confounding.html#exercise-10-size",
    "title": "Confounding variables",
    "section": "Exercise 10: Size",
    "text": "Exercise 10: Size\nIt turns out that carat, the size of a diamond, is an important confounding variable.\nLet‚Äôs explore what happens when we control for this in our model:\n\ndiamond_model_2 &lt;- lm(price ~ clarity + carat, data = diamonds)\n\n# Get a model summary\ncoef(summary(diamond_model_2))\n\n# Plot the model\ndiamonds %&gt;% \n    ggplot(aes(y = price, x = carat, color = clarity)) + \n    geom_line(aes(y = diamond_model_2$fitted.values))\n\nWhat do you think now?\nWhich clarity has the highest expected price?\nThe lowest?\nProvide numerical evidence from the model.",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-11-simpsons-paradox",
    "href": "activities/10_mlr_confounding.html#exercise-11-simpsons-paradox",
    "title": "Confounding variables",
    "section": "Exercise 11: Simpson‚Äôs Paradox",
    "text": "Exercise 11: Simpson‚Äôs Paradox\nControlling for carat didn‚Äôt just change the clarity coefficients, hence our understanding of the relationship between price and clarity‚Ä¶ It flipped the signs of many of these coefficients.\nThis extreme scenario has a name: Simpson‚Äôs paradox.\nCHALLENGE: Explain why this happened and support your argument with graphical evidence.\nHINTS: Think about the causal diagram below. How do you think carat influences clarity? How do you think carat influences price? Make 2 ggplot() that support your answers.",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-12-final-conclusion",
    "href": "activities/10_mlr_confounding.html#exercise-12-final-conclusion",
    "title": "Confounding variables",
    "section": "Exercise 12: Final conclusion",
    "text": "Exercise 12: Final conclusion\nWhat‚Äôs your final conclusion about diamond prices?\nWhich diamonds are more expensive: flawed ones or flawless ones?",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#reflection",
    "href": "activities/10_mlr_confounding.html#reflection",
    "title": "Confounding variables",
    "section": "Reflection",
    "text": "Reflection\nWrite a one-sentence warning label for what might happen if we do not control for confounding variables in our model.\n\nResponse: Put your response here.",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-1-review-1",
    "href": "activities/10_mlr_confounding.html#exercise-1-review-1",
    "title": "Confounding variables",
    "section": "Exercise 1: Review",
    "text": "Exercise 1: Review\n\npeaks_model &lt;- lm(time ~ length + ascent + rating, data = peaks)\ncoef(summary(peaks_model))\n\n                 Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)     6.5106514 1.62983740  3.994663 2.627176e-04\nlength          0.4590819 0.08158314  5.627166 1.465288e-06\nascent          0.1874830 0.34215350  0.547950 5.866973e-01\nratingeasy     -3.1685224 0.86219113 -3.674965 6.827232e-04\nratingmoderate -2.4767827 0.61058560 -4.056405 2.177589e-04\n\n\n\nlength coefficient:\n\nAmong hikes with the same vertical ascent and challenge rating, each additional mile of the hike is associated with a 0.46 hour increase in completion time on average.\nHolding vertical ascent and challenge rating constant (fixed), each additional mile of the hike is associated with a 0.46 hour increase in completion time on average.\n\nratingeasy coefficient:\n\nAmong hikes with the same length and vertical ascent, the average completion time of easy hikes is 3.2 hours less than that of difficult hikes (reference category).\nHolding constant hike length and vertical ascent, the average completion time of easy hikes is 3.2 hours less than that of difficult hikes.",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-2-confounders-1",
    "href": "activities/10_mlr_confounding.html#exercise-2-confounders-1",
    "title": "Confounding variables",
    "section": "Exercise 2: Confounders",
    "text": "Exercise 2: Confounders\nage, education, job industry, ‚Ä¶",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-3-controlling-for-confounders-1",
    "href": "activities/10_mlr_confounding.html#exercise-3-controlling-for-confounders-1",
    "title": "Confounding variables",
    "section": "Exercise 3: Controlling for confounders",
    "text": "Exercise 3: Controlling for confounders\n\ncreate 2 separate groups that are as similar as possible with respect to these variables. give the groups different remedies.\nwe‚Äôd have to get 2 groups that are similar with respect to age, and assign 1 group to get married and 1 group to be single. that would be weird (and unethical).",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-4-age-1",
    "href": "activities/10_mlr_confounding.html#exercise-4-age-1",
    "title": "Confounding variables",
    "section": "Exercise 4: Age",
    "text": "Exercise 4: Age\n\n# Construct the model\nwage_model_2 &lt;- lm(wage ~ marital + age, cps)\ncoef(summary(wage_model_2))\n\n                Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)   -19595.948  3691.6998 -5.308110 1.184066e-07\nmaritalsingle  -7500.146  1191.8526 -6.292847 3.545964e-10\nage             2213.869   120.7701 18.331265 2.035782e-71\n\n\n\n.\n\n\ncps %&gt;%\nggplot(aes(y = wage, x = age, color = marital)) +\n    geom_point(size = 0.1, alpha = 0.5) +\n    geom_line(aes(y = wage_model_2$fitted.values), size = 1.25)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n-$7500\n\nWhen controlling for (‚Äúholding constant‚Äù) age, single workers make $7500 less than married workers on average.\nAmong workers of the same age, single workers make $7500 less than married workers on average.",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-5-more-confounders-1",
    "href": "activities/10_mlr_confounding.html#exercise-5-more-confounders-1",
    "title": "Confounding variables",
    "section": "Exercise 5: More confounders",
    "text": "Exercise 5: More confounders\n\nwage_model_3 &lt;- lm(wage ~ marital + age + education, cps)\ncoef(summary(wage_model_3))\n\n                Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)   -64898.607  4099.8737 -15.829416 2.254709e-54\nmaritalsingle  -6478.094  1119.9345  -5.784351 7.988760e-09\nage             1676.796   116.3086  14.416777 1.102113e-45\neducation       4285.259   207.2158  20.680173 3.209448e-89\n\n\n\n2 planes: There are 2 quantitative predictors which form the dimensions of the plane. The marital status categorical predictor creates 2 planes.\nThe maritalsingle coefficient is of main interest:\n\nAmong workers of the same age and years of education, single workers earn $6478 less than married workers.\n\n\nage coefficient: Among workers of the same marital status and years of education, each additional year of age is associated with a $1677 increase in salary on average.\neducation coefficient: Among workers of the same marital status and age, each additional year of education is associated with a $4285 increase in salary on average.",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-6-even-more-1",
    "href": "activities/10_mlr_confounding.html#exercise-6-even-more-1",
    "title": "Confounding variables",
    "section": "Exercise 6: Even more",
    "text": "Exercise 6: Even more\n\nwage_model_4 &lt;- lm(wage ~ marital + age + education + industry, cps)\ncoef(summary(wage_model_4))\n\n                                  Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)                     -52498.857  7143.8481 -7.3488206 2.533275e-13\nmaritalsingle                    -5892.842  1105.6898 -5.3295615 1.053631e-07\nage                               1493.360   116.1673 12.8552586 6.651441e-37\neducation                         3911.117   243.0192 16.0938565 4.500408e-56\nindustryconstruction              5659.082  6218.5649  0.9100302 3.628760e-01\nindustryinstallation_production   1865.650  6109.2613  0.3053806 7.600964e-01\nindustrymanagement                1476.884  6031.2901  0.2448704 8.065727e-01\nindustryservice                  -7930.403  5945.6509 -1.3338158 1.823603e-01\nindustrytransportation           -1084.176  6197.2462 -0.1749448 8.611342e-01\n\n\n\nAmong workers of the same job industry, education, and age, single workers make $5893 less than a married worker on average.\nhighest = construction (because it has the highest positive coefficient), lowest = service (because it has the most negative coefficient)",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-7-biggest-model-yet-1",
    "href": "activities/10_mlr_confounding.html#exercise-7-biggest-model-yet-1",
    "title": "Confounding variables",
    "section": "Exercise 7: Biggest model yet",
    "text": "Exercise 7: Biggest model yet\n\nwage_model_5 &lt;- lm(wage ~ marital + age + education + industry + hours + health, cps)\ncoef(summary(wage_model_5))\n\n                                   Estimate Std. Error     t value     Pr(&gt;|t|)\n(Intercept)                     -64886.5747 6914.18198 -9.38456275 1.171028e-20\nmaritalsingle                    -4992.7685 1061.84882 -4.70195794 2.687274e-06\nage                               1061.1410  115.83503  9.16079518 9.031462e-20\neducation                         3443.7625  236.12723 14.58435151 1.128646e-46\nindustryconstruction              5381.3857 5959.05620  0.90306007 3.665630e-01\nindustryinstallation_production   2951.0372 5854.23981  0.50408547 6.142365e-01\nindustrymanagement                5107.6364 5782.95334  0.88322283 3.771832e-01\nindustryservice                  -3074.5127 5705.56537 -0.53886207 5.900201e-01\nindustrytransportation            -207.3439 5940.02074 -0.03490626 9.721567e-01\nhours                              732.1340   43.72488 16.74410733 2.340115e-60\nhealthfair                       -7407.7981 2901.71339 -2.55290483 1.072955e-02\nhealthgood                       -2470.8096 1259.44276 -1.96182766 4.987035e-02\nhealthpoor                       -9086.9110 7657.43781 -1.18667774 2.354441e-01\nhealthvery_good                    292.5278 1020.89213  0.28654136 7.744823e-01",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-8-reflection-1",
    "href": "activities/10_mlr_confounding.html#exercise-8-reflection-1",
    "title": "Confounding variables",
    "section": "Exercise 8: Reflection",
    "text": "Exercise 8: Reflection\n\nThese confounders explained away more and more of the wage gap between single and married workers.\nAnswers will vary. A potential factor that we haven‚Äôt considered is a worker‚Äôs role within a given industry.",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-9-a-new-extreme-example-1",
    "href": "activities/10_mlr_confounding.html#exercise-9-a-new-extreme-example-1",
    "title": "Confounding variables",
    "section": "Exercise 9: A new (extreme) example",
    "text": "Exercise 9: A new (extreme) example\n\n\n\nclarity\ndescription\n\n\n\n\nIF\nflawless (no internal imperfections)\n\n\nVVS1\nvery very slightly imperfect\n\n\nVVS2\n‚Äù ‚Äù\n\n\nVS1\nvery slightly imperfect\n\n\nVS2\n‚Äù ‚Äù\n\n\nSI1\nslightly imperfect\n\n\nSI2\n‚Äù ‚Äù\n\n\nI1\nimperfect\n\n\n\n\ndiamond_model_1 &lt;- lm(price ~ clarity, data = diamonds)\n\n# Get a model summary\ncoef(summary(diamond_model_1))\n\n                 Estimate Std. Error      t value      Pr(&gt;|t|)\n(Intercept)  3924.1686910   144.5619 27.145247517 3.513547e-161\nclaritySI2   1138.8599147   150.2746  7.578526239  3.550711e-14\nclaritySI1     71.8324571   148.6049  0.483378837  6.288287e-01\nclarityVS2      0.8207037   148.8672  0.005512992  9.956013e-01\nclarityVS1    -84.7132999   150.9746 -0.561109670  5.747251e-01\nclarityVVS2  -640.4316203   154.7737 -4.137858008  3.510944e-05\nclarityVVS1 -1401.0540535   158.5401 -8.837224284  1.010097e-18\nclarityIF   -1059.3295848   171.8990 -6.162510636  7.210567e-10\n\n\n\nhighest = SI2, lowest = VVS1\nwill vary.",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-10-size-1",
    "href": "activities/10_mlr_confounding.html#exercise-10-size-1",
    "title": "Confounding variables",
    "section": "Exercise 10: Size",
    "text": "Exercise 10: Size\n\ndiamond_model_2 &lt;- lm(price ~ clarity + carat, data = diamonds)\n\n# Get a model summary\ncoef(summary(diamond_model_2))\n\n             Estimate Std. Error    t value Pr(&gt;|t|)\n(Intercept) -6911.566   50.22456 -137.61327        0\nclaritySI2   2879.180   49.47264   58.19742        0\nclaritySI1   3729.449   49.16156   75.86108        0\nclarityVS2   4388.904   49.38115   88.87813        0\nclarityVS1   4613.765   50.13112   92.03393        0\nclarityVVS2  5163.323   51.62127  100.02318        0\nclarityVVS1  5186.619   53.04830   97.77163        0\nclarityIF    5513.139   57.36530   96.10582        0\ncarat        8440.057   12.65126  667.13154        0\n\n# Plot the model\ndiamonds %&gt;% \n    ggplot(aes(y = price, x = carat, color = clarity)) + \n    geom_line(aes(y = diamond_model_2$fitted.values))\n\n\n\n\n\n\n\n\nhighest = IF, lowest = I1 (reference category)\nThis is what we would have expected!",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-11-simpsons-paradox-1",
    "href": "activities/10_mlr_confounding.html#exercise-11-simpsons-paradox-1",
    "title": "Confounding variables",
    "section": "Exercise 11: Simpson‚Äôs Paradox",
    "text": "Exercise 11: Simpson‚Äôs Paradox\nThe bigger the diamond the bigger the price:\n\ndiamonds %&gt;% \n    ggplot(aes(y = price, x = carat)) + \n    geom_point()\n\n\n\n\n\n\n\n\nBUT the bigger the diamond, the more flawed it tends to be:\n\ndiamonds %&gt;% \n    ggplot(aes(y = carat, x = clarity)) + \n    geom_boxplot()\n\n\n\n\n\n\n\n\nThus flawed diamonds looked more expensive, but only because they also tend to be bigger (and size is a bigger driver of price).",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "activities/10_mlr_confounding.html#exercise-12-final-conclusion-1",
    "href": "activities/10_mlr_confounding.html#exercise-12-final-conclusion-1",
    "title": "Confounding variables",
    "section": "Exercise 12: Final conclusion",
    "text": "Exercise 12: Final conclusion\nFlawless diamonds are more expensive.",
    "crumbs": [
      "Confounding variables"
    ]
  },
  {
    "objectID": "assignments/pp2.html",
    "href": "assignments/pp2.html",
    "title": "Practice Problems 2",
    "section": "",
    "text": "Due Friday, 9/20 at 5pm on Moodle.",
    "crumbs": [
      "Practice Problems 2"
    ]
  },
  {
    "objectID": "assignments/pp2.html#context",
    "href": "assignments/pp2.html#context",
    "title": "Practice Problems 2",
    "section": "Context",
    "text": "Context\nIn the following exercises, we‚Äôll work with data from the World Bank on secondary school enrollment.\nWe have access to the following information:\n\nCountry: country name\nYear: year enrollment was measured (ranges from 2004 - 2019)\nGER: gross enrollment rate in secondary school (%)\nNER: net enrollment rate in secondary school (%)\n\nNet enrollment rate (NER) is the ratio of children who are of secondary school age who are enrolled in secondary school, out of the total number of children of secondary school age. In contrast, gross enrollment rate (GER) is the ratio of children who are enrolled in secondary school regardless of age, out of the total number of children of secondary school age. NER ranges from 0 to 100%, since it is a true proportion, while GER can exceed 100% (some children who are not of secondary school age may in fact be enrolled).\nHistorically, NER is more difficult to measure than GER (particularly in low- and middle-income countries), since it requires knowledge of the age of the children enrolled in school.\nA relevant research question is: If we know GER, can we accurately predict NER? In order to answer this question, we first want to better understand the relationship between GER and NER.\nThese practice problems are inspired by a recent paper that aims to develop statistical methods for analyzing this problem. The methods described in this paper are beyond the scope of this course, but we link it here in case you are interested in learning more!",
    "crumbs": [
      "Practice Problems 2"
    ]
  },
  {
    "objectID": "assignments/pp2.html#exercise-1-visual-and-numerical-summaries",
    "href": "assignments/pp2.html#exercise-1-visual-and-numerical-summaries",
    "title": "Practice Problems 2",
    "section": "Exercise 1: Visual and numerical summaries",
    "text": "Exercise 1: Visual and numerical summaries\n\nPart a\nConstruct an appropriate visualization of the GER and NER variables (NER should be the dependent variable in your visualization). Add both a curved and linear trend line to your plot. Based on this visualization, do you think a linear regression model would correctly explain the relationship between NER and GER? Explain why or why not.\n\n\nPart b\nCalculate and report the correlation between GER and NER. Do you think correlation is an appropriate numerical summary for the relationship between GER and NER? Explain why or why not (you may make reference to your answer to Part a, if relevant).",
    "crumbs": [
      "Practice Problems 2"
    ]
  },
  {
    "objectID": "assignments/pp2.html#exercise-2-fitting-a-simple-linear-regression-model",
    "href": "assignments/pp2.html#exercise-2-fitting-a-simple-linear-regression-model",
    "title": "Practice Problems 2",
    "section": "Exercise 2: Fitting a simple linear regression model",
    "text": "Exercise 2: Fitting a simple linear regression model\nSuppose we decide to go ahead and fit a linear regression model, with GER as our predictor and NER as our outcome. Fill in the model statement below (using proper notation) that corresponds to this regression:\n\\[\nE[___ | ___] = ...\n\\]\nFit the linear regression model that you wrote above.",
    "crumbs": [
      "Practice Problems 2"
    ]
  },
  {
    "objectID": "assignments/pp2.html#exercise-3-model-eval-correct-and-fair",
    "href": "assignments/pp2.html#exercise-3-model-eval-correct-and-fair",
    "title": "Practice Problems 2",
    "section": "Exercise 3: Model Eval: ‚ÄúCorrect‚Äù and ‚ÄúFair‚Äù",
    "text": "Exercise 3: Model Eval: ‚ÄúCorrect‚Äù and ‚ÄúFair‚Äù\n\nPart a\nMake a plot of residuals vs.¬†fitted values for the model you fit in Exercise 2. Does this diagnostic plot suggest that your model is wrong? Explain why or why not.\n\n\nPart b\nDoes the diagnostic plot in part a suggest that your model is fair? If the model seems fair, explain what about the plot suggests that. If the model does not seem fair, note which observations your model does a worse job at predicting NER for (high values of GER? low values of GER?).",
    "crumbs": [
      "Practice Problems 2"
    ]
  },
  {
    "objectID": "assignments/pp2.html#exercise-4-model-eval-strong",
    "href": "assignments/pp2.html#exercise-4-model-eval-strong",
    "title": "Practice Problems 2",
    "section": "Exercise 4: Model Eval: ‚ÄúStrong‚Äù",
    "text": "Exercise 4: Model Eval: ‚ÄúStrong‚Äù\nReport and interpret the multiple \\(R^2\\) value from the linear regression model you fit in Exercise 2. Does this value suggest the model is strong or weak?",
    "crumbs": [
      "Practice Problems 2"
    ]
  },
  {
    "objectID": "assignments/pp2.html#exercise-5-thinking-about-transformations",
    "href": "assignments/pp2.html#exercise-5-thinking-about-transformations",
    "title": "Practice Problems 2",
    "section": "Exercise 5: Thinking about transformations",
    "text": "Exercise 5: Thinking about transformations\n\nPart a\nCurrently in the data, NER is reported on a scale from 0 - 100. How would the slope in your model change (if at all) from Exercise 2 if NER were transformed to be on a scale from 0-1? How would the intercept in your model change (if at all) from Exercise 2 if NER were transformed to be on a scale from 0-1? If it helps you think it through, mutate NER and fit this model!\n\n\nPart b\nSuppose I want the intercept of my model of NER by GER to have the interpretation of the expected average NER for a GER of 80%. Suggest a transformation I could make to one or both of these variables that would give the intercept of my linear regression model this interpretation.",
    "crumbs": [
      "Practice Problems 2"
    ]
  },
  {
    "objectID": "assignments/pp4.html",
    "href": "assignments/pp4.html",
    "title": "Practice Problems 4",
    "section": "",
    "text": "Due Friday, 10/11 at 5pm on Moodle.",
    "crumbs": [
      "Practice Problems 4"
    ]
  },
  {
    "objectID": "assignments/pp4.html#context",
    "href": "assignments/pp4.html#context",
    "title": "Practice Problems 4",
    "section": "Context",
    "text": "Context\n\nSchool Enrollment\nFor Exercises 1-4, we‚Äôll revisit the data from the World Bank on secondary school enrollment that we worked with in Practice Problems #2. As a reminder, the context from that assignment is pasted below.\nWe have access to the following information:\n\nCountry: country name\nYear: year enrollment was measured (ranges from 2004 - 2019)\nGER: gross enrollment rate in secondary school (%)\nNER: net enrollment rate in secondary school (%)\n\nNet enrollment rate (NER) is the ratio of children who are of secondary school age who are enrolled in secondary school, out of the total number of children of secondary school age. In contrast, gross enrollment rate (GER) is the ratio of children who are enrolled in secondary school regardless of age, out of the total number of children of secondary school age. NER ranges from 0 to 100%, since it is a true proportion, while GER can exceed 100% (some children who are not of secondary school age may in fact be enrolled).\nHistorically, NER is more difficult to measure than GER (particularly in low- and middle-income countries), since it requires knowledge of the age of the children enrolled in school.\nA relevant research question is: If we know GER, can we accurately predict NER? In order to answer this question, we first want to better understand the relationship between GER and NER.\n\n\nBikes\nFor Exercises 5-7, we‚Äôll revisit the data on bike ridership. We‚Äôve looked at many of the variables included in this dataset so far in this course, but suppose we are now interested in how season impacts the relationship between actual temperature and number of total riders. This is the question we‚Äôll explore in this assignment.",
    "crumbs": [
      "Practice Problems 4"
    ]
  },
  {
    "objectID": "assignments/pp4.html#exercise-1-define-a-new-variable-and-visualize",
    "href": "assignments/pp4.html#exercise-1-define-a-new-variable-and-visualize",
    "title": "Practice Problems 4",
    "section": "Exercise 1: Define a new variable and visualize",
    "text": "Exercise 1: Define a new variable and visualize\n\nPart a\nJust as in Practice Problems #2, construct an appropriate visualization of the GER and NER variables (NER should be the dependent variable in your visualization). You may copy your code from the previous assignment.\nAt roughly what value of GER does the relationship between GER and NER appear to change?\n\n\nPart b\nDefine a binary (indicator) variable called GER_binary, that has value 0 when GER is less than 100, and 1 when GER is greater than or equal to 100.",
    "crumbs": [
      "Practice Problems 4"
    ]
  },
  {
    "objectID": "assignments/pp4.html#exercise-2-compare-models-with-and-without-interaction",
    "href": "assignments/pp4.html#exercise-2-compare-models-with-and-without-interaction",
    "title": "Practice Problems 4",
    "section": "Exercise 2: Compare models with and without interaction",
    "text": "Exercise 2: Compare models with and without interaction\nIn Practice Problems #2, we noted that a linear regression model did not accurately reflect the relationship between GER and NER. We‚Äôll now compare that same simple linear regression model to a multiple linear regression model with an interaction term between GER and GER_binary.\nFit both of these models.",
    "crumbs": [
      "Practice Problems 4"
    ]
  },
  {
    "objectID": "assignments/pp4.html#exercise-3-assess",
    "href": "assignments/pp4.html#exercise-3-assess",
    "title": "Practice Problems 4",
    "section": "Exercise 3: Assess",
    "text": "Exercise 3: Assess\n\nPart a\nFor each of the two models you fit in Exercise 2, plot residuals vs.¬†fitted values. Does the model with the interaction term appear to better predict NER than the simple linear regression model? Explain why or why not.\n\n\nPart b\nCompare the multiple \\(R^2\\) values from each model. Based on these values, which model is stronger?",
    "crumbs": [
      "Practice Problems 4"
    ]
  },
  {
    "objectID": "assignments/pp4.html#exercise-4-visualize-interaction-term---school-enrollment",
    "href": "assignments/pp4.html#exercise-4-visualize-interaction-term---school-enrollment",
    "title": "Practice Problems 4",
    "section": "Exercise 4: Visualize Interaction Term - School Enrollment",
    "text": "Exercise 4: Visualize Interaction Term - School Enrollment\nIt may seem strange to include an interaction term between GER and another variable that was created from GER itself, but this is actually a special type of interaction term that creates what is called a piecewise linear regression model. It allows both the slope and intercept of the regression line to change at specific values of a predictor. Fill in the code below to visualize this model.\n\n# Generate predictions from model with interaction term\nnewdat &lt;- data.frame(GER = seq(from = min(enroll$GER), \n                               to = max(enroll$GER), by = 0.01)) %&gt;%\n  mutate(GER_binary = ifelse(GER &lt; 100, 0, 1))\n\n# Fill in your interaction model object below for ___\nnewdat$NER &lt;- predict(___, newdata = newdat)\n\nggplot() +\n  geom_point(data = enroll, aes(GER, NER)) +\n  geom_line(data = newdat, aes(GER, NER), col = \"red\", size = 1)",
    "crumbs": [
      "Practice Problems 4"
    ]
  },
  {
    "objectID": "assignments/pp4.html#exercise-6-visualize-interaction-term---bikes",
    "href": "assignments/pp4.html#exercise-6-visualize-interaction-term---bikes",
    "title": "Practice Problems 4",
    "section": "Exercise 6: Visualize Interaction Term - Bikes",
    "text": "Exercise 6: Visualize Interaction Term - Bikes\nRecall that we are now interested in how season impacts the relationship between actual temperature and number of total riders. Make an appropriate visualization (consider number of total riders as your outcome variable) to display this impact. Comment on what you notice from the visual.",
    "crumbs": [
      "Practice Problems 4"
    ]
  },
  {
    "objectID": "assignments/pp4.html#exercise-5-model-statement-and-fitting",
    "href": "assignments/pp4.html#exercise-5-model-statement-and-fitting",
    "title": "Practice Problems 4",
    "section": "Exercise 5: Model statement and fitting",
    "text": "Exercise 5: Model statement and fitting\nWrite a model statement in the form \\(E[Y | X, Z] = ...\\) in context, that would allow us to answer the research question we‚Äôre interested in. Additionally, note which regression coefficient will be the relevant one to interpret that will directly address our research question.\nFit this multiple linear regression model.",
    "crumbs": [
      "Practice Problems 4"
    ]
  },
  {
    "objectID": "assignments/pp4.html#exercise-6-interpretation-and-conclusions",
    "href": "assignments/pp4.html#exercise-6-interpretation-and-conclusions",
    "title": "Practice Problems 4",
    "section": "Exercise 6: Interpretation and Conclusions",
    "text": "Exercise 6: Interpretation and Conclusions\nInterpret the regression coefficient you noted would directly address the research question we‚Äôre interested in. Make sure to use appropriate causation vs.¬†association language, include units, and talk about averages rather than individual cases.\nWrite a paragraph conclusion about the results of your analysis, fit for a news article. Does season appear to meaningfully impact the relationship between actual temperature and number of total riders? What impact would this have on cities looking to start bikeshare programs? Should more bikes be provided or expected in certain seasons based on temperature conditions? Explain why or why not.",
    "crumbs": [
      "Practice Problems 4"
    ]
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Project",
    "section": "",
    "text": "A detailed description of the group project can be found here. Important dates and key information are listed below.\n\nImportant dates\n\nWednesday, October 16: Project Checkpoint 1 due (ignore the Friday, October 11 mentioned in the Google Doc above)\nTuesday, November 26: Project Checkpoint 2 due\nWednesday, December 11: Project paper due, presentations in class\n\n\n\nGrading\nThe final paper will be graded using the rubric found here. Although the first two checkpoints will not directly contribute to your course grade, the feedback you receive on them will help you make an excellent final paper.\n\n\nResources\nLater in the semester, I will post at least one example of a successful final paper, as well as a Google Doc template for the final paper.\n\n\nProject Groups\nProject groups will ultimately be determined by me, with the assistance of a Project Group Preferences survey (due Friday 9/20 at 5pm). The survey will allow you to either list people you would like to be in a group with or specify that you would like to be placed in a group randomly by me. I will do my best to ensure that groups are constructed as closely to survey responses as possible. Since one component of the project involves an in-class presentation, you may not work with students in different sections on the project.\nIf for any reason there is an individual (or individuals) with whom you do not feel safe participating in a project group with, there will be a space to note this on the survey. You will not be expected to provide any justification for your response to this question, but note that as I need to place you in groups, the survey will not be anonymous.\nGroup work and collaboration are important components of nearly all statistical analyses and projects. I recognize that individual contributions to a group project are not always consistent between group members. To this end, you will (as a group) be expected to explicitly specify the project tasks completed by each team member along with the final draft of your paper.\nIf there are any concerns with project groups throughout the semester, I am happy to meet with groups or individuals to discuss ways to move forward."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Overview\n\n  \n    Week\n    Monday\n    Wednesday\n    Friday\n    Announcements\n  \n\n  \n    1 (9/2 - 9/6)\n    \n    First day of class\n    \n        Univariate visualization and summaries\n        Checkpoint 1 due 11:30am\n    \n    \n  \n  \n    2 (9/9 - 9/13)\n    Introduction to simple linear regression\n    \n        Simple linear regression: formalizing concepts\n        Checkpoint 2 due 11:30am\n    \n    Time to work on PP1\n    PP1 due Friday 9/13 at 5pm\n  \n  \n    3 (9/16 - 9/20)\n    \n        Model evaluation\n        Checkpoint 3 due 11:30am\n    \n    \n        Variable transformations\n        Checkpoint 4 due 11:30am\n    \n    \n        Data-finding activity with Mac librarians\n        Project Group Preferences survey due 5pm\n    \n    PP2 due Friday 9/20 at 5pm\n  \n  \n    4 (9/23 - 9/27)\n    \n        Categorical predictors\n        Checkpoint 5 due 11:30am\n    \n    Multiple predictors in linear regression\n    Quiz 1 (1 hour in class)\n    No Practice Problems this week, but PP3 due next Friday\n  \n  \n    5 (9/30 - 10/4)\n    \n        Multiple linear regression: principles\n        Checkpoint 6 due 11:30am\n    \n    \n        Multiple linear regression: confounding\n        Checkpoint 7 due 11:30am\n    \n    Work day\n    PP3 due Friday 10/4 at 5pm\n  \n  \n    6 (9/30 - 10/4)\n    \n        Introduction to interaction models\n    \n    \n        Practice with interaction models\n        Checkpoint 8 due 11:30am\n    \n    No class today (Leslie will be at a workshop) but feel free to use the classroom to work with your project groups\n    PP4 due Friday 10/11 at 5pm\n  \n\n\n\n\nWeekly details\n\n\n\n\n\n\nWeek 1: 9/2 - 9/6 (Data fundamentals, univariate visualization and summaries)\n\n\n\n\n\n\nAnnouncements\nWelcome (back) to campus!\n\n\nWednesday (9/4)\nBefore class:\n\nReview the course syllabus\nFamiliarize yourself with Moodle and the course website\nComplete the Welcome survey\nListen to the first episode of the Sold a Story podcast (Episode 1: The Problem) (~30 minutes). This will provide some useful context for what we explore on our first day together.\n\n\n\nFriday (9/6)\nBefore class:\n\nComplete all required steps on the R Resources page\nComplete the Welcome survey (if you haven‚Äôt already!)\nComplete today‚Äôs checkpoint (CP1) on Moodle\n\nDo either the readings or videos for the Day 2 Activity (see Moodle checkpoint description for links to the readings and videos)\nDue 30 minutes before class today (11:30am on 9/6)\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2: 9/9 - 9/13 (Simple linear regression)\n\n\n\n\n\n\nAnnouncements\n\nPractice Problems #1 (PP1) due Friday 9/13 at 5pm\n\n\n\nMonday (9/9)\nBefore class:\n\nReview material as needed\nNo checkpoint today!\n\n\n\nWednesday (9/11)\nBefore class:\n\nComplete today‚Äôs checkpoint (CP2) on Moodle\n\nDo either the readings or videos (see Moodle checkpoint description for links to the readings and videos)\nDue 30 minutes before class today (11:30am on 9/11)\n\n\n\n\nFriday (9/13)\nBefore class:\n\nReview material as needed\nWork on Practice Problems #1 (PP1). We will also have time in class to work on them.\n\n\n\n\n\n\n\n\n\n\n\nWeek 3: 9/16 - 9/20 (Simple linear regression: model evaluation and variable transformations)\n\n\n\n\n\n\nAnnouncements\n\nPractice Problems #2 (PP2) due Friday 9/20 at 5pm\nProject Group Preferences Survey due Friday 9/20 at 5pm\n\nThis is a short survey to help me determine groups for the course project.\n\nQuiz 1 is next Friday 9/27 and covers Activities 1-7 (Data Fundamentals to Simple Linear Regression: Categorical Predictors, which we‚Äôll talk about next Monday.)\n\n\n\nMonday (9/16)\nBefore class:\n\nComplete today‚Äôs checkpoint (CP3) on Moodle.\n\nSee checkpoint description for links to the readings/videos.\nDue 30 minutes before class today (11:30am).\n\n\n\n\nWednesday (9/18)\nBefore class:\n\nComplete today‚Äôs checkpoint (CP4) on Moodle.\n\nSee checkpoint description for links to the readings/videos.\nDue 30 minutes before class today (11:30am).\n\n\n\n\nFriday (9/20)\nBefore class\n\nThink about an area that you might want to explore for the course project. Macalester‚Äôs librarians will be coming in to lead a workshop on finding data.\n\n\n\n\n\n\n\n\n\n\n\nWeek 4: 9/23 - 9/27 (Simple linear regression: categorical predictors)\n\n\n\n\n\n\nAnnouncements\n\nNo Practice Problems due this week to allow time to study for Quiz 1 this Friday 9/27.\nFor a reminder about quiz format, see this part of the syllabus.\n\n\n\nMonday (9/23)\nBefore class:\n\nComplete today‚Äôs checkpoint (CP5) on Moodle.\n\nSee checkpoint description for links to the readings/videos.\nDue 30 minutes before class today (11:30am).\n\n\n\n\nWednesday (9/25)\nBefore class:\n\nReview material as needed\nNo checkpoint today!\n\n\n\nFriday (9/27)\nQuiz 1 (1 hour in class)\n\n\n\n\n\n\n\n\n\n\nWeek 5: 9/30 - 10/4 (Multiple linear regression: confounding)\n\n\n\n\n\n\nAnnouncements\n\nPractice Problems #3 (PP3) due Friday 10/4 at 5pm\nTake a look at project group assignments posted on Moodle.\n\nWork on connecting with your team, finding a dataset, and working on Project Checkpoint 1.\nWednesday, October 16: Project Checkpoint 1 due (ignore the Friday, October 11 mentioned in the Google Doc description of the project at the top of the project page)\n\n\n\n\nMonday (9/30)\nBefore class:\n\nComplete today‚Äôs checkpoint (CP6) on Moodle.\n\nSee checkpoint description for links to the readings/videos.\nDue 30 minutes before class today (11:30am).\n\n\n\n\nWednesday (10/2)\nBefore class:\n\nComplete today‚Äôs checkpoint (CP7) on Moodle.\n\nSee checkpoint description for links to the readings/videos.\nDue 30 minutes before class today (11:30am).\n\n\n\n\nFriday (10/4)\nTime to work on practice problems, project in class\n\n\n\n\n\n\n\n\n\n\nWeek 6: 10/7 - 10/11 (Multiple linear regression: interaction)\n\n\n\n\n\n\nAnnouncements\n\nIntro to R/RStudio Workshop, Thursday, Oct 10, noon-1pm, Library 250\n\nHosted by our MSCS department R preceptor!\n\nQuiz 1, PP1, PP2 corrections due Wednesday 10/9 on Moodle\nPractice Problems #4 (PP4) due Friday 10/11 at 5pm\nTake a look at project group assignments posted on Moodle.\n\nWork on connecting with your team, finding a dataset, and working on Project Checkpoint 1.\nWednesday, October 16: Project Checkpoint 1 due (ignore the Friday, October 11 mentioned in the Google Doc description of the project at the top of the project page)\n\n\n\n\nMonday (10/7)\nBefore class:\n\nReview material as needed\nNo checkpoint today!\n\n\n\nWednesday (10/9)\nBefore class:\n\nComplete today‚Äôs checkpoint (CP8) on Moodle.\n\nSee checkpoint description for links to the readings/videos.\nDue 30 minutes before class today (11:30am).\n\n\n\n\nFriday (10/11)\nNo class today - Leslie will be at a workshop. Feel free to use the classroom to meet with your project groups\n\n\n\n\n\n\n\n\n\n\nWeek 7: 10/14 - 10/18 (Multiple linear regression: model building) (Fall Break)\n\n\n\n\n\n\nAnnouncements\n\n\nMonday\nBefore class:\n\n\nWednesday\nBefore class:\n\n\nFriday\nBefore class:\n\n\n\n\n\n\n\n\n\n\nWeek 8: 10/21 - 10/25 (Logistic regression)\n\n\n\n\n\n\nAnnouncements\n\n\nMonday\nBefore class:\n\n\nWednesday\nBefore class:\n\n\nFriday\nBefore class:\n\n\n\n\n\n\n\n\n\n\nWeek 9: 10/28 - 11/1 (Tools for statistical inference)\n\n\n\n\n\n\nAnnouncements\n\n\nMonday\nBefore class:\n\n\nWednesday\nBefore class:\n\n\nFriday\nQuiz 2 (1 hour in class)\n\n\n\n\n\n\n\n\n\n\nWeek 10: 11/4 - 11/8 (Confidence intervals)\n\n\n\n\n\n\nAnnouncements\n\n\nMonday\nBefore class:\n\n\nWednesday\nBefore class:\n\n\nFriday\nBefore class:\n\n\n\n\n\n\n\n\n\n\nWeek 11: 11/11 - 11/15 (Hypothesis testing)\n\n\n\n\n\n\nAnnouncements\n\n\nMonday\nBefore class:\n\n\nWednesday\nBefore class:\n\n\nFriday\nBefore class:\n\n\n\n\n\n\n\n\n\n\nWeek 12: 11/18 - 11/22 (Hypothesis testing)\n\n\n\n\n\n\nAnnouncements\n\n\nMonday\nBefore class:\n\n\nWednesday\nBefore class:\n\n\nFriday\nBefore class:\n\n\n\n\n\n\n\n\n\n\nWeek 13: 11/25 - 11/29 (Hypothesis testing, p-values) (Thanksgiving Break)\n\n\n\n\n\n\nAnnouncements\n\n\nMonday\nBefore class:\n\n\nWednesday\nBefore class:\n\n\nFriday\nBefore class:\n\n\n\n\n\n\n\n\n\n\nWeek 14: 12/2 - 12/6 (Project work)\n\n\n\n\n\n\nAnnouncements\n\n\nMonday\nBefore class:\n\n\nWednesday\nBefore class:\n\n\nFriday\nBefore class:\n\n\n\n\n\n\n\n\n\n\nWeek 15: 12/9 - 12/13 (Project work, paper, and presentations)\n\n\n\n\n\n\nAnnouncements\n\n\nMonday\nBefore class:\n\n\nWednesday\n(Last day of class)\nBefore class:"
  }
]